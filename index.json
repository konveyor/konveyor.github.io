[{"uri":"http://konveyor.github.io/move2kube/overview/","title":"Overview","tags":[],"description":"","content":"The Move2Kube tool helps application owners migrate legacy workloads to run on Kubernetes clusters and eventually automate their deployments after multiple iterations. It analyzes Docker Compose files, Cloud Foundry manifest files, and even source code to generate Kubernetes deployment files including object YAML files, Helm charts, and operators.\nMove2Kube has a very modular architecture making it easy to custom functionality for a large variety of migration use-cases.\nThe project includes three tools:\nmove2kube: The primary tool is the command line interface (CLI) that takes in application source code and generates Kubernetes artifacts. move2kube-ui: A UI for interacting with the Move2Kube CLI tool for running fully-managed Move2Kube runtimes. move2kube-transformers: A collection of useful transformers for extending Move2Kube\u0026rsquo;s functionality that has been built by the Konveyor community based on experience from performing migrations for clients. View this presentation for an overview of Move2Kube.\nSource\n"},{"uri":"http://konveyor.github.io/crane/","title":"Crane","tags":[],"description":"","content":"Crane Use this section to better understand and use the Konveyor Crane tool.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/cfappstok8/1collect/","title":"1. Collect","tags":[],"description":"","content":" Note: This is an optional step. If you are not familiar with Cloud Foundry or you do not want to collect information from your running app, skip to Customizing the output.\nTo analyze the running application in Cloud Foundry (CF), the Move2Kube CLI tool provides a command called collect. As the name suggests, the collect command collects information about applications running in the cloud.\nFor collecting information from a CF running instance, you might require cf CLI for logging into Cloud Foundry. If you want to target a specific Kubernetes cluster for your yamls, you will need either oc, and kubectl to collect information about the target cluster.\nIf you are logged into the Cloud Foundry instance, information about the apps such as environment variables, services, and more are collected. If you are logged into Kubernetes clusters, it collects information about the types of resources that are installed on the cluster, such as whether it has Tekton, BuildConfigs, etc.\nAll the information that was collected gets written into a directory called m2k_collect as YAML files. In this case, the info about Cloud Foundry apps is written to a sub-directory called cf. These YAMLs can then be used during the plan phase to get a holistic plan combining the source and metadata.\nFor example: Some of the information that is collected is port and environment variable information. This allows Move2Kube to select the right ports and set the right environment variables for each service when generating Dockerfiles for containerizing these services.\nCollecting info from e2e-demo app Prerequisites\nThe cf tool installed and you have logged into your Cloud Foundry instance. Run cf target to check if you are logged in. The output should be similar to this: $ cf target API endpoint: https://api.cf.my.cloud.provider.com API version: 3.107.0 user: user@gmail.com org: my-org space: dev The enterprise-app app in the Cloud Foundry instance is deployed. $ cf apps Getting apps in org my-org / space dev as user@gmail.com... name requested state processes routes frontend started web:1/1 frontend-1234.my.cloud.provider.com gateway started web:1/1, task:0/0 gateway-5678.my.cloud.provider.com orders started web:1/1, task:0/0 orders-1234.my.cloud.provider.com ... Procedure\nRun move2kube collect to collect information about the app from Cloud Foundry. $ move2kube collect INFO[0000] Begin collection INFO[0000] [*collector.ClusterCollector] Begin collection INFO[0000] [*collector.ClusterCollector] Done INFO[0000] [*collector.ImagesCollector] Begin collection INFO[0000] [*collector.ImagesCollector] Done INFO[0000] [*collector.CfAppsCollector] Begin collection INFO[0011] [*collector.CfAppsCollector] Done INFO[0011] [*collector.CfServicesCollector] Begin collection INFO[0026] [*collector.CfServicesCollector] Done INFO[0026] Collection done INFO[0026] Collect Output in [/Users/user/Desktop/tutorial/m2k_collect]. Copy this directory into the source directory to be used for planning. The output will be in a directory called m2k_collect with a sub-directory called cf containing two 2 YAML files: CfApps and CfServices.\n$ ls m2k_collect/ cf\tclusters\timages $ ls m2k_collect/cf/ cfapps-e3a2f9d68a7a5ecc.yaml\tcfservices-32194c9906854947.yaml The CfApps file contains all the information that was collected about the app such as service names, environment variables, ports, etc. An example is provided here\n\u0026lt;details markdown=\u0026#34;block\u0026#34;\u0026gt; \u0026lt;summary markdown=\u0026#34;block\u0026#34;\u0026gt; # click to see the full yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: CfApps ...... The returned YAML.\napiVersion: move2kube.konveyor.io/v1alpha1 kind: CfApps spec: applications: - application: guid: id1 createdat: \u0026#34;2021-12-14T10:01:40Z\u0026#34; updatedat: \u0026#34;2021-12-14T10:03:08Z\u0026#34; name: orders memory: 1024 instances: 1 Click to see the rest of the yaml. diskquota: 1024 spaceguid: space-id1 stackguid: stack-id1 state: STARTED packagestate: STAGED command: \u0026#34;\u0026#34; buildpack: https://github.com/cloudfoundry/java-buildpack detectedbuildpack: java detectedbuildpackguid: \u0026#34;\u0026#34; healthcheckhttpendpoint: \u0026#34;\u0026#34; healthchecktype: port healthchecktimeout: 0 diego: true enablessh: true detectedstartcommand: \u0026#39;JAVA_OPTS=\u0026#34;-agentpath:$PWD/.java-buildpack/open_jdk_jre/bin/jvmkill-1.16.0_RELEASE=printHeapHistogram=1 -Djava.io.tmpdir=$TMPDIR -XX:ActiveProcessorCount=$(nproc) -Djava.ext.dirs=$PWD/.java-buildpack/container_security_provider:$PWD/.java-buildpack/open_jdk_jre/lib/ext -Djava.security.properties=$PWD/.java-buildpack/java_security/java.security $JAVA_OPTS\u0026#34; \u0026amp;\u0026amp; CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-3.13.0_RELEASE -totMemory=$MEMORY_LIMIT -loadedClasses=23193 -poolType=metaspace -stackThreads=250 -vmOptions=\u0026#34;$JAVA_OPTS\u0026#34;) \u0026amp;\u0026amp; echo JVM Memory Configuration: $CALCULATED_MEMORY \u0026amp;\u0026amp; JAVA_OPTS=\u0026#34;$JAVA_OPTS $CALCULATED_MEMORY\u0026#34; \u0026amp;\u0026amp; MALLOC_ARENA_MAX=2 SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher\u0026#39; dockerimage: \u0026#34;\u0026#34; dockercredentialsjson: {} dockercredentials: username: \u0026#34;\u0026#34; password: \u0026#34;\u0026#34; environment: {} stagingfailedreason: \u0026#34;\u0026#34; stagingfaileddescription: \u0026#34;\u0026#34; ports: - 8080 spaceurl: /v2/spaces/space-id1 spacedata: meta: guid: space-id1 url: /v2/spaces/space-id1 createdat: \u0026#34;2020-10-05T05:29:46Z\u0026#34; updatedat: \u0026#34;2020-10-05T05:29:46Z\u0026#34; entity: guid: space-id1 createdat: \u0026#34;\u0026#34; updatedat: \u0026#34;\u0026#34; name: dev organizationguid: org-id1 orgurl: /v2/organizations/org-id1 orgdata: meta: guid: org-id1 url: /v2/organizations/org-id1 createdat: \u0026#34;2020-10-05T05:29:31Z\u0026#34; updatedat: \u0026#34;2020-10-05T05:29:31Z\u0026#34; entity: guid: org-id1 createdat: \u0026#34;\u0026#34; updatedat: \u0026#34;\u0026#34; name: org-name status: active quotadefinitionguid: quota-id defaultisolationsegmentguid: \u0026#34;\u0026#34; quotadefinitionguid: \u0026#34;\u0026#34; isolationsegmentguid: \u0026#34;\u0026#34; allowssh: true packageupdatedat: \u0026#34;2021-12-14T10:01:49Z\u0026#34; environment: environment: {} stagingenv: BLUEMIX_REGION: region runningenv: BLUEMIX_REGION: region systemenv: VCAP_SERVICES: \u0026#39;{}\u0026#39; applicationenv: VCAP_APPLICATION: \u0026#39;{\u0026#34;application_id\u0026#34;:\u0026#34;id1\u0026#34;,\u0026#34;application_name\u0026#34;:\u0026#34;orders\u0026#34;,\u0026#34;application_uris\u0026#34;:[\u0026#34;orders-proud-bilby-rf.net\u0026#34;],\u0026#34;application_version\u0026#34;:\u0026#34;app-ver1\u0026#34;,\u0026#34;cf_api\u0026#34;:\u0026#34;app-url\u0026#34;,\u0026#34;limits\u0026#34;:{\u0026#34;disk\u0026#34;:1024,\u0026#34;fds\u0026#34;:16384,\u0026#34;mem\u0026#34;:1024},\u0026#34;name\u0026#34;:\u0026#34;orders\u0026#34;,\u0026#34;organization_id\u0026#34;:\u0026#34;org-id1\u0026#34;,\u0026#34;organization_name\u0026#34;:\u0026#34;org-name\u0026#34;,\u0026#34;process_id\u0026#34;:\u0026#34;id1\u0026#34;,\u0026#34;process_type\u0026#34;:\u0026#34;web\u0026#34;,\u0026#34;space_id\u0026#34;:\u0026#34;space-id1\u0026#34;,\u0026#34;space_name\u0026#34;:\u0026#34;dev\u0026#34;,\u0026#34;uris\u0026#34;:[\u0026#34;orders-proud-bilby-rf.net\u0026#34;],\u0026#34;users\u0026#34;:null,\u0026#34;version\u0026#34;:\u0026#34;app-ver1\u0026#34;}\u0026#39; - application: guid: id2 createdat: \u0026#34;2021-12-14T10:04:00Z\u0026#34; updatedat: \u0026#34;2021-12-14T10:05:43Z\u0026#34; name: gateway memory: 1024 instances: 1 diskquota: 1024 spaceguid: space-id1 stackguid: stack-id1 state: STARTED packagestate: STAGED command: \u0026#34;\u0026#34; buildpack: https://github.com/cloudfoundry/java-buildpack detectedbuildpack: java detectedbuildpackguid: \u0026#34;\u0026#34; healthcheckhttpendpoint: \u0026#34;\u0026#34; healthchecktype: port healthchecktimeout: 0 diego: true enablessh: true detectedstartcommand: \u0026#39;JAVA_OPTS=\u0026#34;-agentpath:$PWD/.java-buildpack/open_jdk_jre/bin/jvmkill-1.16.0_RELEASE=printHeapHistogram=1 -Djava.io.tmpdir=$TMPDIR -XX:ActiveProcessorCount=$(nproc) -Djava.ext.dirs=$PWD/.java-buildpack/container_security_provider:$PWD/.java-buildpack/open_jdk_jre/lib/ext -Djava.security.properties=$PWD/.java-buildpack/java_security/java.security $JAVA_OPTS\u0026#34; \u0026amp;\u0026amp; CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-3.13.0_RELEASE -totMemory=$MEMORY_LIMIT -loadedClasses=24458 -poolType=metaspace -stackThreads=250 -vmOptions=\u0026#34;$JAVA_OPTS\u0026#34;) \u0026amp;\u0026amp; echo JVM Memory Configuration: $CALCULATED_MEMORY \u0026amp;\u0026amp; JAVA_OPTS=\u0026#34;$JAVA_OPTS $CALCULATED_MEMORY\u0026#34; \u0026amp;\u0026amp; MALLOC_ARENA_MAX=2 SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher\u0026#39; dockerimage: \u0026#34;\u0026#34; dockercredentialsjson: {} dockercredentials: username: \u0026#34;\u0026#34; password: \u0026#34;\u0026#34; environment: {} stagingfailedreason: \u0026#34;\u0026#34; stagingfaileddescription: \u0026#34;\u0026#34; ports: - 8080 spaceurl: /v2/spaces/space-id1 spacedata: meta: guid: space-id1 url: /v2/spaces/space-id1 createdat: \u0026#34;2020-10-05T05:29:46Z\u0026#34; updatedat: \u0026#34;2020-10-05T05:29:46Z\u0026#34; entity: guid: space-id1 createdat: \u0026#34;\u0026#34; updatedat: \u0026#34;\u0026#34; name: dev organizationguid: org-id1 orgurl: /v2/organizations/org-id1 orgdata: meta: guid: org-id1 url: /v2/organizations/org-id1 createdat: \u0026#34;2020-10-05T05:29:31Z\u0026#34; updatedat: \u0026#34;2020-10-05T05:29:31Z\u0026#34; entity: guid: org-id1 createdat: \u0026#34;\u0026#34; updatedat: \u0026#34;\u0026#34; name: org-name status: active quotadefinitionguid: quota-id defaultisolationsegmentguid: \u0026#34;\u0026#34; quotadefinitionguid: \u0026#34;\u0026#34; isolationsegmentguid: \u0026#34;\u0026#34; allowssh: true packageupdatedat: \u0026#34;2021-12-14T10:04:09Z\u0026#34; environment: environment: {} stagingenv: BLUEMIX_REGION: region runningenv: BLUEMIX_REGION: region systemenv: VCAP_SERVICES: \u0026#39;{}\u0026#39; applicationenv: VCAP_APPLICATION: \u0026#39;{\u0026#34;application_id\u0026#34;:\u0026#34;id2\u0026#34;,\u0026#34;application_name\u0026#34;:\u0026#34;gateway\u0026#34;,\u0026#34;application_uris\u0026#34;:[\u0026#34;gateway-restless-fossa-ws.net\u0026#34;],\u0026#34;application_version\u0026#34;:\u0026#34;app-ver2\u0026#34;,\u0026#34;cf_api\u0026#34;:\u0026#34;app-url\u0026#34;,\u0026#34;limits\u0026#34;:{\u0026#34;disk\u0026#34;:1024,\u0026#34;fds\u0026#34;:16384,\u0026#34;mem\u0026#34;:1024},\u0026#34;name\u0026#34;:\u0026#34;gateway\u0026#34;,\u0026#34;organization_id\u0026#34;:\u0026#34;org-id1\u0026#34;,\u0026#34;organization_name\u0026#34;:\u0026#34;org-name\u0026#34;,\u0026#34;process_id\u0026#34;:\u0026#34;id2\u0026#34;,\u0026#34;process_type\u0026#34;:\u0026#34;web\u0026#34;,\u0026#34;space_id\u0026#34;:\u0026#34;space-id1\u0026#34;,\u0026#34;space_name\u0026#34;:\u0026#34;dev\u0026#34;,\u0026#34;uris\u0026#34;:[\u0026#34;gateway-restless-fossa-ws.net\u0026#34;],\u0026#34;users\u0026#34;:null,\u0026#34;version\u0026#34;:\u0026#34;app-ver2\u0026#34;}\u0026#39; - application: guid: id3 createdat: \u0026#34;2021-12-14T14:54:25Z\u0026#34; updatedat: \u0026#34;2021-12-14T15:15:38Z\u0026#34; name: frontend memory: 1024 instances: 1 diskquota: 1024 spaceguid: space-id1 stackguid: stack-id1 state: STARTED packagestate: STAGED command: npm run start buildpack: https://github.com/cloudfoundry/nodejs-buildpack detectedbuildpack: nodejs detectedbuildpackguid: \u0026#34;\u0026#34; healthcheckhttpendpoint: \u0026#34;\u0026#34; healthchecktype: port healthchecktimeout: 0 diego: true enablessh: true detectedstartcommand: npm start dockerimage: \u0026#34;\u0026#34; dockercredentialsjson: {} dockercredentials: username: \u0026#34;\u0026#34; password: \u0026#34;\u0026#34; environment: {} stagingfailedreason: \u0026#34;\u0026#34; stagingfaileddescription: \u0026#34;\u0026#34; ports: - 8080 spaceurl: /v2/spaces/space-id1 spacedata: meta: guid: space-id1 url: /v2/spaces/space-id1 createdat: \u0026#34;2020-10-05T05:29:46Z\u0026#34; updatedat: \u0026#34;2020-10-05T05:29:46Z\u0026#34; entity: guid: space-id1 createdat: \u0026#34;\u0026#34; updatedat: \u0026#34;\u0026#34; name: dev organizationguid: org-id1 orgurl: /v2/organizations/org-id1 orgdata: meta: guid: org-id1 url: /v2/organizations/org-id1 createdat: \u0026#34;2020-10-05T05:29:31Z\u0026#34; updatedat: \u0026#34;2020-10-05T05:29:31Z\u0026#34; entity: guid: org-id1 createdat: \u0026#34;\u0026#34; updatedat: \u0026#34;\u0026#34; name: org-name status: active quotadefinitionguid: quota-id defaultisolationsegmentguid: \u0026#34;\u0026#34; quotadefinitionguid: \u0026#34;\u0026#34; isolationsegmentguid: \u0026#34;\u0026#34; allowssh: true packageupdatedat: \u0026#34;2021-12-14T14:59:40Z\u0026#34; environment: environment: {} stagingenv: BLUEMIX_REGION: region runningenv: BLUEMIX_REGION: region systemenv: VCAP_SERVICES: \u0026#39;{}\u0026#39; applicationenv: VCAP_APPLICATION: \u0026#39;{\u0026#34;application_id\u0026#34;:\u0026#34;id3\u0026#34;,\u0026#34;application_name\u0026#34;:\u0026#34;frontend\u0026#34;,\u0026#34;application_uris\u0026#34;:[\u0026#34;frontend-patient-oryx-mc.net\u0026#34;],\u0026#34;application_version\u0026#34;:\u0026#34;app-ver3\u0026#34;,\u0026#34;cf_api\u0026#34;:\u0026#34;app-url\u0026#34;,\u0026#34;limits\u0026#34;:{\u0026#34;disk\u0026#34;:1024,\u0026#34;fds\u0026#34;:16384,\u0026#34;mem\u0026#34;:1024},\u0026#34;name\u0026#34;:\u0026#34;frontend\u0026#34;,\u0026#34;organization_id\u0026#34;:\u0026#34;org-id1\u0026#34;,\u0026#34;organization_name\u0026#34;:\u0026#34;org-name\u0026#34;,\u0026#34;process_id\u0026#34;:\u0026#34;id3\u0026#34;,\u0026#34;process_type\u0026#34;:\u0026#34;web\u0026#34;,\u0026#34;space_id\u0026#34;:\u0026#34;space-id1\u0026#34;,\u0026#34;space_name\u0026#34;:\u0026#34;dev\u0026#34;,\u0026#34;uris\u0026#34;:[\u0026#34;frontend-patient-oryx-mc.net\u0026#34;],\u0026#34;users\u0026#34;:null,\u0026#34;version\u0026#34;:\u0026#34;app-ver3\u0026#34;}\u0026#39; Now that we have collected the runtime information from the app running in our Cloud Foundry instance, we can use it during the planning phase by simply copying it into the source directory before starting the planning. All the steps are same as the Plan step.\nNext steps Next we will customize the output that Move2Kube produces using customizations.\n"},{"uri":"http://konveyor.github.io/move2kube/concepts/","title":"Concepts","tags":[],"description":"","content":"Move2Kube has four concepts that are useful to understand when customizing output and this section covers the more important ones.\nImportant: If you have not followed the tutorials, we recommend checking those out first, then coming back here to see each concept in more detail.\nArtifacts Source code\nArtifacts represent the application objects that can be passed between transformers.\ntype Artifact struct { Name string `yaml:\u0026#34;name,omitempty\u0026#34; json:\u0026#34;name,omitempty\u0026#34;` Type ArtifactType `yaml:\u0026#34;type,omitempty\u0026#34; json:\u0026#34;type,omitempty\u0026#34;` ProcessWith metav1.LabelSelector `yaml:\u0026#34;processWith,omitempty\u0026#34; json:\u0026#34;processWith,omitempty\u0026#34;` // Selector for choosing transformers that should process this artifact, empty is everything Paths map[PathType][]string `yaml:\u0026#34;paths,omitempty\u0026#34; json:\u0026#34;paths,omitempty\u0026#34; m2kpath:\u0026#34;normal\u0026#34;` Configs map[ConfigType]interface{} `yaml:\u0026#34;configs,omitempty\u0026#34; json:\u0026#34;config,omitempty\u0026#34;` // Could be IR or template config or any custom configuration } Artifact fields Each artifact is an object with fields that need to be understood in order to write transformers effectively.\nname : string - Name of the artifact. type : string - Type of the artifact (any artifact type can be used). Important: Transformers consume artifacts based on their type, so custom artifact types can only be consumed by custom transformers that understand them. Example built-in artifact types: IR, KubernetesYamls, Dockerfile, etc.\nprocessWith : object - Same as the Kubernetes label selector field. See https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#resources-that-support-set-based-requirements paths : object ([string]: []string) - Mapping from file type to a list of directories containing files of that type. The key is a string containing the file type. The value is a list of strings/paths to directories containing files of that type. configs : object ([string]: any) - Mapping between different types of configurations and the configuration data. The key is a string containing the type of configuration. The value can be anything but usually an object. Example built-in configs: types/transformer/artifacts/cloudfoundry.go#L35-L39 types/transformer/artifacts/java.go#L49-L75 types/transformer/artifacts/gradle.go#L24-L27 Transformers Move2Kube uses transformers to modify input into the desired output form. Each transformer consumes artifacts as input and returns outputs artifacts and PathMappings.\nThe artifacts allow multiple transformers to be chained together to achieve an end to end transformation and the PathMappings are used for persisting the changes in the file system. Some transformers have detection capability to go through the source directories to identify one it understands and creates new artifacts to start the process.\nTransformer directories Each transformer generally has its own directory with all the configuration parameters required for that transformer whether it is a built-in transformer or external transformer. The transformer YAML is the most important part of the definition because it specifies its behavior. It also can have a templates directory for template files to be used by the transformer, and other files/configurations that are specific to each transformer.\nTransformer YAML Source code\nTransformers define the definition of the Cloud Foundry (CF) runtime instance app file.\ntype Transformer struct { types.TypeMeta `yaml:\u0026#34;,inline\u0026#34; json:\u0026#34;,inline\u0026#34;` types.ObjectMeta `yaml:\u0026#34;metadata,omitempty\u0026#34; json:\u0026#34;metadata,omitempty\u0026#34;` Spec TransformerSpec `yaml:\u0026#34;spec,omitempty\u0026#34; json:\u0026#34;spec,omitempty\u0026#34;` } TransformerSpec stores the data.\ntype TransformerSpec struct { FilePath string `yaml:\u0026#34;-\u0026#34; json:\u0026#34;-\u0026#34;` Class string `yaml:\u0026#34;class\u0026#34; json:\u0026#34;class\u0026#34;` Isolated bool `yaml:\u0026#34;isolated\u0026#34; json:\u0026#34;isolated\u0026#34;` DirectoryDetect DirectoryDetect `yaml:\u0026#34;directoryDetect\u0026#34; json:\u0026#34;directoryDetect\u0026#34;` ExternalFiles map[string]string `yaml:\u0026#34;externalFiles\u0026#34; json:\u0026#34;externalFiles\u0026#34;` // [source]destination ConsumedArtifacts map[ArtifactType]ArtifactProcessConfig `yaml:\u0026#34;consumes\u0026#34; json:\u0026#34;consumes\u0026#34;` ProducedArtifacts map[ArtifactType]ProducedArtifact `yaml:\u0026#34;produces\u0026#34; json:\u0026#34;produces\u0026#34;` Dependency interface{} `yaml:\u0026#34;dependency\u0026#34; json:\u0026#34;dependency\u0026#34;` // metav1.LabelSelector Override interface{} `yaml:\u0026#34;override\u0026#34; json:\u0026#34;override\u0026#34;` // metav1.LabelSelector DependencySelector labels.Selector `yaml:\u0026#34;-\u0026#34; json:\u0026#34;-\u0026#34;` OverrideSelector labels.Selector `yaml:\u0026#34;-\u0026#34; json:\u0026#34;-\u0026#34;` TemplatesDir string `yaml:\u0026#34;templates\u0026#34; json:\u0026#34;templates\u0026#34;` // Relative to yaml directory or working directory in image Config interface{} `yaml:\u0026#34;config\u0026#34; json:\u0026#34;config\u0026#34;` } The example above shows the format of the transformer YAML file. For more information on the YAML format, see this quick tutorial.\nYAML files have four main fields with sub-fields that define them.\nNote: The apiVersion and kind are necessary to tell Move2Kube that this is a transformer.\napiVersion : string - Similar to Kubernetes apiVersion strings. This should be move2kube.konveyor.io/v1alpha1 for now.\nkind : string - Resource type contained in the YAML file which should be labeled Transformer for transformers.\nmetadata : object - Defines the transformer name and can set optional labels used to enable or disable the transformer.\nname : string - Transformer name. labels : object ([string]: string) - Set of labels similar to Kubernetes used to enable/disable a set of transformers during both planning and transformation phases. For more details run the move2kube help transform command. spec : object - Main transformer data.\nclass : string - Mandatory field specifying which Move2Kube internal implementation to use for this transformer. Examples are: Kubernetes, Parameterizer, GolangDockerfileGenerator, Executable, Starlark, etc. isolated : boolean - If true, the transformer will receive a full unmodified copy of the source directory. By default, transformers do not run in isolation but instead receive a temporary directory containing a copy of the source directory that has already been used by other transformers. If other transformers have created temporary files, all of those files will be visible to the transformer. Note: Running in isolation increases the run time of your transformer but makes writing transformers easier because no clean up after transformer has finished is necessary.\ndirectoryDetect : object - Used to control the directories the transformer runs on during the planning phase. levels : int - Supported values: -1: Runs on the source directory and all sub-directories. 0: Skips directory detect entirely, does not run on any directories. 1: Runs on only the source directory, not on any of the sub-directories. externalFiles : object ([string]: string) - Used to specify files that need to be copied from outside the context of the transformer into the transformer. This is helpful to specify files used by multiple transformers in a single location. consumes : object ([string]: object) - Used to narrow down the artifacts that the transformer runs on during the transformation phase. The key is a string containing the type of the artifact and the value is an object with the following fields: merge : boolean - If true, all artifacts of this type will be merged into a single artifact before being passed to the transformer. produces : object ([string]: object) - Used to tell Move2Kube the type of output artifacts the transformer will return. The key is a string containing the type of the artifact and the value is an object with the following fields: changeTypeTo : string - Used to change the artifact type to something else. Useful for overriding the behavior of existing transformers. dependency : any - If the transformer wants the artifacts that are about to be processed by this transformer to be preprocessed by another transformer, this field specifies the transformer to use for preprocessing. override : any - If this transformer overrides the behavior of other transformers, a selector can be specified to disable those transformers. templates : string - Specifies the template directory. The default value is templates config : any - Each transformer has a type/class specified by the class field which provides certain configuration options that can be configured here. For more details refer documentation for the transformer class being used. Example: Parameterizer config Other files/directories templates - If the Template type path mapping created by this transformer uses a relative path, it is considered to be relative to this directory. There can be other files/configs in the directory that are interpreted differently by each transformer class which then determines how the values are interpreted and executed.\nTransformer Class The Transformer Class determines the code used for the internal execution of the transformer using the configuration in the Transformer Yaml and other config files to model its behavior. There are many transformer classes supported by Move2Kube, Kubernetes, Parameterizer, GolangDockerfileGenerator, Executable, Starlark, Router, etc. Most of them have a specific task, but some transformer classes like Executable and Starlark are customizable allowing users to write the entire logic of the transformer in the customization.\nTransformer Class Internal Implementation Source code\nThe transformer interface defines the transformer that modifies and converts files to IR representation.\ntype Transformer interface { Init(tc transformertypes.Transformer, env *environment.Environment) (err error) // GetConfig returns the transformer config GetConfig() (transformertypes.Transformer, *environment.Environment) DirectoryDetect(dir string) (services map[string][]transformertypes.Artifact, err error) Transform(newArtifacts []transformertypes.Artifact, alreadySeenArtifacts []transformertypes.Artifact) ([]transformertypes.PathMapping, []transformertypes.Artifact, error) } This example is the interface all transformers are expected to implement.\nTransform: The main function that needs to be implemented. Use DirectoryDetect for custom behavior during the planning phase. Important: Set directoryDetect to a value other than 0 in the transformer YAML.\nThe Init and GetConfig functions are fixed and implemented by transformers built into Move2Kube. They cannot be implemented by custom transformers. Methods Init : (Transformer, Environment) -\u0026gt; (error) - TODO GetConfig : (Transformer, Environment) -\u0026gt; () - TODO DirectoryDetect : (string) -\u0026gt; (object ([string]: []Artifact), error): This function is called during the planning phase and given the path of a directory containing the source files and then returns a list of artifacts listed in the plan-file generated by Move2Kube. It will also return an error if planning does not run correctly. Input is a string containing the path to a directory with source files which could be the source directory itself or a sub-directory based on the value of directoryDetect in the transformer YAML. A value of -1 for directoryDetect: The function runs on the source directory and all of its sub-directories. A value of 0 for directoryDetect: The function is disabled entirely (it will not be run on any directories). A value of 1 for directoryDetect: The function runs only on the source directory but not on its sub-directories. The output is a list of artifacts which will be included in the plan-file. Transform : ([]Artifact, []Artifact) -\u0026gt; ([]PathMapping, []Artifact, error): This function is called during the transformation phase and contains the code to perform the actual transformation and produce part of the Move2Kube output. The path mappings returned by this function cause changes to the Move2Kube output and the artifacts returned are passed to other transformers during the next iteration. It will also return an error if planning does not run correctly. The first input is a list of new artifacts produced during the previous iteration. The second input is a list of artifacts that the transformer has already seen. The first output is a list of path mappings. Path Mapping Path mappings are a way for transformers to add files to the Move2Kube output directory. They can be used to generate new files, delete exiting files, modify the output directory structure, etc. Usually transformers deal with artifacts as they take them as input and output new artifacts, but does nothing to change the Move2Kube output since all transformers are run inside temporary directories.\nIn order to affect the output directory, transformers need to return path mappings indicating the type of change to be made.\nFor example: Consider a transformer that adds an annotation to Kubernetes Ingress YAML files. The transformer reads the file, adds the annotation, and then writes it back out. However this modified file is only present inside the temporary directory and does not appear in the output directory of Move2Kube. To copy this file over to the output directory, create a path mapping to return this from the transformer.:\n{ \u0026#34;type\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;sourcePath\u0026#34;: \u0026#34;annotated-ingress.yaml\u0026#34;, \u0026#34;destinationPath\u0026#34;: \u0026#34;deploy/yamls/ingress.yaml\u0026#34; } Once the transformer is finished, Move2Kube will look at the path mapping the transformer returned and copy over the file to the output directory.\nThe example above shows the simplest use case for path mappings, but they are capable of much more advanced uses. For example: the source file is a template and needs to be filled in before being copied to the output.\nAnother example is when the source and destination paths are template strings that need to be filled in order to get the actual paths.\nDifferent type of path mappings Source code\nPathMappingType refers to the Path Mapping type.\ntype PathMappingType = string const ( // DefaultPathMappingType allows normal copy with overwrite // TemplatePathMappingType allows copy of source to destination and applying of template TemplatePathMappingType PathMappingType = \u0026#34;Template\u0026#34; // Source path when relative, is relative to yaml file location // SourcePathMappingType allows for copying of source directory to another directory SourcePathMappingType PathMappingType = \u0026#34;Source\u0026#34; // Source path becomes relative to source directory // DeletePathMappingType allows for deleting of files or directories DeletePathMappingType PathMappingType = \u0026#34;Delete\u0026#34; // Delete path becomes relative to source directory // ModifiedSourcePathMappingType allows for copying of deltas wrt source ModifiedSourcePathMappingType PathMappingType = \u0026#34;SourceDiff\u0026#34; // Source path becomes relative to source directory // PathTemplatePathMappingType allows for path template registration PathTemplatePathMappingType PathMappingType = \u0026#34;PathTemplate\u0026#34; // Path Template type // SpecialTemplatePathMappingType allows copy of source to destination and applying of template with custom delimiter SpecialTemplatePathMappingType PathMappingType = \u0026#34;SpecialTemplate\u0026#34; // Source path when relative, is relative to yaml file location ) PathMapping is the mapping between source and intermediate files and output files.\ntype PathMapping struct { Type PathMappingType `yaml:\u0026#34;type,omitempty\u0026#34; json:\u0026#34;type,omitempty\u0026#34;` // Default - Normal copy SrcPath string `yaml:\u0026#34;sourcePath\u0026#34; json:\u0026#34;sourcePath\u0026#34; m2kpath:\u0026#34;normal\u0026#34;` DestPath string `yaml:\u0026#34;destinationPath\u0026#34; json:\u0026#34;destinationPath\u0026#34; m2kpath:\u0026#34;normal\u0026#34;` // Relative to output directory TemplateConfig interface{} `yaml:\u0026#34;templateConfig\u0026#34; json:\u0026#34;templateConfig\u0026#34;` } There are seven different types of path mappings:\nDefault - sourcePath must be an absolute path. destinationPath must be a relative path, relative to Move2Kube\u0026rsquo;s output directory. This will copy the directory/file specified in sourcePath to destinationPath. Template - sourcePath must be a relative path, relative to the templates directory of the transformer. destinationPath must be a relative path, relative to Move2Kube\u0026rsquo;s output directory. This fills the template in the file given by sourcePath and copies the filled template to destinationPath. The values for filling the template are given in templateConfig. Source - Same as Default path mapping except now the sourcePath can now be a relative path, relative to the temporary directory where the transformer is running. Delete - sourcePath must be a relative path, relative to Move2Kube\u0026rsquo;s output directory. The directory/file specified by sourcePath will be deleted. SourceDiff - TODO PathTemplate - The path itself becomes a template. sourcePath contains the template path. templateConfig can be used to set a name for this template path. SpecialTemplate - Same as Template path mapping except now the template has a different syntax. The delimiters used in normal templates are {{ and }}. In special templates, the delimiters are \u0026lt;~ and ~\u0026gt;. Same as before, the values for filling the template are provided in templateConfig. Phases Move2Kube uses two key phases:\nPlanning Transformation Planning phase This phase starts by running the move2kube plan -s path/to/source/directory command. Move2Kube runs all the transformers that support the detect capability on the source directory to create a plan. The plan is written to a file called m2k.plan in YAML format which the transformation phase Move2Kube will use this plan to modify the source files into the desired output. The plan-file is human readable and can be edited manually to change the modifications performed during the transformation phase.\nThe plan-file contains the list of detected services that Move2Kube found inside the source directory, including the path to the sub-directories/files where it detected information about those services. It also contains a list of all the built-in and external transformers that were detected which will be run during the transformation phase. Custom transformers can be written and provided during the plan phase to affect the contents of the plan file.\nTransformation phase This phase starts by running the move2kube transform command. Move2Kube evaluates which transformers to run in an iterative manner. Each iteration will evaluate the list of artifacts produced during the previous iteration and run all transformers that consume those artifact types. This continues until it hits an iteration where there are no more artifacts or transformers that consume those artifact types at which point the transformation phase is complete.\nThe evaluated result of all PathMappings is the output.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/cfappstok8/2plan/","title":"2. Plan","tags":[],"description":"","content":" We start by planning the migration. During the plan phase, Move2Kube will analyze the files in the source directory, detect what services exist, create a plan on how to containerize them using Dockerfiles, and transform them into Kubernetes deployments, services, ingress, etc.\nIn order to do the planning, Move2Kube has a large number of built-in transformers for different languages and platforms. Each transformer walks through the source directory from top to bottom and tries to find files that it recognizes. For example, a Golang transformer will try to find a go.mod file to detect a Golang project. Once it detects a directory containing a service, it will try to extract as much information from it as possible. Some of the information it tries to find are the service name, ports, environment variables, etc.\nThis information is stored in YAML format in a plan file called m2k.plan which is used later during the transformation phase. We can edit this file to enable/disable transformers, add/remove detected services, etc.\nPrerequisites We will be using the enterprise-app app. Download it using the below command.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/src -r move2kube-demos $ ls src README.md\tconfig-utils\tcustomers\tdocs\tfrontend\tgateway\torders Planning using the CLI Run move2kube plan -s src to generate a plan for migrating the multiple components of the app to Kubernetes. $ move2kube plan -s src INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [CloudFoundry] Planning transformation INFO[0000] Identified 3 named services and 0 to-be-named services INFO[0000] [CloudFoundry] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [Base Directory] Identified 3 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in config-utils INFO[0000] Identified 1 named services and 0 to-be-named services in customers INFO[0000] Identified 1 named services and 0 to-be-named services in frontend INFO[0000] Identified 1 named services and 0 to-be-named services in gateway INFO[0000] Identified 1 named services and 0 to-be-named services in inventory INFO[0000] Identified 1 named services and 0 to-be-named services in orders INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 6 named services and 0 to-be-named services INFO[0000] [Named Services] Identified 6 named services INFO[0000] No of services identified : 6 INFO[0000] Plan can be found at [/Users/user/Desktop/tutorial/m2k.plan]. Look at the plan file that was generated. $ cat m2k.plan apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan ...... apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: src services: config-utils: Click to see the rest of the yaml. - transformerName: Maven paths: MavenPom: - config-utils/pom.xml ServiceDirPath: - config-utils configs: Maven: mavenAppName: config-utils artifactType: jar mvnwPresent: false customers: - transformerName: Maven paths: MavenPom: - customers/pom.xml ServiceDirPath: - customers configs: Maven: mavenAppName: customers artifactType: war mavenProfiles: - dev-inmemorydb - prod-externaldb mvnwPresent: true SpringBoot: springBootVersion: 2.5.0 springBootProfiles: - dev-inmemorydb - prod-externaldb frontend: - transformerName: CloudFoundry paths: CfManifest: - frontend/manifest.yml ServiceDirPath: - frontend configs: CloudFoundryService: serviceName: frontend ContainerizationOptions: - Nodejs-Dockerfile - transformerName: Nodejs-Dockerfile paths: ServiceDirPath: - frontend gateway: - transformerName: CloudFoundry paths: BuildArtifact: - gateway/target/ROOT.jar CfManifest: - gateway/manifest.yml ServiceDirPath: - gateway configs: CloudFoundryService: serviceName: gateway ContainerizationOptions: - Maven - transformerName: Maven paths: MavenPom: - gateway/pom.xml ServiceDirPath: - gateway configs: Maven: mavenAppName: gateway artifactType: jar mavenProfiles: - dev - prod mvnwPresent: true SpringBoot: springBootAppName: gateway springBootProfiles: - dev - prod inventory: - transformerName: Maven paths: MavenPom: - inventory/pom.xml ServiceDirPath: - inventory configs: Maven: mavenAppName: inventory artifactType: jar mavenProfiles: - dev-inmemorydb - prod-externaldb mvnwPresent: true SpringBoot: springBootProfiles: - dev-inmemorydb - prod-externaldb orders: - transformerName: CloudFoundry paths: BuildArtifact: - orders/target/ROOT.jar CfManifest: - orders/manifest.yml ServiceDirPath: - orders configs: CloudFoundryService: serviceName: orders ContainerizationOptions: - Maven - transformerName: Maven paths: MavenPom: - orders/pom.xml ServiceDirPath: - orders configs: Maven: mavenAppName: orders artifactType: jar mavenProfiles: - dev-inmemorydb - prod-externaldb mvnwPresent: true SpringBoot: springBootAppName: orders springBootProfiles: - dev-inmemorydb - prod-externaldb transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/transformer.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/transformer.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/transformer.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/transformer.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/transformer.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimagespushscript/transformer.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/transformer.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/transformer.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/transformer.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/transformer.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/transformer.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/transformer.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/transformer.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/transformer.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/transformer.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/transformer.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/transformer.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/transformer.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/transformer.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/transformer.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/transformer.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/transformer.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/transformer.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/transformer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/transformer.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/transformer.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/transformer.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/transformer.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/transformer.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/transformer.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/transformer.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/transformer.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/transformer.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/transformer.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/transformer.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/transformer.yaml The plan file contains all the transformers that Move2Kube detected and ran. These transformers will be run again during the transformation phase.\nThe plan file also contains all the services that Move2Kube was able to detect. The service name comes from the transformer that detected that service. We can edit this plan before we move on to the transformation phase. For now we will leave it as is.\nNext step: Transform\nPlanning using the UI Open the UI: $ docker run --rm -it -p 8080:8080 quay.io/konveyor/move2kube-ui:v0.3.1 INFO[0000] Starting Move2Kube API server at port: 8080 Create a new workspace. Create a new project. Scroll down to the project inputs section and then upload the source directory and the collected information zip files. Optional: If you have collected cloud foundry runtime metadata using the move2kube collect command you can create a zip file and upload that as well. Make sure to upload it as sources.\nScroll down to the planning section and click Start Planning. Note Planning takes a few minutes.\nNow that we have generated a plan file we can move on to the Transform phase which generates the output we need to deploy our app to Kubernetes.\n"},{"uri":"http://konveyor.github.io/forklift/","title":"Forklift","tags":[],"description":"","content":"Forklift Use this section to better understand and use the Konveyor Forklift tool.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/cfappstok8/3transform/","title":"3. Transform","tags":[],"description":"","content":"Now we can run the transformation according to the plan file generated in the previous step. The transformation phase runs all of the transformers again, but this time the transformers will use the plan to generate the output files.\nDuring this process, the transformers might run into situations where it requires some more information to generate the output. In order to get this information, it will ask the user some questions. The questions range from yes/no, to multiple choice, to string input. Most questions will have a default answer.\nExample: Some of the questions Move2Kube will ask is about the type of container registry where you want to push the images to. It also needs to know the registry namespace and any authentication necessary for pulling images from that registry.\nIf you want to skip the QA, use the --qa-skip flag to accept the default answers. However rather than skipping the questions, use a config file that contains all of the answers using the --config flag.\nAfter the transformation is finished, all the answers are written to a config file called m2kconfig.yaml which can be used for later transformations.\nThe transformation phase produces all the necessary output files including the Dockerfiles, build scripts for containerizing various services and Kubernetes deployment, and the service and ingress YAMLs necessary for deploying our application to a Kubernetes cluster.\nMove2Kube also generates the CI/CD pipeline and parameterized versions of all the Kubernetes YAMLs (Helm chart, Kustomize YAMLs, Openshift templates, etc.) for various environments (dev, staging, prod, etc.).\nPrerequisites Perform the Plan step before this procedure. Transforming using the CLI Run the transformation in the same directory as the plan file. This will detect the plan file and use it to find the source directory. $ move2kube transform Optional: Provide answers to questions using a config file... If you want to avoid the question answers during transformation, you can use this config file\n$ move2kube transform --config m2kconfig.yaml Answer all the questions as appropriate. For most questions accept the default answers. Some questions to watch out for are: A spurious service called config-utils was detected by one of the transformers. We can deselect it when we are asked to select the services we are interested in or by editing the plan file. Move2Kube has detected the Maven profiles for each of the Java services. If you are deploying to MiniKube, select the dev-inmemorydb profile. Similar questions for the SpringBoot profiles. The container registry and namespace that you want to use. A container registry is where all the images are stored (Example: Quay, Docker Hub, etc.). The ingress hostname and ingress TLS secret. If you are deploying to MiniKube then give localhost as the ingress host and leave the TLS secret blank. We will select ClusterIP to only expose the order customers inventory and gateway services inside the cluster. We will choose Ingress and / as the path to expose the frontend service. This way only the frontend will be exposed outside the cluster through the ingress. $ move2kube transform INFO[0000] Detected a plan file at path /Users/user/Desktop/tutorial/m2k.plan. Will transform using this plan. INFO[0000] Starting Plan Transformation ? Select all transformer types that you are interested in: ID: move2kube.transformers.types Hints: [Services that don\u0026#39;t support any of the transformer types you are interested in will be ignored.] Buildconfig, CloudFoundry, ClusterSelector, ComposeAnalyser, ComposeGenerator, ContainerImagesPushScriptGenerator, DockerfileDetector, DockerfileImageBuildScript, DockerfileParser, DotNetCore-Dockerfile,EarAnalyser, EarRouter, Golang-Dockerfile, Gradle, Jar, Jboss, Knative, Kubernetes, KubernetesVersionChanger, Liberty, Maven, Nodejs-Dockerfile, PHP-Dockerfile, Parameterizer, Python-Dockerfile, ReadMeGenerator,Ruby-Dockerfile, Rust-Dockerfile, Tekton, Tomcat, WarAnalyser, WarRouter, WinConsoleApp-Dockerfile, WinSLWebApp-Dockerfile, WinWebApp-Dockerfile, ZuulAnalyser ? Select all services that are needed: ID: move2kube.services.[].enable Hints: [The services unselected here will be ignored.] customers, frontend, gateway, inventory, orders INFO[0005] Iteration 1 INFO[0005] Iteration 2 - 5 artifacts to process INFO[0005] Transformer CloudFoundry processing 3 artifacts INFO[0005] Transformer CloudFoundry Done INFO[0005] Transformer Maven processing 2 artifacts Click to see the remaining transform questions. ? Choose the Maven profile to be used for the service customers ID: move2kube.services.customers.activemavenprofiles Hints: [Selected Maven profiles will be used for setting configuration for the service customers] prod-externaldb ? Choose Springboot profiles to be used for the service customers ID: move2kube.services.customers.activespringbootprofiles Hints: [Selected Springboot profiles will be used for setting configuration for the service customers] prod-externaldb ? Choose the Maven profile to be used for the service inventory ID: move2kube.services.inventory.activemavenprofiles Hints: [Selected Maven profiles will be used for setting configuration for the service inventory] prod-externaldb ? Choose Springboot profiles to be used for the service inventory ID: move2kube.services.inventory.activespringbootprofiles Hints: [Selected Springboot profiles will be used for setting configuration for the service inventory] prod-externaldb ? Select port to be exposed for the service inventory : ID: move2kube.services.inventory.port Hints: [Select Other if you want to expose the service inventory to some other port] 8080 INFO[0010] Transformer WarRouter processing 2 artifacts ? Select the transformer to use for service customers ID: move2kube.services.customers.wartransformer Tomcat INFO[0012] Transformer WarRouter Done INFO[0012] Transformer Maven Done INFO[0012] Created 2 pathMappings and 6 artifacts. Total Path Mappings : 2. Total Artifacts : 5. INFO[0012] Iteration 3 - 6 artifacts to process INFO[0012] Transformer Jar processing 1 artifacts INFO[0012] Transformer Jar Done INFO[0012] Transformer Maven processing 2 artifacts ? Choose the Maven profile to be used for the service gateway ID: move2kube.services.gateway.activemavenprofiles Hints: [Selected Maven profiles will be used for setting configuration for the service gateway] prod ? Choose Springboot profiles to be used for the service gateway ID: move2kube.services.gateway.activespringbootprofiles Hints: [Selected Springboot profiles will be used for setting configuration for the service gateway] prod ? Select port to be exposed for the service gateway : ID: move2kube.services.gateway.port Hints: [Select Other if you want to expose the service gateway to some other port] 8080 ? Choose the Maven profile to be used for the service orders ID: move2kube.services.orders.activemavenprofiles Hints: [Selected Maven profiles will be used for setting configuration for the service orders] prod-externaldb ? Choose Springboot profiles to be used for the service orders ID: move2kube.services.orders.activespringbootprofiles Hints: [Selected Springboot profiles will be used for setting configuration for the service orders] prod-externaldb ? Select port to be exposed for the service orders : ID: move2kube.services.orders.port Hints: [Select Other if you want to expose the service orders to some other port] 8080 INFO[0018] Transformer Maven Done INFO[0018] Transformer Nodejs-Dockerfile processing 1 artifacts ? Enter the port to be exposed for the service frontend: ID: move2kube.services.frontend.port Hints: [The service frontend will be exposed to the specified port] 8080 INFO[0021] Transformer Nodejs-Dockerfile Done INFO[0021] Transformer Tomcat processing 2 artifacts INFO[0021] Transformer Tomcat Done INFO[0021] Created 10 pathMappings and 10 artifacts. Total Path Mappings : 12. Total Artifacts : 11. INFO[0021] Iteration 4 - 10 artifacts to process INFO[0021] Transformer DockerfileImageBuildScript processing 4 artifacts ? Select the container runtime to use : ID: move2kube.containerruntime Hints: [The container runtime selected will be used in the scripts] docker INFO[0022] Transformer DockerfileImageBuildScript Done INFO[0022] Transformer DockerfileParser processing 4 artifacts INFO[0022] Transformer ZuulAnalyser processing 2 artifacts INFO[0022] Transformer ZuulAnalyser Done INFO[0022] Transformer DockerfileParser Done INFO[0022] Transformer Jar processing 2 artifacts INFO[0022] Transformer Jar Done INFO[0022] Created 5 pathMappings and 10 artifacts. Total Path Mappings : 17. Total Artifacts : 21. INFO[0022] Iteration 5 - 10 artifacts to process INFO[0022] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: ID: move2kube.target.clustertype Hints: [Choose the cluster type you would like to target] Kubernetes INFO[0024] Transformer ClusterSelector Done INFO[0024] Transformer Buildconfig processing 2 artifacts ? What kind of service/ingress to create for inventory\u0026#39;s 8080 port? ID: move2kube.services.\u0026#34;inventory\u0026#34;.\u0026#34;8080\u0026#34;.servicetype Hints: [Choose Ingress if you want a ingress/route resource to be created] ClusterIP ? What kind of service/ingress to create for frontend\u0026#39;s 8080 port? ID: move2kube.services.\u0026#34;frontend\u0026#34;.\u0026#34;8080\u0026#34;.servicetype Hints: [Choose Ingress if you want a ingress/route resource to be created] Ingress ? Specify the ingress path to expose frontend\u0026#39;s 8080 port? ID: move2kube.services.\u0026#34;frontend\u0026#34;.\u0026#34;8080\u0026#34;.urlpath Hints: [Leave out leading / to use first part as subdomain] / ? What kind of service/ingress to create for customers\u0026#39;s 8080 port? ID: move2kube.services.\u0026#34;customers\u0026#34;.\u0026#34;8080\u0026#34;.servicetype Hints: [Choose Ingress if you want a ingress/route resource to be created] ClusterIP ? Provide the minimum number of replicas each service should have ID: move2kube.minreplicas Hints: [If the value is 0 pods won\u0026#39;t be started by default] 2 ? Enter the URL of the image registry : ID: move2kube.target.imageregistry.url Hints: [You can always change it later by changing the yamls.] quay.io ? Enter the namespace where the new images should be pushed : ID: move2kube.target.imageregistry.namespace Hints: [Ex : myproject] move2kube ? [quay.io] What type of container registry login do you want to use? ID: move2kube.target.imageregistry.logintype Hints: [Docker login from config mode, will use the default config from your local machine.] No authentication INFO[0051] Transformer Buildconfig Done INFO[0051] Transformer ComposeGenerator processing 2 artifacts INFO[0051] Transformer ComposeGenerator Done INFO[0051] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[0051] Transformer ContainerImagesPushScriptGenerator Done INFO[0051] Transformer DockerfileImageBuildScript processing 3 artifacts INFO[0051] Transformer DockerfileImageBuildScript Done INFO[0051] Transformer DockerfileParser processing 2 artifacts INFO[0051] Transformer ZuulAnalyser processing 2 artifacts INFO[0051] Transformer ZuulAnalyser Done INFO[0051] Transformer DockerfileParser Done INFO[0051] Transformer ClusterSelector processing 2 artifacts INFO[0051] Transformer ClusterSelector Done INFO[0051] Transformer Knative processing 2 artifacts INFO[0051] Transformer Knative Done INFO[0051] Transformer ClusterSelector processing 2 artifacts INFO[0051] Transformer ClusterSelector Done INFO[0051] Transformer Kubernetes processing 2 artifacts ? Provide the ingress host domain ID: move2kube.target.ingress.host Hints: [Ingress host domain is part of service URL] localhost ? Provide the TLS secret for ingress ID: move2kube.target.ingress.tls Hints: [Leave empty to use http] INFO[0058] Transformer Kubernetes Done INFO[0058] Transformer ClusterSelector processing 2 artifacts INFO[0058] Transformer ClusterSelector Done INFO[0058] Transformer Tekton processing 2 artifacts INFO[0059] Transformer Tekton Done INFO[0059] Created 32 pathMappings and 15 artifacts. Total Path Mappings : 49. Total Artifacts : 31. INFO[0059] Iteration 6 - 15 artifacts to process INFO[0059] Transformer ClusterSelector processing 2 artifacts INFO[0059] Transformer ClusterSelector Done INFO[0059] Transformer Buildconfig processing 2 artifacts ? What kind of service/ingress to create for orders\u0026#39;s 8080 port? ID: move2kube.services.\u0026#34;orders\u0026#34;.\u0026#34;8080\u0026#34;.servicetype Hints: [Choose Ingress if you want a ingress/route resource to be created] ClusterIP ? What kind of service/ingress to create for gateway\u0026#39;s 8080 port? ID: move2kube.services.\u0026#34;gateway\u0026#34;.\u0026#34;8080\u0026#34;.servicetype Hints: [Choose Ingress if you want a ingress/route resource to be created] ClusterIP INFO[0066] Transformer Buildconfig Done INFO[0066] Transformer ComposeGenerator processing 2 artifacts INFO[0066] Transformer ComposeGenerator Done INFO[0066] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[0066] Transformer ContainerImagesPushScriptGenerator Done INFO[0066] Transformer ClusterSelector processing 2 artifacts INFO[0067] Transformer ClusterSelector Done INFO[0067] Transformer Knative processing 2 artifacts INFO[0067] Transformer Knative Done INFO[0067] Transformer ClusterSelector processing 2 artifacts INFO[0067] Transformer ClusterSelector Done INFO[0067] Transformer Kubernetes processing 2 artifacts INFO[0067] Transformer Kubernetes Done INFO[0067] Transformer Parameterizer processing 4 artifacts INFO[0067] Transformer Parameterizer Done INFO[0067] Transformer ReadMeGenerator processing 5 artifacts INFO[0067] Transformer ReadMeGenerator Done INFO[0067] Transformer ClusterSelector processing 2 artifacts INFO[0067] Transformer ClusterSelector Done INFO[0067] Transformer Tekton processing 2 artifacts INFO[0068] Transformer Tekton Done INFO[0068] Created 52 pathMappings and 7 artifacts. Total Path Mappings : 101. Total Artifacts : 46. INFO[0068] Iteration 7 - 7 artifacts to process INFO[0068] Transformer Parameterizer processing 4 artifacts INFO[0068] Transformer Parameterizer Done INFO[0068] Transformer ReadMeGenerator processing 5 artifacts INFO[0068] Transformer ReadMeGenerator Done INFO[0069] Plan Transformation done INFO[0069] Transformed target artifacts can be found at [/Users/user/Desktop/tutorial/myproject]. Transforming using the UI Continue from the previous step in the UI.\nScroll down from the Plan section Outputs section. spec: sourceDir:sources services: config-utils: -transformerName: Maven paths: MavenPom: -src/src/config-utils/pom.xml ServiceDirPath: -src/src/config-utils configs: Maven: mavenAppName: config-utils artifactType: jar customers-tomcat: - transformerName: Maven Click the Start transformation button. A form to ask the user questions to guide the transformation opens.\nAnswer all the questions as appropriate. For most questions we can go with the default answers. Some questions to watch out for are: A spurious service called config-utils was detected by one of the transformers. We can deselect it when we are asked to select the services we are interested in or by editing the plan file. Move2Kube has detected the Maven profiles for each of the Java services. Since we are deploying to a cluster (like MiniKube), select the prod-externaldb profile. There are similar questions for the SpringBoot profiles. The container registry and namespace that you want to use. A container registry is where all the images are stored (Example: Quay, Docker Hub, etc.) The ingress hostname and ingress TLS secret. If you are deploying to MiniKube, use localhost as the ingress host and leave the TLS secret blank. We will select ClusterIP to only expose the order customers inventory and gateway services inside the cluster. Choose Ingress and / as the path to expose the frontend service. This way only the frontend will be exposed outside the cluster through the Ingress. Click the Next button to continue going through the questions and finally to run the tranformation. Move2Kube processes the transformation and the output appears.\nClick the output ID link to download. Using the output generated by Move2Kube transform For a sample output of what Move2Kube generates for this enterprise app, see this\nNow that we have generated the output, we can run the scripts inside the scripts directory.\nRun the builddockerimages.sh script to build all the container images for each service using the Dockerfiles that were generated. $ cd myproject/scripts/ $ ./builddockerimages.sh Run the pushimages.sh script to push them to the specified container registry. $ ./pushimages.sh Because we selected the prod-externaldb profile, deploy the database using the yamls given here $ cd .. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/database -r move2kube-demos $ minikube start --memory 8192 --cpus 2 # do this only if you are deploying to Minikube $ kubectl apply -f database/ Deploy the Kubernetes yamls that Move2Kube generated to our cluster $ kubectl apply -f deploy/yamls The application is now running on the cluster.\nGet the URL where the app has been deployed to, using kubectl get ingress myproject -o yaml Note: If you deployed to Minikube, enable the ingress addon and start minikube tunnel to access the ingress on localhost.\n$ minikube addons enable ingress  After the addon is enabled, please run \u0026#34;minikube tunnel\u0026#34; and your ingress resources would be available at \u0026#34;127.0.0.1\u0026#34;  Using image k8s.gcr.io/ingress-nginx/controller:v1.0.4  Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1  Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1  Verifying ingress addon...  The \u0026#39;ingress\u0026#39; addon is enabled $ minikube addons enable ingress-dns  After the addon is enabled, please run \u0026#34;minikube tunnel\u0026#34; and your ingress resources would be available at \u0026#34;127.0.0.1\u0026#34;  Using image gcr.io/k8s-minikube/minikube-ingress-dns:0.0.2  The \u0026#39;ingress-dns\u0026#39; addon is enabled $ minikube tunnel  The service/ingress myproject requires privileged ports to be exposed: [80 443]  sudo permission will be asked for it.  Starting tunnel for service myproject. Password: The app is now available on http://localhost.\nOptional: As part of the transformation, if Cloud Foundry runtime information is required, use the collect output in planning and transformation: Collect information from running apps.\nCustomizing the output After inspecting the output that Move2Kube produced you might see some things you want to change. For example, you might want to change the base image used in the Dockerfiles, add some annotations to the Ingress YAML, maybe change the output directory structure, change which values are parameterized in the Helm chart, generate some new files, etc. For all these user specific requirements and more, use customizations.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/usingcli/","title":"Using Move2Kube CLI","tags":[],"description":"","content":"In this tutorial we will see how we can transform a set of sample applications to run on Kubernetes. We will use the Move2Kube CLI tool to generate the Kubernetes YAMLs, Dockerfiles, and build scripts for each application. We will then build the container images and deploy them to a cluster.\nPrerequisites Install the Move2Kube CLI tool.\nWe will use language-platforms sample. The language-platforms directory has a combination of multiple applications in different languages (Java, Go, Python, Ruby, etc.) which need to be containerized and deployed to Kubernetes.\nUsing the CLI to perform a transformation Download the language platforms sample. Each directory contains a simple web application written in different languages. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/language-platforms -r move2kube-demos $ ls language-platforms django\tgolang\tjava-gradle\tjava-gradle-war\tjava-maven\tjava-maven-war\tnodejs\tphp\tpython\truby\trust Run move2kube plan -s language-platforms to generate a plan file. The -s flag specifies a source directory as language-platforms because we want Move2Kube to analyze the source code inside the language-platforms directory and come up with a plan for transforming them to Kubernetes YAMLs. $ move2kube plan -s language-platforms INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [CloudFoundry] Planning transformation INFO[0000] [CloudFoundry] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [Base Directory] Identified 0 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done Click to see the remaining output. INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in django INFO[0000] Identified 1 named services and 0 to-be-named services in golang INFO[0000] Identified 1 named services and 0 to-be-named services in java-gradle INFO[0000] Identified 1 named services and 0 to-be-named services in java-gradle-war INFO[0000] Identified 1 named services and 0 to-be-named services in java-maven INFO[0000] Identified 1 named services and 0 to-be-named services in java-maven-war INFO[0000] Identified 1 named services and 0 to-be-named services in nodejs INFO[0000] Identified 1 named services and 0 to-be-named services in php INFO[0000] Identified 1 named services and 0 to-be-named services in python INFO[0000] Identified 1 named services and 0 to-be-named services in ruby INFO[0000] Identified 1 named services and 0 to-be-named services in rust INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 6 named services and 5 to-be-named services INFO[0000] [Named Services] Identified 11 named services INFO[0000] No of services identified : 11 INFO[0000] Plan can be found at [/Users/user/Desktop/tutorial/m2k.plan]. Look at the plan file we generated in YAML format. Notice Move2Kube has detected all the different services, one for each web app. $ ls language-platforms\tlanguage-platforms.zip\tm2k.plan $ cat m2k.plan apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: language-platforms services: Click to see the rest of the yaml. golang: - transformerName: Golang-Dockerfile paths: GoModFilePath: - golang/go.mod ServiceDirPath: - golang myproject-django: - transformerName: Python-Dockerfile paths: MainPythonFilesPathType: [] PythonFilesPathType: - django/manage.py RequirementsTxtPathType: - django/requirements.txt ServiceDirPath: - django configs: PythonConfig: IsDjango: true myproject-java-gradle: - transformerName: Gradle paths: GradleBuildFile: - java-gradle/build.gradle ServiceDirPath: - java-gradle configs: Gradle: artifactType: jar myproject-java-gradle-war: - transformerName: WarAnalyser paths: ServiceDirPath: - java-gradle-war War: - java-gradle-war/java-gradle-war.war configs: War: deploymentFile: java-gradle-war.war javaVersion: \u0026#34;\u0026#34; buildContainerName: \u0026#34;\u0026#34; deploymentFileDirInBuildContainer: \u0026#34;\u0026#34; envVariables: {} myproject-java-maven-war: - transformerName: WarAnalyser paths: ServiceDirPath: - java-maven-war War: - java-maven-war/java-maven-war.war configs: War: deploymentFile: java-maven-war.war javaVersion: \u0026#34;\u0026#34; buildContainerName: \u0026#34;\u0026#34; deploymentFileDirInBuildContainer: \u0026#34;\u0026#34; envVariables: {} myproject-php: - transformerName: PHP-Dockerfile paths: ServiceDirPath: - php myproject-python: - transformerName: Python-Dockerfile paths: MainPythonFilesPathType: [] PythonFilesPathType: - python/main.py RequirementsTxtPathType: - python/requirements.txt ServiceDirPath: - python configs: PythonConfig: IsDjango: false nodejs: - transformerName: Nodejs-Dockerfile paths: ServiceDirPath: - nodejs ruby: - transformerName: Ruby-Dockerfile paths: ServiceDirPath: - ruby rust: - transformerName: Rust-Dockerfile paths: ServiceDirPath: - rust simplewebapp: - transformerName: Maven paths: MavenPom: - java-maven/pom.xml ServiceDirPath: - java-maven configs: Maven: mavenAppName: simplewebapp artifactType: war transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/buildconfig.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/cloudfoundry.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/clusterselector.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/composeanalyser.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/composegenerator.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimage/containerimagespushscript/containerimagespushscript.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/dockerfiledetector.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/dockerfilebuildscriptgenerator.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/dockerfileparser.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/dotnetcore.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/ear.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/earrouter.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/golang.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/gradle.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/jar.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/jboss.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/knative.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/kubernetes.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/kubernetesversionchanger.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/liberty.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/maven.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/nodejs.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/php.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/parameterizer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/python.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/readmegenerator.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/ruby.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/rust.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/tekton.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/tomcat.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/war.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/warrouter.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/winconsole.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/winsilverlightweb.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/winweb.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/zuulanalyser.yaml Run the transformation using move2kube transform to perform the transformation according to the generated plan. By default Move2Kube looks for a plan file in the current directory. If you want to specify the path to a different plan file you can do so using the -p flag. During transformation Move2Kube will ask several questions to help guide the transformation process. For most questions we can go with the default answers. Some questions to watch out for are:\nThe container registry and namespace that you want to use. A container registry is where all the images are stored (Example: Quay, Docker Hub, etc.). The ingress hostname and ingress TLS secret. If you are deploying to MiniKube then give localhost as the ingress host and leave the TLS secret blank. For all other questions accept the default answers by pressing Enter for each.\n$ move2kube transform INFO[0000] Detected a plan file at path /Users/user/Desktop/tutorial/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: Hints: [Services that don\u0026#39;t support any of the transformer types you are interested in will be ignored.] ComposeGenerator, DockerfileDetector, Jboss, WinSLWebApp-Dockerfile, ZuulAnalyser, Buildconfig, Maven, Tekton, Tomcat, WarRouter, WinConsoleApp-Dockerfile,DotNetCore-Dockerfile, EarAnalyser, KubernetesVersionChanger, Nodejs-Dockerfile, Ruby-Dockerfile, WinWebApp-Dockerfile, CloudFoundry, ComposeAnalyser, DockerfileParser, EarRouter, Gradle, ClusterSelector, Rust-Dockerfile, ContainerImagesPushScriptGenerator, ReadMeGenerator, WarAnalyser, Jar, Golang-Dockerfile, Knative, Kubernetes, Liberty, PHP-Dockerfile, Parameterizer, Python-Dockerfile, DockerfileImageBuildScript ? Select all services that are needed: Hints: [The services unselected here will be ignored.] golang, myproject-python, nodejs, rust, simplewebapp, myproject-django, myproject-java-gradle, myproject-java-gradle-war, myproject-java-maven-war, myproject-php, ruby INFO[0009] Starting Plan Transformation Click to see the remaining questions and output. INFO[0009] Iteration 1 INFO[0009] Iteration 2 - 11 artifacts to process INFO[0009] Transformer Maven processing 1 artifacts INFO[0009] Transformer WarRouter processing 2 artifacts ? Select the transformer to use for service simplewebapp Tomcat INFO[0014] Transformer WarRouter Done INFO[0014] Transformer Maven Done INFO[0014] Transformer PHP-Dockerfile processing 1 artifacts INFO[0014] Transformer PHP-Dockerfile Done INFO[0014] Transformer Nodejs-Dockerfile processing 1 artifacts ? Enter the port to be exposed for the service nodejs: Hints: [The service nodejs will be exposed to the specified port] 8080 INFO[0016] Transformer Nodejs-Dockerfile Done INFO[0016] Transformer Ruby-Dockerfile processing 1 artifacts ? Select port to be exposed for the service ruby : Hints: [Select Other if you want to expose the service ruby to some other port] 8080 INFO[0017] Transformer Ruby-Dockerfile Done INFO[0017] Transformer WarAnalyser processing 2 artifacts INFO[0017] Transformer WarRouter processing 3 artifacts ? Select the transformer to use for service myproject-java-gradle-war Tomcat ? Select the transformer to use for service myproject-java-maven-war Tomcat INFO[0020] Transformer WarRouter Done INFO[0020] Transformer WarAnalyser Done INFO[0020] Transformer Golang-Dockerfile processing 1 artifacts ? Select ports to be exposed for the service golang : Hints: [Select Other if you want to add more ports] 8080 INFO[0021] Transformer Golang-Dockerfile Done INFO[0021] Transformer Gradle processing 1 artifacts ? Select port to be exposed for the service myproject-java-gradle : Hints: [Select Other if you want to expose the service myproject-java-gradle to some other port] 8080 INFO[0022] Transformer Gradle Done INFO[0022] Transformer Rust-Dockerfile processing 1 artifacts ? Select port to be exposed for the service rust : Hints: [Select Other if you want to expose the service rust to some other port] 8085 INFO[0023] Transformer Rust-Dockerfile Done INFO[0023] Transformer Python-Dockerfile processing 2 artifacts ? Select port to be exposed for the service myproject-django : Hints: [Select Other if you want to expose the service myproject-django to some other port] 8080 ? Select port to be exposed for the service myproject-python : Hints: [Select Other if you want to expose the service myproject-python to some other port] 8080 INFO[0024] Transformer Python-Dockerfile Done INFO[0024] Created 16 pathMappings and 20 artifacts. Total Path Mappings : 16. Total Artifacts : 11. INFO[0024] Iteration 3 - 20 artifacts to process INFO[0024] Transformer DockerfileImageBuildScript processing 8 artifacts ? Select the container runtime to use : Hints: [The container runtime selected will be used in the scripts] docker INFO[0028] Transformer DockerfileImageBuildScript Done INFO[0028] Transformer Jar processing 1 artifacts INFO[0028] Transformer Jar Done INFO[0028] Transformer DockerfileParser processing 7 artifacts INFO[0028] Transformer ZuulAnalyser processing 2 artifacts INFO[0028] Transformer ZuulAnalyser Done INFO[0028] Transformer DockerfileParser Done INFO[0028] Transformer Tomcat processing 4 artifacts INFO[0028] Transformer Tomcat Done INFO[0028] Created 11 pathMappings and 20 artifacts. Total Path Mappings : 27. Total Artifacts : 31. INFO[0028] Iteration 4 - 20 artifacts to process INFO[0028] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: Hints: [Choose the cluster type you would like to target] Kubernetes INFO[0030] Transformer ClusterSelector Done INFO[0030] Transformer Tekton processing 2 artifacts ? What URL/path should we expose the service rust\u0026#39;s 8085 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /rust ? What URL/path should we expose the service golang\u0026#39;s 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /golang ? What URL/path should we expose the service ruby\u0026#39;s 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /ruby ? What URL/path should we expose the service myproject-python\u0026#39;s 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-python ? What URL/path should we expose the service nodejs\u0026#39;s 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /nodejs ? What URL/path should we expose the service myproject-php\u0026#39;s 8082 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-php ? What URL/path should we expose the service myproject-django\u0026#39;s 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-django ? Provide the minimum number of replicas each service should have Hints: [If the value is 0 pods won\u0026#39;t be started by default] 2 ? Enter the URL of the image registry : Hints: [You can always change it later by changing the yamls.] quay.io ? Enter the namespace where the new images should be pushed : Hints: [Ex : myproject] move2kube ? [quay.io] What type of container registry login do you want to use? Hints: [Docker login from config mode, will use the default config from your local machine.] No authentication ? Provide the ingress host domain Hints: [Ingress host domain is part of service URL] localhost ? Provide the TLS secret for ingress Hints: [Leave empty to use http] INFO[0049] Transformer Tekton Done INFO[0049] Transformer ClusterSelector processing 2 artifacts INFO[0049] Transformer ClusterSelector Done INFO[0049] Transformer Knative processing 2 artifacts INFO[0050] Transformer Knative Done INFO[0050] Transformer ComposeGenerator processing 2 artifacts INFO[0050] Transformer ComposeGenerator Done INFO[0050] Transformer ClusterSelector processing 2 artifacts INFO[0050] Transformer ClusterSelector Done INFO[0050] Transformer Kubernetes processing 2 artifacts INFO[0050] Transformer Kubernetes Done INFO[0050] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[0050] Transformer ContainerImagesPushScriptGenerator Done INFO[0050] Transformer DockerfileImageBuildScript processing 5 artifacts INFO[0050] Transformer DockerfileImageBuildScript Done INFO[0050] Transformer DockerfileParser processing 5 artifacts INFO[0050] Transformer ZuulAnalyser processing 2 artifacts INFO[0050] Transformer ZuulAnalyser Done INFO[0050] Transformer DockerfileParser Done INFO[0050] Transformer ClusterSelector processing 2 artifacts INFO[0050] Transformer ClusterSelector Done INFO[0050] Transformer Buildconfig processing 2 artifacts INFO[0050] Transformer Buildconfig Done INFO[0050] Created 40 pathMappings and 21 artifacts. Total Path Mappings : 67. Total Artifacts : 51. INFO[0050] Iteration 5 - 21 artifacts to process INFO[0050] Transformer ClusterSelector processing 2 artifacts INFO[0050] Transformer ClusterSelector Done INFO[0050] Transformer Buildconfig processing 2 artifacts ? What URL/path should we expose the service simplewebapp\u0026#39;s 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /simplewebapp ? What URL/path should we expose the service myproject-java-gradle\u0026#39;s 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-java-gradle ? What URL/path should we expose the service myproject-java-gradle-war\u0026#39;s 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-java-gradle-war ? What URL/path should we expose the service myproject-java-maven-war\u0026#39;s 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-java-maven-war INFO[0054] Transformer Buildconfig Done INFO[0054] Transformer ReadMeGenerator processing 5 artifacts INFO[0054] Transformer ReadMeGenerator Done INFO[0054] Transformer ClusterSelector processing 2 artifacts INFO[0054] Transformer ClusterSelector Done INFO[0054] Transformer Knative processing 2 artifacts INFO[0055] Transformer Knative Done INFO[0055] Transformer ClusterSelector processing 2 artifacts INFO[0055] Transformer ClusterSelector Done INFO[0055] Transformer Tekton processing 2 artifacts INFO[0055] Transformer Tekton Done INFO[0055] Transformer ClusterSelector processing 2 artifacts INFO[0055] Transformer ClusterSelector Done INFO[0055] Transformer Kubernetes processing 2 artifacts INFO[0055] Transformer Kubernetes Done INFO[0055] Transformer ComposeGenerator processing 2 artifacts INFO[0055] Transformer ComposeGenerator Done INFO[0055] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[0055] Transformer ContainerImagesPushScriptGenerator Done INFO[0055] Transformer Parameterizer processing 4 artifacts INFO[0055] Transformer Parameterizer Done INFO[0056] Created 60 pathMappings and 7 artifacts. Total Path Mappings : 127. Total Artifacts : 72. INFO[0056] Iteration 6 - 7 artifacts to process INFO[0056] Transformer Parameterizer processing 4 artifacts INFO[0056] Transformer Parameterizer Done INFO[0056] Transformer ReadMeGenerator processing 5 artifacts INFO[0056] Transformer ReadMeGenerator Done INFO[0056] Plan Transformation done INFO[0056] Transformed target artifacts can be found at [/Users/user/Desktop/tutorial/myproject]. After the questions are finished wait a few minutes for it to finish processing. Once it\u0026rsquo;s done, we can see it has generated a directory called myproject. The name of the output directory is the same as the project name (by default myproject). The project name can be changed using the -n flag.\n$ ls language-platforms\tlanguage-platforms.zip\tm2k.plan\tm2kconfig.yaml\tm2kqacache.yaml\tmyproject $ ls myproject/ Readme.md\tdeploy\tscripts\tsource The applications can now be deployed to Kubernetes using these generated artifacts.\nDeploying the application to Kubernetes with the generated artifacts View the full structure of the output directory by executing the tree command. $ cd myproject/ $ tree $ cd myproject/ $ tree .  Readme.md  deploy   cicd    tekton     myproject-clone-build-push-pipeline.yaml     myproject-clone-push-serviceaccount.yaml     myproject-git-event-triggerbinding.yaml     myproject-git-repo-eventlistener.yaml     myproject-image-registry-secret.yaml     myproject-ingress.yaml     myproject-run-clone-build-push-triggertemplate.yaml     myproject-tekton-triggers-admin-role.yaml     myproject-tekton-triggers-admin-rolebinding.yaml     myproject-tekton-triggers-admin-serviceaccount.yaml    tekton-parameterized Click to see the rest of the tree.    helm-chart     myproject     Chart.yaml     templates     myproject-clone-build-push-pipeline.yaml     myproject-clone-push-serviceaccount.yaml     myproject-git-event-triggerbinding.yaml     myproject-git-repo-eventlistener.yaml     myproject-image-registry-secret.yaml     myproject-ingress.yaml     myproject-run-clone-build-push-triggertemplate.yaml     myproject-tekton-triggers-admin-role.yaml     myproject-tekton-triggers-admin-rolebinding.yaml     myproject-tekton-triggers-admin-serviceaccount.yaml    kustomize     base     kustomization.yaml     myproject-clone-build-push-pipeline.yaml     myproject-clone-push-serviceaccount.yaml     myproject-git-event-triggerbinding.yaml     myproject-git-repo-eventlistener.yaml     myproject-image-registry-secret.yaml     myproject-ingress.yaml     myproject-run-clone-build-push-triggertemplate.yaml     myproject-tekton-triggers-admin-role.yaml     myproject-tekton-triggers-admin-rolebinding.yaml     myproject-tekton-triggers-admin-serviceaccount.yaml    openshift-template    template.yaml   compose    docker-compose.yaml   knative    golang-service.yaml    myproject-django-service.yaml    myproject-java-gradle-service.yaml    myproject-java-gradle-war-service.yaml    myproject-java-maven-war-service.yaml    myproject-php-service.yaml    myproject-python-service.yaml    nodejs-service.yaml    ruby-service.yaml    rust-service.yaml    simplewebapp-service.yaml   knative-parameterized    helm-chart     myproject     Chart.yaml     templates     golang-service.yaml     myproject-django-service.yaml     myproject-java-gradle-service.yaml     myproject-java-gradle-war-service.yaml     myproject-java-maven-war-service.yaml     myproject-php-service.yaml     myproject-python-service.yaml     nodejs-service.yaml     ruby-service.yaml     rust-service.yaml     simplewebapp-service.yaml    kustomize     base     golang-service.yaml     kustomization.yaml     myproject-django-service.yaml     myproject-java-gradle-service.yaml     myproject-java-gradle-war-service.yaml     myproject-java-maven-war-service.yaml     myproject-php-service.yaml     myproject-python-service.yaml     nodejs-service.yaml     ruby-service.yaml     rust-service.yaml     simplewebapp-service.yaml    openshift-template    template.yaml   yamls    golang-deployment.yaml    golang-service.yaml    myproject-django-deployment.yaml    myproject-django-service.yaml    myproject-ingress.yaml    myproject-java-gradle-deployment.yaml    myproject-java-gradle-service.yaml    myproject-java-gradle-war-deployment.yaml    myproject-java-gradle-war-service.yaml    myproject-java-maven-war-deployment.yaml    myproject-java-maven-war-service.yaml    myproject-php-deployment.yaml    myproject-php-service.yaml    myproject-python-deployment.yaml    myproject-python-service.yaml    nodejs-deployment.yaml    nodejs-service.yaml    ruby-deployment.yaml    ruby-service.yaml    rust-deployment.yaml    rust-service.yaml    simplewebapp-deployment.yaml    simplewebapp-service.yaml   yamls-parameterized   helm-chart    myproject    Chart.yaml    templates     golang-deployment.yaml     golang-service.yaml     myproject-django-deployment.yaml     myproject-django-service.yaml     myproject-ingress.yaml     myproject-java-gradle-deployment.yaml     myproject-java-gradle-service.yaml     myproject-java-gradle-war-deployment.yaml     myproject-java-gradle-war-service.yaml     myproject-java-maven-war-deployment.yaml     myproject-java-maven-war-service.yaml     myproject-php-deployment.yaml     myproject-php-service.yaml     myproject-python-deployment.yaml     myproject-python-service.yaml     nodejs-deployment.yaml     nodejs-service.yaml     ruby-deployment.yaml     ruby-service.yaml     rust-deployment.yaml     rust-service.yaml     simplewebapp-deployment.yaml     simplewebapp-service.yaml    values-dev.yaml    values-prod.yaml    values-staging.yaml   kustomize    base     golang-deployment.yaml     golang-service.yaml     kustomization.yaml     myproject-django-deployment.yaml     myproject-django-service.yaml     myproject-ingress.yaml     myproject-java-gradle-deployment.yaml     myproject-java-gradle-service.yaml     myproject-java-gradle-war-deployment.yaml     myproject-java-gradle-war-service.yaml     myproject-java-maven-war-deployment.yaml     myproject-java-maven-war-service.yaml     myproject-php-deployment.yaml     myproject-php-service.yaml     myproject-python-deployment.yaml     myproject-python-service.yaml     nodejs-deployment.yaml     nodejs-service.yaml     ruby-deployment.yaml     ruby-service.yaml     rust-deployment.yaml     rust-service.yaml     simplewebapp-deployment.yaml     simplewebapp-service.yaml    overlays    dev     apps-v1-deployment-golang.yaml     apps-v1-deployment-myproject-django.yaml     apps-v1-deployment-myproject-java-gradle-war.yaml     apps-v1-deployment-myproject-java-gradle.yaml     apps-v1-deployment-myproject-java-maven-war.yaml     apps-v1-deployment-myproject-php.yaml     apps-v1-deployment-myproject-python.yaml     apps-v1-deployment-nodejs.yaml     apps-v1-deployment-ruby.yaml     apps-v1-deployment-rust.yaml     apps-v1-deployment-simplewebapp.yaml     kustomization.yaml    prod     apps-v1-deployment-golang.yaml     apps-v1-deployment-myproject-django.yaml     apps-v1-deployment-myproject-java-gradle-war.yaml     apps-v1-deployment-myproject-java-gradle.yaml     apps-v1-deployment-myproject-java-maven-war.yaml     apps-v1-deployment-myproject-php.yaml     apps-v1-deployment-myproject-python.yaml     apps-v1-deployment-nodejs.yaml     apps-v1-deployment-ruby.yaml     apps-v1-deployment-rust.yaml     apps-v1-deployment-simplewebapp.yaml     kustomization.yaml    staging    apps-v1-deployment-golang.yaml    apps-v1-deployment-myproject-django.yaml    apps-v1-deployment-myproject-java-gradle-war.yaml    apps-v1-deployment-myproject-java-gradle.yaml    apps-v1-deployment-myproject-java-maven-war.yaml    apps-v1-deployment-myproject-php.yaml    apps-v1-deployment-myproject-python.yaml    apps-v1-deployment-nodejs.yaml    apps-v1-deployment-ruby.yaml    apps-v1-deployment-rust.yaml    apps-v1-deployment-simplewebapp.yaml    kustomization.yaml   openshift-template   parameters-dev.yaml   parameters-prod.yaml   parameters-staging.yaml   template.yaml  scripts   builddockerimages.bat   builddockerimages.sh   pushimages.bat   pushimages.sh  source  django   Dockerfile   Pipfile   Pipfile.lock   db.sqlite3   manage.py   requirements.txt   simplewebapp    __init__.py    asgi.py    settings.py    urls.py    wsgi.py   webroot   __init__.py   admin.py   apps.py   migrations    __init__.py   models.py   tests.py   urls.py   views.py  golang   Dockerfile   go.mod   main.go  java-gradle   Dockerfile   build.gradle   src   main   java    simplewebapp    MainServlet.java   webapp   WEB-INF   web.xml  java-gradle-war   Dockerfile   java-gradle-war.war  java-maven   Dockerfile   pom.xml   src   main   webapp   WEB-INF    web.xml   index.jsp  java-maven-war   Dockerfile   java-maven-war.war  nodejs   Dockerfile   main.js   package.json  php   Dockerfile   index.php   site.conf  python   Dockerfile   main.py   requirements.txt  ruby   Dockerfile   Gemfile   config.ru   ruby.rb   views   main.erb  rust  Cargo.toml  Dockerfile  Rocket.toml  src  main.rs 59 directories, 241 files The CLI has created Kubernetes YAMLs which are stored inside the deploy/yamls directory. For each of the directories and the services identified, it has created the deployment artifacts, service artifacts, and the ingress as required. The scripts directory contains the scripts for building the images for the applications using Dockerfiles.\nMany scripts like builddockerimages.sh and pushimages.sh are also present inside the directory. It has also created a simple deploy/compose/docker-compose.yaml to test the images locally. It has also created Tekton artifacts inside the deploy/cicd/tekton directory that are required if you want to use Tekton as your CI/CD pipeline.\nThe Readme.md file guides on the next steps to be followed.\n$ cat Readme.md Move2Kube --------- Congratulations! Move2Kube has generated the necessary build artfiacts for moving all your application components to Kubernetes. Using the artifacts in this directory you can deploy your application in a kubernetes cluster. Next Steps ---------- To try locally use the scripts in the \u0026#34;./scripts\u0026#34; directory, to build, push and deploy. For production usage use the CI/CD pipelines for deployment. Run the builddockerimages.sh script. Note: This step may take some time to complete.\n$ cd scripts/ $ ./builddockerimages.sh [+] Building 2.2s (11/11) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 1.36kB 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for registry.access.redhat.com/ubi8/ubi-minimal:latest 2.0s =\u0026gt; [1/6] FROM registry.access.redhat.com/ubi8/ubi-minimal:latest@sha256:cf1c63e3247e4074ee3549a064b8798a1a2513ad57dd79c9edb979836355b469 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 19.68kB 0.0s Click to see the remaining output. =\u0026gt; CACHED [2/6] RUN microdnf update \u0026amp;\u0026amp; microdnf install -y java-11-openjdk-devel wget tar \u0026amp;\u0026amp; microdnf clean all 0.0s =\u0026gt; CACHED [3/6] WORKDIR /usr/local 0.0s =\u0026gt; CACHED [4/6] RUN wget https://downloads.apache.org/tomcat/tomcat-9/v9.0.56/bin/apache-tomcat-9.0.56.tar.gz \u0026amp;\u0026amp; tar -zxf apache-tomcat-9.0.56.tar.gz \u0026amp;\u0026amp; rm -f apache-tomcat-9.0.56.tar.gz \u0026amp;\u0026amp; mv apache-tomcat-9.0.56 tomcat9 0.0s =\u0026gt; CACHED [5/6] RUN useradd -r tomcat \u0026amp;\u0026amp; chown -R tomcat:tomcat tomcat9 0.0s =\u0026gt; CACHED [6/6] COPY --chown=tomcat:tomcat java-gradle-war.war /usr/local/tomcat9/webapps/ 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:3b3a60601e19f502b4163984702bc4e35729a26470ec2e3b8e5e076ad0662db6 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/myproject-java-gradle-war 0.0s Use \u0026#39;docker scan\u0026#39; to run Snyk tests against images to find vulnerabilities and learn how to fix them /Users/user/Desktop/tutorial/myproject [+] Building 0.9s (11/11) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 1.36kB 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for registry.access.redhat.com/ubi8/ubi-minimal:latest 0.7s =\u0026gt; [1/6] FROM registry.access.redhat.com/ubi8/ubi-minimal:latest@sha256:cf1c63e3247e4074ee3549a064b8798a1a2513ad57dd79c9edb979836355b469 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 3.15kB 0.0s =\u0026gt; CACHED [2/6] RUN microdnf update \u0026amp;\u0026amp; microdnf install -y java-11-openjdk-devel wget tar \u0026amp;\u0026amp; microdnf clean all 0.0s =\u0026gt; CACHED [3/6] WORKDIR /usr/local 0.0s =\u0026gt; CACHED [4/6] RUN wget https://downloads.apache.org/tomcat/tomcat-9/v9.0.56/bin/apache-tomcat-9.0.56.tar.gz \u0026amp;\u0026amp; tar -zxf apache-tomcat-9.0.56.tar.gz \u0026amp;\u0026amp; rm -f apache-tomcat-9.0.56.tar.gz \u0026amp;\u0026amp; mv apache-tomcat-9.0.56 tomcat9 0.0s =\u0026gt; CACHED [5/6] RUN useradd -r tomcat \u0026amp;\u0026amp; chown -R tomcat:tomcat tomcat9 0.0s =\u0026gt; CACHED [6/6] COPY --chown=tomcat:tomcat java-maven-war.war /usr/local/tomcat9/webapps/ 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:07fb8e8c412414af37d0cc43100325eae37f0ede3885f09f627836c540f8514e 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/myproject-java-maven-war 0.0s Use \u0026#39;docker scan\u0026#39; to run Snyk tests against images to find vulnerabilities and learn how to fix them /Users/user/Desktop/tutorial/myproject [+] Building 3.5s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 798B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for registry.access.redhat.com/ubi8/php-74:latest 3.3s =\u0026gt; [1/3] FROM registry.access.redhat.com/ubi8/php-74:latest@sha256:6409cbedb0f80c0ffc823e8b9912245ea40f6bac7ec980c80f838554a8356d55 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 1.79kB 0.0s =\u0026gt; CACHED [2/3] COPY site.conf /etc/httpd/conf.d/ 0.0s =\u0026gt; CACHED [3/3] COPY . /var/www/html/ 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:0c2f598d30a420c255d546fddbbf7ba5431bddd4befb6ce2c1e79caec95e6a89 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/myproject-php 0.0s Use \u0026#39;docker scan\u0026#39; to run Snyk tests against images to find vulnerabilities and learn how to fix them /Users/user/Desktop/tutorial/myproject [+] Building 1.6s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 747B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for registry.access.redhat.com/ubi8/nodejs-12:latest 1.4s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 1.77kB 0.0s =\u0026gt; [1/3] FROM registry.access.redhat.com/ubi8/nodejs-12@sha256:0d0632645a115013db659b59aaadc56473c628b0fe4f14585eee12d37d15b66e 0.0s =\u0026gt; CACHED [2/3] COPY . . 0.0s =\u0026gt; CACHED [3/3] RUN npm install 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:c1e60390daed5c3e6e2268b9fcbcd395da58c926d2b6ca542432aff3fbceada4 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/nodejs 0.0s ... Login into a container registry from the terminal. Refer to the instructions for Quay and Docker Hub\nPush the container images to the registry specified during the transformation using the pushimages.sh script.\n$ ./pushimages.sh Using default tag: latest The push refers to repository [quay.io/move2kube/myproject-python] 8558b30e6fa3: Pushed c25982faebf0: Pushed e262751a7e43: Layer already exists 5e6a0ab87a4b: Layer already exists 7b17276847a2: Layer already exists 558b534f4e1b: Layer already exists 3ba8c926eef9: Layer already exists 352ba846236b: Layer already exists Click to see the remaining output. latest: digest: sha256:d76b1dc841442b0e31c533c1e2419b3ae670de7b4381d4ff1ca2eaf1fbf5dfe6 size: 1999 Using default tag: latest The push refers to repository [quay.io/move2kube/myproject-php] 44abc3aaaae4: Layer already exists c6a7a5dc5bb6: Layer already exists 6a1b2ca6ba18: Layer already exists 7b17276847a2: Layer already exists 558b534f4e1b: Mounted from quay.io/move2kube/myproject-django 3ba8c926eef9: Layer already exists 352ba846236b: Layer already exists latest: digest: sha256:a42087a9c938dc46a265c2aba787040ddd106248954c09a50a8e45e2d9e068f7 size: 1788 Using default tag: latest The push refers to repository [quay.io/move2kube/nodejs] c76f2a9d04f4: Layer already exists d9ba94873664: Layer already exists 24607ae115a3: Layer already exists 7b17276847a2: Layer already exists 558b534f4e1b: Layer already exists 3ba8c926eef9: Layer already exists 352ba846236b: Layer already exists latest: digest: sha256:169443441285694a29ec9235ac0b4d07c1f23a0e20b1f41be967e4416f8d1687 size: 1788 Using default tag: latest The push refers to repository [quay.io/move2kube/ruby] 2177ca5056ad: Layer already exists 856aec824b2a: Layer already exists 5f70bf18a086: Layer already exists ... Deploy the applications using kubectl apply -f ./deploy/yamls. $ cd .. $ kubectl apply -f deploy/yamls deployment.apps/golang created service/golang created deployment.apps/myproject-django created service/myproject-django created ingress.networking.k8s.io/myproject created deployment.apps/myproject-java-gradle created service/myproject-java-gradle created deployment.apps/myproject-java-gradle-war created service/myproject-java-gradle-war created deployment.apps/myproject-java-maven-war created service/myproject-java-maven-war created deployment.apps/myproject-php created service/myproject-php created deployment.apps/myproject-python created service/myproject-python created deployment.apps/nodejs created service/nodejs created deployment.apps/ruby created service/ruby created deployment.apps/rust created service/rust created deployment.apps/simplewebapp created service/simplewebapp created Now all our applications are accessible on the cluster.\nGet the ingress to see the URLs where the apps have been deployed to kubectl get ingress myproject -o yaml. Note: If you deployed to Minikube, make sure to enable the ingress addon.\n$ minikube addons enable ingress  After the addon is enabled, please run \u0026#34;minikube tunnel\u0026#34; and your ingress resources would be available at \u0026#34;127.0.0.1\u0026#34;  Using image k8s.gcr.io/ingress-nginx/controller:v1.0.4  Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1  Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1  Verifying ingress addon...  The \u0026#39;ingress\u0026#39; addon is enabled $ minikube addons enable ingress-dns  After the addon is enabled, please run \u0026#34;minikube tunnel\u0026#34; and your ingress resources would be available at \u0026#34;127.0.0.1\u0026#34;  Using image gcr.io/k8s-minikube/minikube-ingress-dns:0.0.2  The \u0026#39;ingress-dns\u0026#39; addon is enabled $ minikube tunnel  The service/ingress myproject requires privileged ports to be exposed: [80 443]  sudo permission will be asked for it.  Starting tunnel for service myproject. Password: The apps should be available at the URLs below:\ngolang - http://localhost/golang\nnodejs - http://localhost/nodejs\npython - http://localhost/myproject-python\nruby - http://localhost/ruby\nrust - http://localhost/rust\nConclusion You can have a very diverse source environment like the language-platforms sample, which has multiple apps in different languages, and in a very simple way you can containerize and deploy them to Kubernetes. If you don\u0026rsquo;t like working with the terminal there is a Move2Kube UI tool which has all the same features as the CLI.\n"},{"uri":"http://konveyor.github.io/move2kube/","title":"Move2Kube","tags":[],"description":"","content":"Move2Kube Use this section to better understand and use the Konveyor Move2Kube tool.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/usingui/","title":"Using Move2Kube UI","tags":[],"description":"","content":"Similar to the command line tool, the Move2Kube Web-UI can also perform the transformation with all the capabilities that are in the command line tool. This document explains the steps to bring up the UI and backend using Docker and use it for transformation.\nPrerequisites Install Docker.\nWe will use language-platforms sample. The language-platforms file has a combination of multiple applications in different languages (Java, Go, Python, Ruby, etc.) which needs to be containerized and then put into Kubernetes.\nUsing the UI to perform a transformation Download the language platforms sample as a zip.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/language-platforms -r move2kube-demos -z $ ls language-platforms.zip Run docker run --rm -it -p 8080:8080 quay.io/konveyor/move2kube-ui.\n$ docker run --rm -it -p 8080:8080 quay.io/konveyor/move2kube-ui INFO[0000] Starting Move2Kube API server at port: 8080 This starts a container using the Move2Kube UI image on port 8080.\n```console # Optionally if you need persistence then mount the current directory: $ docker run --rm -it -p 8080:8080 -v \u0026quot;${PWD}/move2kube-api-data:/move2kube-api/data\u0026quot; quay.io/konveyor/move2kube-ui:latest # And if you also need more advanced features of Move2Kube then mount the docker socket. This will allow Move2Kube to run container based transformers: $ docker run --rm -it -p 8080:8080 -v \u0026quot;${PWD}/move2kube-api-data:/move2kube-api/data\u0026quot; -v //var/run/docker.sock:/var/run/docker.sock quay.io/konveyormove2kube-ui:latest ``` Open http://localhost:8080 in your browser.\nClick the New Workspace button to create a new workspace named Workspace 1.\nScroll down and click the New Project button to create a new project named Project 1.\nScroll down to the Project inputs section and upload the language-platforms.zip file downloaded earlier in this tutorial and wait for it to finish uploading.\nScroll down to the Plan section and click on the Start Planning button.\nNote: Generating the plan takes about three to five minutes to generate the plan in YAML format.\nScroll to view the different services. Important: If you edit the plan you must click Save.\nNow scroll down to Outputs section and click on the Start Transformation button.\nMove2Kube will ask some questions to aid in the transfomation process.\nFor the container registry question, specify the container registry where you want to push the images.\nSame for the container registry namespace question.\nIf your container registry requires authentication for pulling images, specify that in the container registry login question.\nFor the ingress host question, specify the hostname of the Kubernetes cluster. If you are deploying to Minikube then specify localhost as the hostname and leave the TLS secret blank.\nFor all other questions, click the Next button to accept the default answers.\nClick on the output to download the generated artifacts as a zip file (here workspace-1-project-1-output-bcad1e64-23d0-4ea1-ad47-9d060e870b4f.zip), extract it, and browse them. The applications can now be deployed to Kubernetes using these generated artifacts.\nNow we can build and push the container images and deploy to Kubernetes using the output we downloaded. The steps for doing that are same as the CLI tutorial.\nConclusion We have seen how easy it is to to do a transformation using the UI. The UI can be hosted on a common server and used by different teams using different workspaces and also has authentication and authorization capabilities to restrict access to particular workspaces.\n"},{"uri":"http://konveyor.github.io/tackle/","title":"Tackle","tags":[],"description":"","content":"Tackle Use this section to better understand and use the Konveyor Tackle tool.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/cfappstok8/","title":"Migrating Enterprise Scale Cloud Foundry Apps to Kubernetes","tags":[],"description":"","content":"In this tutorial we will go through the entire workflow of migratinging a Cloud Foundry application with several micro-services to run on Kubernetes.\nWe will be using the enterprise-app which is a retail website for shopping online. The website shows some products for sale and you can create orders by adding things to cart and checking out.\nThis application consists of five different services:\nFrontend: Website content written using React and Patternfly meant to be run on an Nginx server. Gateway: Portal to all the API servers that aggregates the orders and customer information and acts as a circuit breaker in case one of the API servers start to fail. (Written using the Java Spring Boot and PostGreSQL stack.) Customers: Manages everything related to customers. (Runs using Tomcat and PostGreSQL for the database.) Orders: Manages everything related to orders. (Written using Spring Boot and PostGreSQL for the database.) Inventory: Manages everything related to products. (Written using Spring Boot and PostGreSQL for the database.) This tutorial will go through the workflow for containerizing this application using Move2Kube to get it running on Kubernetes. This tutorial is split into sections to make it easy to skip around.\n"},{"uri":"http://konveyor.github.io/move2kube/installation/","title":"Installation","tags":[],"description":"","content":"Move2Kube can be consumed as a command line tool or as a web-based interface for creating the Kubernetes/OpenShift deployment artifacts.\n"},{"uri":"http://konveyor.github.io/move2kube/commands/","title":"Commands","tags":[],"description":"","content":"Move2Kube creates all resources required for deploying applications into Kubernetes including containerization and Kubernetes resources. It supports translating from Docker swarm/Docker-compose, Cloud Foundry, and other non-containerized applications. If the application does not use any of those or is not containerized, it can still be transformed by Move2Kube.\nNote: Use these commands to view the options available in Move2Kube.\n-h, --help help for move2kube --log-file string File to store the logs in. By default it only prints to console. --log-level string Set logging levels. (default \u0026#34;info\u0026#34;) Move2Kube commands There are four phases of the Move2Kube processes that are created, configured, and run using four commands and their options:\nMove2Kube collect - Collects and processes metadata from multiple sources. Move2Kube plan - Plans the deployment of an application into Kubernetes. Move2Kube transform - Transforms the application using the results of Move2Kube plan stage. Move2Kube version - Assigns the application version information. Move2Kube collect command Move2Kube collects metadata from multiple sources (cluster, image repo etc.), then filters and summarizes them into a YAML.\nmove2kube collect [flags] Collect options Move2Kube provides the following options for collecting.\n-a, --annotations string Specify annotations to select collector subset. -h, --help help for collect -o, --output string Specify output directory for collect. (default \u0026#34;.\u0026#34;) -s, --source string Specify source directory for the artifacts to be considered while collecting. Options inherited from parent commands --log-file string File to store the logs in. By default it only prints to console. --log-level string Set logging levels. (default \u0026#34;info\u0026#34;) Move2Kube plan Move2Kube discovers and creates a plan-file based on an input directory built after running the collect command.\nmove2kube plan [flags] Plan options Move2Kube provides the following options for planning.\n-f, --config strings Specify config file locations. -c, --customizations string Specify directory where customizations are stored. --disable-local-execution Allow files to be executed locally. -h, --help help for plan -n, --name string Specify the project name. (default \u0026#34;myproject\u0026#34;) -p, --plan string Specify a file path to save plan to. (default \u0026#34;m2k.plan\u0026#34;) --preset strings Specify preset config to use. --set-config stringArray Specify config key-value pairs. -s, --source string Specify source directory. (default \u0026#34;.\u0026#34;) -t, --transformer-selector string Specify the transformer selector. Options inherited from parent commands --log-file string File to store the logs in. By default it only prints to console. --log-level string Set logging levels. (default \u0026#34;info\u0026#34;) Move2Kube transform Transform functionality modifies artifacts using the Move2Kube plan-file.\nmove2kube transform [flags] Transform options Move2Kube provides the following options for transform.\n-f, --config strings Specify config file locations. --config-out string Specify config file output location. (default \u0026#34;.\u0026#34;) -c, --customizations string Specify directory where customizations are stored. --disable-local-execution Allow files to be executed locally. -h, --help help for transform --ignore-env Ignore data from local machine. -n, --name string Specify the project name. (default \u0026#34;myproject\u0026#34;) -o, --output string Path for output. Default will be directory with the project name. (default \u0026#34;.\u0026#34;) --overwrite Overwrite the output directory if it exists. By default we don\u0026#39;t overwrite. -p, --plan string Specify a plan file to execute. (default \u0026#34;m2k.plan\u0026#34;) --preset strings Specify preset config to use. --qa-cache-out string Specify cache file output location. (default \u0026#34;.\u0026#34;) --qa-persist-passwords Stores passwords too in the config. --qa-skip Enable/disable the default answers to questions posed in QA Cli sub-system. If disabled, you will have to answer the questions posed by QA during interaction. --set-config stringArray Specify config key-value pairs. -s, --source string Specify source directory to transform. If you already have a m2k.plan then this will override the sourceDir value specified in that plan. -t, --transformer-selector string Specify the transformer selector. Options inherited from parent commands --log-file string File to store the logs in. By default it only prints to console. --log-level string Set logging levels. (default \u0026#34;info\u0026#34;) Move2Kube version Version functionality prints the application version information.\nmove2kube version [flags] Version options Move2Kube provides the following options for versioning.\n-h, --help help for version -l, --long Print the version details. Options inherited from parent commands --log-file string File to store the logs in. By default it only prints to console. --log-level string Set logging levels. (default \u0026#34;info\u0026#34;) "},{"uri":"http://konveyor.github.io/move2kube/transformers/","title":"Transformers","tags":[],"description":"","content":"Move2Kube uses a suite of transformers to modify objects. To customize the output artifacts generated for a specific input, these transformers can be configured or new custom transformers can be created to achieve the required result. Transformer behavior and configuration is determined by the Transformer Class it uses. Though all the transformer classes are equal internally in Move2Kube, from a usage perspective, they are classified into three categories.\nPurpose Built - Has a specific job and the customization allows for changing the parameters/configuration required for performing the specific job. Ex: Kubernetes, Parameterizer, GolangDockerfileGenerator, etc.. External - Allows you to write custom transformers performing any behavior. It exposes the internal functions of the transformer class through different interfaces to be implemented by the transformer externally. Ex: Starlark, Executable Special - These classes allow special behaviors. Ex: Router Purpose Built These transformer classes do a specific job, and the customization allows for changing the parameters/configuration required for performing the specific job.\nIn most cases, these classes have one internal implementation and a transformer configuration can be found here.\nThe general steps to use these transformer classes are:\nFind the internal implementation here. Copy the directory. Change the configuration like the transformer name, templates, etc.. Set it to override the internal implementation. The custom transformer tutorial that uses node.js/Kubernetes transformers and the parameterization tutorial that uses the parameterization transformer are examples.\nTo understand the configuration provided by each of these transformers, the YAML in the built-in transformer should have a good overview. The other location to look for is the structure in the transformer implementation. In most classes which have a configuration, there will be a structure with a name ending in YamlConfig. For example, the Kubernetes transformer class has KubernetesYamlConfig. This is the configuation specified in the spec.config field.\nParameterizer Find the source code here, and follow this tutorial to customize paramaterization.\nSyntax for parameterizing specific fields Move2Kube provides a way to parameterize any field in the Kubernetes YAML files, Helm charts, and Openshift Templates that Move2Kube generates. It can also parameterize fields in Kubernetes YAMLs found in the source directory.\nIn order to parameterize a specific field, Move2Kube needs to use a custom transformer which means a directory with some YAML files inside it needs to be created. Below is an example parameterizer in detail:\n$ ls README.md deployment-parameterizers.yaml parameterizers.yaml Look at the deployment-parameterizers.yaml to understand the syntax.\napiVersion: move2kube.konveyor.io/v1alpha1 kind: Parameterizer metadata: name: deployment-parameterizers spec: parameterizers: - target: \u0026#34;spec.replicas\u0026#34; template: \u0026#34;${common.replicas}\u0026#34; default: 10 filters: - kind: Deployment apiVersion: \u0026#34;.*/v1.*\u0026#34; Now the `values.yaml` looks like this - target: \u0026#39;spec.template.spec.containers.[containerName:name].image\u0026#39; template: \u0026#39;${imageregistry.url}/${imageregistry.namespace}/${services.$(metadataName).containers.$(containerName).image.name}:${services.$(metadataName).containers.$(containerName).image.tag}\u0026#39; default: quay.io/konveyor/myimage:latest filters: - kind: Deployment apiVersion: \u0026#34;.*/v1.*\u0026#34; parameters: - name: services.$(metadataName).containers.$(containerName).image.name values: - envs: [dev, staging, prod] metadataName: frontend value: frontend - envs: [prod] metadataName: orders value: orders custom: containerName: orders Notice the parameterizer YAML follows the same conventions as Kubernetes YAMLs.\napiVersion : string - The version of the Move2Kube API being used which is currently set at move2kube.konveyor.io/v1alpha1. kind : string - Tells Move2Kube the YAML file type which in this case it is set at Parameterizer. metadata : object name : string - Name of the parameterizer. spec : object parameterizers : array[object] - List of parameterizer objects. target : string - Sets the field to parameterize. The syntax is same as yq dot notation which is explained in more detail in a later section.\ntemplate : string - Specifies how the field should get its value. For example: \u0026quot;${common.replicas}\u0026quot; means the generated Helm chart which contains the values.yaml there is the common field, and inside it replicas as shown below.\n$ cat values.yaml common: replicas: 10 The value of this field will be the same as the value in the original YAML, but it can be overriden using the default parameter.\ndefault : any can be used to override the default value for the field being parameterized. For example: if the original value of spec.replicas was 2, then the values.yaml would look like this:\n$ cat values.yaml common: replicas: 2 To set a different value like 10, specify default: 10 in the parameterizer YAML and now the values.yaml looks like this:\n$ cat values.yaml common: replicas: 10 filters : array[object] - Used to filter the Kubernetes YAML files that being targeted.\nkind : string - Only parameterizes Kubernetes YAMLs that match this kind field. Specify a regex to match multiple kinds. apiVersion : string - Only parameterizers Kubernetes YAMLs that match this apiVersion field. A regex can be specified. name : string - Only parameterizes Kubernetes YAMLs that have the same metadata.name field. A regex can be specified. envs : array[string] - Only apply this parameterization when targeting one of the environments listed here. parameters : array[object] - This can be used to specify defaults for each parameter inside the template.\nname : string - Name of a parameter inside the template. default : string - Default value for this parameter. Template parameter When parameterizing a specific a field in a Kubernetes YAML, it can be templatized in different ways. If the template parameter is not specified, it will default to the target parameter.\nExample 1 For example: target: \u0026quot;spec.replicas\u0026quot; would cause the spec.replicas to be parameterized (probably in Deployment YAMLs). Move2Kube would parameterize it under the key \u0026lt;kind\u0026gt;.\u0026lt;apiVersion\u0026gt;.\u0026lt;metadata.name\u0026gt;.spec.replicas and the values.yaml would look like this:\nDeployment: v1: myDeploymentName: spec: replicas: 2 In this example myDeploymentName is the metadata.name field in the Deployment YAML and 2 is the original value for spec.replicas from the YAML file.\nBy specifying template: \u0026quot;${common.replicas}\u0026quot; the default key is overridden and now Move2Kube puts the following in the values.yaml\ncommon: replicas: 2 By specifying default: 10 the default value is overridden and now Move2Kube puts the following in the values.yaml\ncommon: replicas: 10 Example 2 This example is a more complicated scenario of parameterizing the image name of some container in a Deployment YAML. First, because the containers field in the Deployment YAML is a list, use the syntax [\u0026lt;index\u0026gt;] to parameterize a single element in the list.\nUse target: \u0026quot;spec.template.spec.containers.[0].image\u0026quot; to parameterize the first container in the Deployment. The values.yaml looks like this:\nDeployment: v1: myDeploymentName: spec: template: spec: containers: - image: \u0026#39;my-repo.com/my-namespace/my-image-name:my-image-tag\u0026#39; This may not be enough and may want to parameterize the container image registry URL, registry namespace, image name, and image tag separately.\nTo do this use:\ntemplate: \u0026#39;${imageregistry.url}/${imageregistry.namespace}/${containers.[0].image.name}:${containers.[0].image.tag}\u0026#39; This will cause the values.yaml to look like this:\nimageregistry: url: \u0026#39;my-repo.com\u0026#39; namespace: \u0026#39;my-namespace\u0026#39; containers: - image: name: \u0026#39;my-image-name\u0026#39; tag: \u0026#39;my-image-tag\u0026#39; The Helm template will look like this:\n{% raw %} spec: template: spec: containers: - image: \u0026#39;{{ index .Values \u0026#34;imageregistry\u0026#34; \u0026#34;url\u0026#34; }}/{{ index .Values \u0026#34;imageregistry\u0026#34; \u0026#34;namespace\u0026#34; }}/{{ index .Values \u0026#34;containers\u0026#34; \u0026#34;[0]\u0026#34; \u0026#34;image\u0026#34; \u0026#34;name\u0026#34; }}:{{ index .Values \u0026#34;containers\u0026#34; \u0026#34;[0]\u0026#34; \u0026#34;image\u0026#34; \u0026#34;tag\u0026#34; }}\u0026#39; {% endraw %} Example 3 This is an even more complicated scenario continuing from example 2 that adds a dynamic key in our values.yaml.\nTo do this use the [] square brackets and $ dollar sign syntax:\ntarget: \u0026#39;spec.template.spec.containers.[containerName:name].image\u0026#39; template: \u0026#39;${imageregistry.url}/${imageregistry.namespace}/${containers.$(containerName).image.name}:${containers.$(containerName).image.tag}\u0026#39; Here [containerName:name] in target tells Move2Kube to extract the name field from the container object in the Deployment YAML and make it available as containerName. The $(containerName) in template gets replaced by the name that was extracted.\nThe values.yaml looks like this:\nimageregistry: url: \u0026#39;my-repo.com\u0026#39; namespace: \u0026#39;my-namespace\u0026#39; containers: myContainerName1: image: name: \u0026#39;my-image-name\u0026#39; tag: \u0026#39;my-image-tag\u0026#39; myContainerName2: image: name: \u0026#39;my-image-name-2\u0026#39; tag: \u0026#39;my-image-tag-2\u0026#39; This provides a very powerful way to parameterize the image name of containers and simultaneously have a common registry URL and namespace for all the images while also parameterizing the image name and tag for each container separately.\nExternal These transformer classes allow you to write custom transformers performing any behavior. It exposes the internal functions of the transformer class through different interfaces to be implemented by the transformer externally.\nExecutable An Executable class based on a transformer can be configured to run commands locally or as containers. These transformers essentially implement the DirectoryDetect and Transform functions of the transformer as executable commands. This allows using any language to write the function and makes them very powerful.\nStarlark A Starlark class based transformer allows for writing a full fledged transformer in Starlark by implementing the directory_detect and transform functions.\nSee examples of using this transform class here and here.\nSpecial These classes allow special behaviors.\nRouter The Router transformer direct an artifact to one of the eligible transformers, like choosing the server for a WAR file.\nWarRouter and EarRouter are examples of using this transformer class.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/customizeoutput/addcustfiledir/","title":"Add custom files and directories in custom locations","tags":[],"description":"","content":"Move2Kube allows custom template files to be added to the directories of your choice. In this example, we illustrate this by adding a custom helm-chart.\nStart by creating an empty workspace directory say workspace and make it the current working directory. We will assume all commands are executed within this directory. $ mkdir workspace \u0026amp;\u0026amp; cd workspace Use the enterprise-app as input for this flow. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/src -r move2kube-demos $ ls src README.md\tconfig-utils\tcustomers\tdocs\tfrontend\tgateway\torders In this project, all the apps have a pom.xml file. We will use a custom transformer to place a helm chart created from a template into each of those project directories.\nUse the Starlark based custom transformer located here. We copy it into the customizations sub-directory. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d add-custom-files-directories-in-custom-locations -r move2kube-transformers -o customizations Transform using this customization and specify the customization using the -c flag. $ move2kube transform -s src/ -c customizations/ --qa-skip Once the output is generated, we can observe one helm-chart was generated for each service and placed within the service directory. Also, note that every helm-chart project is named after the service it is meant for. The contents are shown below for reference:\n{% raw %} $ tree myproject myproject/  config-utils   helm-chart    config-utils    Chart.yaml    templates     config-utils-deployment.yaml     config-utils-ingress.yaml     config-utils-service.yaml    values.yaml   pom.xml   src   main   java   io   konveyor   demo   config   ApplicationConfiguration.java  customers   Makefile   helm-chart    customers    Chart.yaml    templates     customers-deployment.yaml     customers-ingress.yaml     customers-service.yaml    values.yaml   pom.xml   src   main   java    io    konveyor    demo    ordermanagement    OrderManagementAppInitializer.java    config     PersistenceConfig.java     WebConfig.java    controller     CustomerController.java    exception     ResourceNotFoundException.java     handler     ExceptionHandlingController.java    model     Customer.java    repository     CustomerRepository.java    service    CustomerService.java   resources   import.sql   persistence.properties  gateway  helm-chart   snowdrop-dependencies   Chart.yaml   templates    snowdrop-dependencies-deployment.yaml    snowdrop-dependencies-ingress.yaml    snowdrop-dependencies-service.yaml   values.yaml  manifest.yml  pom.xml  src  main   java    META-INF     MANIFEST.MF    io    konveyor    demo    gateway    Application.java    command     ProductCommand.java    controller     CustomersController.java     InventoryController.java     OrdersController.java    exception     ResourceNotFoundException.java     handler     ExceptionHandlingController.java    model     Customer.java     Order.java     OrderItem.java     OrderSummary.java     Product.java    repository     CustomerRepository.java     GenericRepository.java     InventoryRepository.java     OrderRepository.java    serialization     CustomerDeserializer.java     ProductDeserializer.java    service    CustomersService.java    InventoryService.java    OrdersService.java   resources   application-local.properties   application-openshift.properties   bootstrap.properties  test  java   io   konveyor   demo   gateway   controller    OrdersControllerTest.java   model    OrderTest.java   repository    CustomerRepositoryTest.java    InventoryRepositoryTest.java    OrderRepositoryTest.java   service   OrdersServiceTest.java  resources  application-test.properties  bootstrap.properties {% endraw %} Anatomy of transformer in add-custom-files-directories-in-custom-locations This custom transformer is more advanced compared to previous cases. It uses a Starlark script (customhelmchartgen.star) and several templatization features to achieve the per-service helm-chart requirement. Notice the {% raw %}{{\\ .ServiceName\\ }}{% endraw %} template in the file names of custom helm-chart template in the templates sub-directory. The contents of the add-custom-files-directories-in-custom-locations custom transformer are shown below:\n{% raw %}customization/add-custom-files-directories-in-custom-locations/  customhelmchartgen.star  customhelmchartgen.yaml  templates  helm-chart  {{\\ .ServiceName\\ }}  Chart.yaml  templates   {{\\ .ServiceName\\ }}-deployment.yaml   {{\\ .ServiceName\\ }}-ingress.yaml   {{\\ .ServiceName\\ }}-service.yaml  values.yaml{% endraw %} The code of the Starlark script (cat customizations/add-custom-files-directories-in-custom-locations/customhelmchartgen.star) is shown below. At a high level, the custom transformer detects a Java project if it finds pom.xml in the directory when the directory_detect() function is invoked in the detect phase. Once the Java project is detected, the corresponding project path and service name are passed to the transform phase through Move2Kube.\nIn the transform phase, the transform() function is invoked with the discovered service artifacts from the detect phase. These artifacts are used to fill the helm-chart templates shown above and produced as the output in a per-service directory structure.\n{% raw %}PomFile = \u0026#34;pom.xml\u0026#34; # Performs the detection of pom file and extracts service name def directory_detect(dir): dataFilePath = fs.pathjoin(dir, PomFile) if fs.exists(dataFilePath): serviceName = getServiceName(dataFilePath) return {serviceName: [{ \u0026#34;paths\u0026#34;: {\u0026#34;ProjectPath\u0026#34;: [dir]} }] } # Creates the customized helm chart for every service def transform(new_artifacts, old_artifacts): pathMappings = [] artifacts = [] pathTemplate = \u0026#34;{{ SourceRel .ServiceFsPath }}\u0026#34; for v in new_artifacts: serviceName = v[\u0026#34;configs\u0026#34;][\u0026#34;Service\u0026#34;][\u0026#34;serviceName\u0026#34;] dir = v[\u0026#39;paths\u0026#39;][\u0026#39;ProjectPath\u0026#39;][0] # Create a path template for the service pathTemplateName = serviceName.replace(\u0026#34;-\u0026#34;, \u0026#34;\u0026#34;) + \u0026#39;path\u0026#39; tplPathData = {\u0026#39;ServiceFsPath\u0026#39;: dir, \u0026#39;PathTemplateName\u0026#39;: pathTemplateName} pathMappings.append({\u0026#39;type\u0026#39;: \u0026#39;PathTemplate\u0026#39;, \\ \u0026#39;sourcePath\u0026#39;: pathTemplate, \\ \u0026#39;templateConfig\u0026#39;: tplPathData}) # Since the helm chart uses the same templating character {{ }} as Golang templates, # we use `SpecialTemplate` type here where the templating character is \u0026lt;~ ~\u0026gt;. # The `Template` type can be used for all normal cases pathMappings.append({\u0026#39;type\u0026#39;: \u0026#39;SpecialTemplate\u0026#39;, \\ \u0026#39;destinationPath\u0026#39;: \u0026#34;{{ .\u0026#34; + pathTemplateName + \u0026#34; }}\u0026#34;, \\ \u0026#39;templateConfig\u0026#39;: {\u0026#39;ServiceFsPath\u0026#39;: dir, \u0026#39;ServiceName\u0026#39;: serviceName}}) pathMappings.append({\u0026#39;type\u0026#39;: \u0026#39;Source\u0026#39;, \\ \u0026#39;sourcePath\u0026#39;: \u0026#34;{{ .\u0026#34; + pathTemplateName + \u0026#34; }}\u0026#34;, \u0026#39;destinationPath\u0026#39;: \u0026#34;{{ .\u0026#34; + pathTemplateName + \u0026#34; }}\u0026#34;}) return {\u0026#39;pathMappings\u0026#39;: pathMappings, \u0026#39;artifacts\u0026#39;: artifacts} # Extracts service name from pom file def getServiceName(filePath): data = fs.read(filePath) lines = data.splitlines() for l in lines: if \u0026#39;artifactId\u0026#39; in l: t = l.split(\u0026#39;\u0026gt;\u0026#39;) t2 = t[1].split(\u0026#39;\u0026lt;\u0026#39;) return t2[0]{% endraw %} "},{"uri":"http://konveyor.github.io/move2kube/tutorials/customizeoutput/paramcustomfieldshelm/","title":"Parameterizing custom fields in Helm Chart, Kustomize, OC templates","tags":[],"description":"","content":"In this tutorial, we illustrate how to parameterize a custom field in the Helm chart generated by Move2Kube.\nStart by creating an empty workspace directory say workspace and make it the current working directory. We will assume all commands are executed within this directory. $ mkdir workspace \u0026amp;\u0026amp; cd workspace Use the enterprise-app as input for this flow. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/src -r move2kube-demos $ ls src README.md\tconfig-utils\tcustomers\tdocs\tfrontend\tgateway\torders Run Move2Kube without any customization. The relevant snippet from the deployment YAML generated in the path myproject/deploy/yamls-parameterized/helm-chart/myproject/templates/orders-deployment.yaml looks like this. When complete, delete the myproject directory.\n$ move2kube transform -s src/ --qa-skip \u0026amp;\u0026amp; cat myproject/deploy/yamls-parameterized/helm-chart/myproject/templates/orders-deployment.yaml \u0026amp;\u0026amp; rm -rf myproject apiVersion: apps/v1 kind: Deployment metadata: annotations: move2kube.konveyor.io/service.expose: \u0026#34;true\u0026#34; creationTimestamp: null labels: move2kube.konveyor.io/service: orders name: orders spec: progressDeadlineSeconds: 600 replicas: {% raw %}{{ index .Values \u0026#34;common\u0026#34; \u0026#34;replicas\u0026#34; }}{% endraw %} revisionHistoryLimit: 10 selector: matchLabels: move2kube.konveyor.io/service: orders strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: annotations: move2kube.konveyor.io/service.expose: \u0026#34;true\u0026#34; creationTimestamp: null labels: move2kube.konveyor.io/service: orders name: orders spec: containers: - env: - name: PORT value: \u0026#34;8080\u0026#34; image: quay.io/myproject/orders:latest imagePullPolicy: Always name: orders ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: quay-io-imagepullsecret restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: {} Notice that except the replicas field, no other field is parameterized.\nUse a custom configured version of the parameterizer transformer to view the parameterizing the other fields in the transformer. Then copy it into the customizations sub-directory.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d custom-helm-kustomize-octemplates-parameterization -r move2kube-transformers -o customizations Transform using this customization and specify the customization using the -c flag. $ move2kube transform -s src/ -c customizations/ --qa-skip When the output is generated, observe the same deployment as mentioned before.\n$ cat myproject/deploy/yamls-parameterized/helm-chart/myproject/templates/orders-deployment.yaml {% raw %}apiVersion: apps/v1 kind: Deployment metadata: annotations: move2kube.konveyor.io/service.expose: \u0026#34;true\u0026#34; creationTimestamp: null labels: move2kube.konveyor.io/service: orders name: orders spec: progressDeadlineSeconds: 600 replicas: {{ index .Values \u0026#34;common\u0026#34; \u0026#34;replicas\u0026#34; }} revisionHistoryLimit: 10 selector: matchLabels: move2kube.konveyor.io/service: orders strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: annotations: move2kube.konveyor.io/service.expose: \u0026#34;true\u0026#34; creationTimestamp: null labels: move2kube.konveyor.io/service: orders name: orders spec: containers: - env: - name: PORT value: \u0026#34;8080\u0026#34; image: {{ index .Values \u0026#34;imageregistry\u0026#34; \u0026#34;url\u0026#34; }}/{{ index .Values \u0026#34;imageregistry\u0026#34; \u0026#34;namespace\u0026#34; }}/{{ index .Values \u0026#34;services\u0026#34; \u0026#34;orders\u0026#34; \u0026#34;containers\u0026#34; \u0026#34;orders\u0026#34; \u0026#34;image\u0026#34; \u0026#34;name\u0026#34; }}:{{ index .Values \u0026#34;services\u0026#34; \u0026#34;orders\u0026#34; \u0026#34;containers\u0026#34; \u0026#34;orders\u0026#34; \u0026#34;image\u0026#34; \u0026#34;tag\u0026#34; }} imagePullPolicy: Always name: orders ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: quay-io-imagepullsecret restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: {} {% endraw %} A few of the parameterized yamls are:\n$ cat myproject/deploy/yamls-parameterized/helm-chart/myproject/values-prod.yaml common: replicas: 10 imageregistry: namespace: konveyor url: quay.io services: config-utils: containers: config-utils: image: name: myimage tag: latest customers: containers: customers: image: name: myimage tag: latest frontend: containers: frontend: image: name: frontend tag: latest gateway: containers: gateway: image: name: myimage tag: latest orders: containers: orders: image: name: orders tag: latest $ cat myproject/deploy/yamls-parameterized/kustomize/overlays/prod/apps-v1-deployment-orders.yaml - op: replace path: /spec/replicas value: 10 - op: replace path: /spec/template/spec/containers/0/image value: orders $ cat myproject/deploy/yamls-parameterized/openshift-template/parameters-prod.yaml COMMON_REPLICAS=10 IMAGEREGISTRY_URL=quay.io IMAGEREGISTRY_NAMESPACE=konveyor SERVICES_FRONTEND_CONTAINERS_FRONTEND_IMAGE_TAG=latest SERVICES_GATEWAY_CONTAINERS_GATEWAY_IMAGE_NAME=myimage SERVICES_ORDERS_CONTAINERS_ORDERS_IMAGE_NAME=orders SERVICES_ORDERS_CONTAINERS_ORDERS_IMAGE_TAG=latest SERVICES_CUSTOMERS_TOMCAT_CONTAINERS_CUSTOMERS_TOMCAT_IMAGE_NAME=myimage SERVICES_CUSTOMERS_TOMCAT_CONTAINERS_CUSTOMERS_TOMCAT_IMAGE_TAG=latest SERVICES_FRONTEND_CONTAINERS_FRONTEND_IMAGE_NAME=frontend SERVICES_CONFIG_UTILS_CONTAINERS_CONFIG_UTILS_IMAGE_NAME=myimage SERVICES_CONFIG_UTILS_CONTAINERS_CONFIG_UTILS_IMAGE_TAG=latest SERVICES_GATEWAY_CONTAINERS_GATEWAY_IMAGE_TAG=latest Anatomy of the parameterizer The contents of parameterizer are as shown below:\n$ ls customizations/custom-helm-kustomize-octemplates-parameterization/ README.md\tdeployment-parameterizers.yaml\tparameterizers.yaml The transformer configuration is in parameterizers.yaml and the parameterization config is in deployment-parameterizers.yaml. The configuration below specifies the fields that need to be parameterized, how to parameterize them, and the default values those fields should take. This can be extended to parameterize any field in any Kubernetes Yaml, and generate the appropriate helm chart, Kustomize YAMLs and Openshift templates.\n$ cat customizations/custom-helm-kustomize-octemplates-parameterization/deployment-parameterizers.yaml {% raw %}apiVersion: move2kube.konveyor.io/v1alpha1 kind: Parameterizer metadata: name: deployment-parameterizers spec: parameterizers: - target: \u0026#34;spec.replicas\u0026#34; template: \u0026#34;${common.replicas}\u0026#34; default: 10 filters: - kind: Deployment apiVersion: \u0026#34;.*/v1.*\u0026#34; - target: \u0026#39;spec.template.spec.containers.[containerName:name].image\u0026#39; template: \u0026#39;${imageregistry.url}/${imageregistry.namespace}/${services.$(metadataName).containers.$(containerName).image.name}:${services.$(metadataName).containers.$(containerName).image.tag}\u0026#39; default: quay.io/konveyor/myimage:latest filters: - kind: Deployment apiVersion: \u0026#34;.*/v1.*\u0026#34; parameters: - name: services.$(metadataName).containers.$(containerName).image.name values: - envs: [dev, staging, prod] metadataName: frontend value: frontend - envs: [prod] metadataName: orders value: orders custom: containerName: orders {% endraw %} Next step is adding custom files and directories in custom locations.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/customizeoutput/customannotationsyaml/","title":"Add custom annotations to Kubernetes YAMLs","tags":[],"description":"","content":"Move2Kube generates Kubernetes YAMLs based on the needs of the application, but there might be situations where you might require specific fields to be different in the output. In this example, we illustrate how we can add an annotation to the Ingress YAML specifying an ingress class.\nCreate an empty workspace directory mamed workspace and make it the current working directory. Assume all commands are executed within this directory. $ mkdir workspace \u0026amp;\u0026amp; cd workspace Use the enterprise-app as the input for this flow. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/src -r move2kube-demos $ ls src README.md\tconfig-utils\tcustomers\tdocs\tfrontend\tgateway\torders Run Move2Kube without any customization and the output ingress does not have any annotation. Once done, delete the myproject directory. $ move2kube transform -s src/ --qa-skip \u0026amp;\u0026amp; cat myproject/deploy/yamls/myproject-ingress.yaml \u0026amp;\u0026amp; rm -rf myproject apiVersion: networking.k8s.io/v1 kind: Ingress metadata: creationTimestamp: null labels: move2kube.konveyor.io/service: myproject name: myproject Get the Starlark based custom transformer here and copy it into the customizations sub-directory. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d add-custom-kubernetes-annotation -r move2kube-transformers -o customizations Transform using this customization and specify the customization using the -c flag. $ move2kube transform -s src/ -c customizations/ --qa-skip Once the output is generated, we can observe from the snippet of the ingress file (myproject/deploy/yamls/myproject-ingress.yaml) that there is an annotation for the ingress class added (kubernetes.io/ingress.class: haproxy):\n$ cat myproject/deploy/yamls/myproject-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: haproxy creationTimestamp: null labels: move2kube.konveyor.io/service: myproject name: myproject Anatomy of ingress annotator transformer This custom transformer uses a configuration YAML (ingress-annotator.yaml) and a Starlark script (ingress-annotator.star) to add an annotation to the ingress YAML. The contents of custom transformer are as shown below:\n$ ls customizations/add-custom-kubernetes-annotation ingress-annotator.star ingress-annotator.yaml The configuration YAML specifies that the custom transformer consumes and produces a Kubernetes YAML artifact type as shown in the consumes and produces section.\n$ cat customizations/add-custom-kubernetes-annotation/ingress-annotator.yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: Transformer metadata: name: IngressAnnotator labels: move2kube.konveyor.io/built-in: false spec: class: \u0026#34;Starlark\u0026#34; consumes: KubernetesYamls: merge: false # Ensures a artifact of this type gets immediately intercepted by this transformer mode: \u0026#34;MandatoryPassThrough\u0026#34; produces: KubernetesYamls: disabled: false config: starFile: \u0026#34;ingress-annotator.star\u0026#34; The code of the Starlark script is shown below. At a high-level, the code requires only the transform() function as it acts upon any Kubernetes YAML generated within Move2Kube. The transform() function loops through every YAML generated for every detected service, checks whether it is an ingress YAML, and if so adds the annotation. The path mappings are meant to persist these changes.\n$ cat customizations/add-custom-kubernetes-annotation/ingress-annotator.star {% raw %}def transform(new_artifacts, old_artifacts): pathMappings = [] artifacts = [] for a in new_artifacts: yamlsPath = a[\u0026#34;paths\u0026#34;][\u0026#34;KubernetesYamls\u0026#34;][0] serviceName = a[\u0026#34;name\u0026#34;] artifacts.append(a) fileList = fs.readdir(yamlsPath) yamlsBasePath = yamlsPath.split(\u0026#34;/\u0026#34;)[-1] # Create a custom path template for the service, whose values gets filled and can be used in other pathmappings pathTemplateName = serviceName.replace(\u0026#34;-\u0026#34;, \u0026#34;\u0026#34;) + yamlsBasePath tplPathData = {\u0026#39;PathTemplateName\u0026#39;: pathTemplateName} pathMappings.append({\u0026#39;type\u0026#39;: \u0026#39;PathTemplate\u0026#39;, \\ \u0026#39;sourcePath\u0026#39;: \u0026#34;{{ OutputRel \\\u0026#34;\u0026#34; + yamlsPath + \u0026#34;\\\u0026#34; }}\u0026#34;, \\ \u0026#39;templateConfig\u0026#39;: tplPathData}) for f in fileList: filePath = fs.pathjoin(yamlsPath, f) s = fs.read(filePath) yamlData = yaml.loads(s) if yamlData[\u0026#39;kind\u0026#39;] != \u0026#39;Ingress\u0026#39;: continue if \u0026#39;annotations\u0026#39; not in yamlData[\u0026#39;metadata\u0026#39;]: yamlData[\u0026#39;metadata\u0026#39;][\u0026#39;annotations\u0026#39;] = {\u0026#39;kubernetes.io/ingress.class\u0026#39;: \u0026#39;haproxy\u0026#39;} else: yamlData[\u0026#39;metadata\u0026#39;][\u0026#39;annotations\u0026#39;][\u0026#39;kubernetes.io/ingress.class\u0026#39;] = \u0026#39;haproxy\u0026#39; s = yaml.dumps(yamlData) fs.write(filePath, s) pathMappings.append({\u0026#39;type\u0026#39;: \u0026#39;Default\u0026#39;, \\ \u0026#39;sourcePath\u0026#39;: yamlsPath, \\ \u0026#39;destinationPath\u0026#39;: \u0026#34;{{ .\u0026#34; + pathTemplateName + \u0026#34; }}\u0026#34;}) return {\u0026#39;pathMappings\u0026#39;: pathMappings, \u0026#39;artifacts\u0026#39;: artifacts}{% endraw %} This tutorial can be replicated in the UI by uploading the zip file of the custom transformer as a customization. You can get the zip of the source and customization by adding a -z to the end of the commands used in steps 2 and 4.\nThe next step is parameterizing custom fields in Helm Chart, Kustomize, and OC Templates.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/customizeoutput/customgendockerfile/","title":"Customize generated Dockerfile and built-in transformer behavior","tags":[],"description":"","content":"In this tutorial we will make Move2Kube add custom Dockerfile, and a custom file.\nCreate an empty workspace directory named workspace and make it the current working directory. Assume all commands are executed within this directory. $ mkdir workspace \u0026amp;\u0026amp; cd workspace Use the enterprise-app as input for this flow. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/src -r move2kube-demos $ ls src README.md\tconfig-utils\tcustomers\tdocs\tfrontend\tgateway\torders Run Move2Kube without any customization. If the Dockerfile is generated for the frontend app, it uses registry.access.redhat.com/ubi8/nodejs-12 as the base image. There are no scripts named start-nodejs.sh in the frontend service directory. The Kubernetes YAMLs are generated in myproject/deploy/yamls directory. $ move2kube transform -s src/ --qa-skip \u0026amp;\u0026amp; ls myproject/source/frontend \u0026amp;\u0026amp; cat myproject/source/frontend/Dockerfile \u0026amp;\u0026amp; ls myproject/deploy \u0026amp;\u0026amp; rm -rf myproject Dockerfile\tREADME.md\tdr-surge.js\tmanifest.yml\tpackage-lock.json\tserver.js\tstories\ttest-setup.js\twebpack.common.js\twebpack.prod.js LICENSE\t__mocks__\tjest.config.js\tnodemon.json\tpackage.json\tsrc\tstylePaths.js\ttsconfig.json\twebpack.dev.js FROM registry.access.redhat.com/ubi8/nodejs-12 COPY . . RUN npm install RUN npm run build EXPOSE 8080 CMD npm run start cicd\tcompose\tknative\tknative-parameterized\tyamls\tyamls-parameterized For this tutorial we want to change:\nThe base image of the Dockerfile generated for Node.js from registry.access.redhat.com/ubi8/nodejs-12 to quay.io/konveyor/nodejs-12. Add a new script named start-nodejs.sh in the Node.js app directories along with the Dockerfile in the frontend directory. Change the location of Kubernetes YAMLs from myproject/deploy/yamls to myproject/yamls-elsewhere. Use a custom configured version of the Node.js built-in transformer and the Kubernetes built-in transformer to achieve this. Copy it into the customizations sub-directory. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d custom-dockerfile-change-built-in-behavior -r move2kube-transformers -o customizations Transform using this customization and specify the customization using the -c flag. $ move2kube transform -s src/ -c customizations/ --qa-skip Once the output is generated, we can observe:\nThe Dockerfile generated for the frontend app contains the custom base image. A new file named start-nodejs.sh was generated in the frontend directory. The Kubernetes YAMLs are now generated in myproject/yamls-elsewhere directory and the parameterized YAMLs are also in myproject/yamls-elsewhere-parameterized directory. $ ls myproject/source/frontend Dockerfile\tREADME.md\tdr-surge.js\tmanifest.yml\tpackage-lock.json\tserver.js\tstart-nodejs.sh\tstylePaths.js\ttsconfig.json\twebpack.dev.js LICENSE\t__mocks__\tjest.config.js\tnodemon.json\tpackage.json\tsrc\tstories\ttest-setup.js\twebpack.common.js\twebpack.prod.js $ cat myproject/source/frontend/Dockerfile FROM quay.io/konveyor/nodejs-12 COPY . . RUN npm install RUN npm run build EXPOSE 8080 CMD sh start-nodejs.sh $ ls myproject Readme.md\tdeploy\tscripts\tsource\tyamls-elsewhere\tyamls-elsewhere-parameterized Anatomy of transformers in custom-dockerfile-change-built-in-behavior The two customized transformers in the directory are nodejs and kubernetes. The contents of custom-dockerfile-custom-files are shown below:\n$ tree customizations customizations  custom-dockerfile-change-built-in-behavior  kubernetes   kubernetes.yaml  nodejs  nodejs.yaml  templates  Dockerfile  start-nodejs.sh To custom configure a built-in transformer, copy the built-in transformer\u0026rsquo;s configuration directory from move2kube source, change the configurations, and use it as a customization. You can make it override the built-in transformer using the override config in the yaml.\nIn this case, change the Dockerfile template, add a script, and change the transformer configuration YAML.\nTo change the template, we have added our custom template in customizations/custom-dockerfile-change-built-in-behavior/nodejs/templates/Dockerfile. The template is the same as the one used in the built-in transformer, except we are using a custom base image and a custom CMD here. {% raw %}$ cat customizations/custom-dockerfile-change-built-in-behavior/nodejs/templates/Dockerfile FROM quay.io/konveyor/nodejs-12 COPY . . RUN npm install {{- if .Build }} RUN npm run build {{- end}} EXPOSE {{ .Port }} CMD sh start-nodejs.sh{% endraw %} Add customizations/custom-dockerfile-change-built-in-behavior/nodejs/templates/start-nodejs.sh. $ ls customizations/custom-dockerfile-change-built-in-behavior/nodejs/templates/ Dockerfile\tstart-nodejs.sh The transformer.yaml is the transformer configuration with two changes compared to the built-in transformer: The name of our custom transformer is Nodejs-CustomFiles (see name field in the metadata section). We are also specifying an override section which is asking Move2Kube to disable the transformer named Nodejs-Dockerfile if it is present. $ cat customizations/custom-dockerfile-change-built-in-behavior/nodejs/nodejs.yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: Transformer metadata: name: Nodejs-CustomFiles labels: move2kube.konveyor.io/task: containerization move2kube.konveyor.io/built-in: true spec: class: \u0026#34;NodejsDockerfileGenerator\u0026#34; directoryDetect: levels: -1 consumes: Service: merge: false produces: Dockerfile: disabled: false DockerfileForService: disabled: false override: matchLabels: move2kube.konveyor.io/name: Nodejs-Dockerfile config: defaultNodejsVersion: \u0026#34;12\u0026#34; In the kubernetes transformer, change the name and override the config. Also change the default behavior of the transformer, which is to put the Kubernetes yamls in deploy/yamls directory by changing the spec.config.outputPath to yamls-elsewhere. $ cat customizations/custom-dockerfile-change-built-in-behavior/kubernetes/kubernetes.yaml {% raw %}apiVersion: move2kube.konveyor.io/v1alpha1 kind: Transformer metadata: name: KubernetesWithCustomOutputDirectory labels: move2kube.konveyor.io/built-in: true spec: class: \u0026#34;Kubernetes\u0026#34; directoryDetect: levels: 0 consumes: IR: merge: true produces: KubernetesYamls: disabled: false override: matchLabels: move2kube.konveyor.io/name: Kubernetes dependency: matchLabels: move2kube.konveyor.io/kubernetesclusterselector: \u0026#34;true\u0026#34; config: outputPath: \u0026#34;yamls-elsewhere\u0026#34; ingressName: \u0026#34;{{ .ProjectName }}\u0026#34;{% endraw %} The next step is adding custom annotations to Kubernetes YAMLs.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/customizeoutput/","title":"Customizing the output","tags":[],"description":"","content":"This tutorial picks up where we left off with the migration workflow. After examining the output that Move2Kube generated for our application, we might find some things that we want to change.\nIn this section we will look at how we can customize the output of Move2Kube to our needs using Transformers.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/customkubeyaml/","title":"Customize Kubernetes YAMLs to target specific clusters","tags":[],"description":"","content":"Move2Kube already supports targeting across multiple clusters includig: Kubernetes, Openshift, IBM-IKS, IBM-Openshift, Azure-EKS, Azure-AKS and GCP-GKS. There might be situations where you require generating Kubernetes YAMLs to target a particular cluster. In this tutorial we will see how we can use Konveyor Move2Kube to change the versions of existing Kubernetes resources to target a particular cluster. Move2Kube can also be customized to generate Kubernetes YAMLS deployable on a particular cluster.\nPrerequisites Install the Move2Kube CLI tool.\nInstall Kubectl.\nUse the kubernetes-to-kubernetes sample. This directory has some Kubernetes YAMLs that deploy a web app with multiple services. There are three services: a frontend website in PHP, a backend API in NodeJS, and a cache service using Redis.\nSteps Collect data about the Kubernetes cluster using move2kube collect. Limit the collection to only cluster information using the -a k8s annotation flag. Note: Before running the below command, log in to your target cluster. To check whether you are logged in to the target cluster run kubectl get pods.\n$ move2kube collect -a k8s INFO[0000] Begin collection INFO[0000] [*collector.ClusterCollector] Begin collection INFO[0006] [*collector.ClusterCollector] Done INFO[0006] [*collector.ImagesCollector] Begin collection INFO[0006] [*collector.ImagesCollector] Done INFO[0006] Collection done INFO[0006] Collect Output in [/Users/user/m2k_collect]. Copy this directory into the source directory to be used for planning. The data we collected will be stored in a new directory called ./m2k_collect.\n$ ls m2k_collect cf The ./m2k_collect/clusters directory contains the YAML file which has the cluster application information including the buildpacks that are supported, the memory, the number of instances and the ports that are supported. If there are environment variables, it collects that information too. The name of the cluster can be found in the metadata.name file, and can be renamed in the YAML file.\nFor this tutorial, we have already collected the move2kube collect output yaml file which contains the cluster related information and stored it here. We have renamed the cluster metadata.name as my-kubernetes-cluster in the yaml file.\nWe will use a custom configured version of the clusterselector transformer.\nDownload the custom-cluster-selector transformer into the customizations sub-directory. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d custom-cluster-selector -r move2kube-transformers -o customizations The customizations/custom-cluster-selector/transformer.yaml is the transformer configuration. There are two changes in the custom-cluster-selector/transformer.yaml compared to the built-in clusterselector/transformer.yaml :\nThe name of the custom transformer is CustomClusterSelector (see name field in the metadata section). We are also specifying an override section which is asking Move2Kube to disable the transformer named ClusterSelector if CustomClusterSelector transformer is present. $ cat customizations/custom-cluster-selector/transformer.yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: Transformer metadata: name: CustomClusterSelector labels: move2kube.konveyor.io/built-in: true move2kube.konveyor.io/kubernetesclusterselector: true spec: class: \u0026#34;ClusterSelectorTransformer\u0026#34; directoryDetect: levels: 0 consumes: IR: merge: true mode: OnDemandPassThrough KubernetesOrgYamlsInSource: merge: false mode: OnDemandPassThrough produces: IR: disabled: false KubernetesOrgYamlsInSource: disabled: false override: matchLabels: move2kube.konveyor.io/name: ClusterSelector To check what information did move2kube collect -a k8s collected for us, let\u0026rsquo;s see the content inside the my-kubernetes-cluster.yaml.\n$ cat customizations/custom-cluster-selector/clusters/my-kubernetes-cluster.yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: ClusterMetadata metadata: name: my-kubernetes-cluster spec: storageClasses: - default - ibmc-file-bronze - ibmc-file-bronze-gid - ibmc-file-custom - ibmc-file-gold Click to see the rest of the yaml. - ibmc-file-gold-gid - ibmc-file-retain-bronze - ibmc-file-retain-custom - ibmc-file-retain-gold - ibmc-file-retain-silver - ibmc-file-silver - ibmc-file-silver-gid apiKindVersionMap: APIService: - apiregistration.k8s.io/v1 BGPConfiguration: - crd.projectcalico.org/v1 BGPPeer: - crd.projectcalico.org/v1 Binding: - v1 BlockAffinity: - crd.projectcalico.org/v1 CSIDriver: - storage.k8s.io/v1 CSINode: - storage.k8s.io/v1 CSIStorageCapacity: - storage.k8s.io/v1beta1 CatalogSource: - operators.coreos.com/v1alpha1 CertificateSigningRequest: - certificates.k8s.io/v1 ClusterInformation: - crd.projectcalico.org/v1 ClusterRole: - rbac.authorization.k8s.io/v1 ClusterRoleBinding: - rbac.authorization.k8s.io/v1 ClusterServiceVersion: - operators.coreos.com/v1alpha1 ComponentStatus: - v1 ConfigMap: - v1 ControllerRevision: - apps/v1 CronJob: - batch/v1 - batch/v1beta1 CustomResourceDefinition: - apiextensions.k8s.io/v1 DaemonSet: - apps/v1 Deployment: - apps/v1 EndpointSlice: - discovery.k8s.io/v1 - discovery.k8s.io/v1beta1 Endpoints: - v1 Event: - events.k8s.io/v1 - events.k8s.io/v1beta1 - v1 Eviction: - v1 FelixConfiguration: - crd.projectcalico.org/v1 FlowSchema: - flowcontrol.apiserver.k8s.io/v1beta1 GlobalNetworkPolicy: - crd.projectcalico.org/v1 GlobalNetworkSet: - crd.projectcalico.org/v1 GuestBook: - webapp.metamagical.dev/v1 HorizontalPodAutoscaler: - autoscaling/v1 - autoscaling/v2beta1 - autoscaling/v2beta2 HostEndpoint: - crd.projectcalico.org/v1 IPAMBlock: - crd.projectcalico.org/v1 IPAMConfig: - crd.projectcalico.org/v1 IPAMHandle: - crd.projectcalico.org/v1 IPPool: - crd.projectcalico.org/v1 Ingress: - networking.k8s.io/v1 IngressClass: - networking.k8s.io/v1 InstallPlan: - operators.coreos.com/v1alpha1 Job: - batch/v1 KubeControllersConfiguration: - crd.projectcalico.org/v1 Lease: - coordination.k8s.io/v1 LimitRange: - v1 LocalSubjectAccessReview: - authorization.k8s.io/v1 MutatingWebhookConfiguration: - admissionregistration.k8s.io/v1 Namespace: - v1 NetworkPolicy: - networking.k8s.io/v1 - crd.projectcalico.org/v1 NetworkSet: - crd.projectcalico.org/v1 Node: - v1 NodeMetrics: - metrics.k8s.io/v1beta1 NodeProxyOptions: - v1 Operator: - operators.coreos.com/v1 OperatorGroup: - operators.coreos.com/v1 - operators.coreos.com/v1alpha2 PersistentVolume: - v1 PersistentVolumeClaim: - v1 Pod: - v1 PodAttachOptions: - v1 PodDisruptionBudget: - policy/v1 - policy/v1beta1 PodExecOptions: - v1 PodMetrics: - metrics.k8s.io/v1beta1 PodPortForwardOptions: - v1 PodProxyOptions: - v1 PodSecurityPolicy: - policy/v1beta1 PodTemplate: - v1 PriorityClass: - scheduling.k8s.io/v1 PriorityLevelConfiguration: - flowcontrol.apiserver.k8s.io/v1beta1 RBACSync: - ibm.com/v1alpha1 Redis: - webapp.metamagical.dev/v1 ReplicaSet: - apps/v1 ReplicationController: - v1 ResourceQuota: - v1 Role: - rbac.authorization.k8s.io/v1 RoleBinding: - rbac.authorization.k8s.io/v1 RuntimeClass: - node.k8s.io/v1 - node.k8s.io/v1beta1 Scale: - apps/v1 - v1 Secret: - v1 SelfSubjectAccessReview: - authorization.k8s.io/v1 SelfSubjectRulesReview: - authorization.k8s.io/v1 Service: - v1 ServiceAccount: - v1 ServiceProxyOptions: - v1 StatefulSet: - apps/v1 StorageClass: - storage.k8s.io/v1 SubjectAccessReview: - authorization.k8s.io/v1 Subscription: - operators.coreos.com/v1alpha1 TokenRequest: - v1 TokenReview: - authentication.k8s.io/v1 ValidatingWebhookConfiguration: - admissionregistration.k8s.io/v1 VolumeAttachment: - storage.k8s.io/v1 VolumeSnapshot: - snapshot.storage.k8s.io/v1 - snapshot.storage.k8s.io/v1beta1 VolumeSnapshotClass: - snapshot.storage.k8s.io/v1 - snapshot.storage.k8s.io/v1beta1 VolumeSnapshotContent: - snapshot.storage.k8s.io/v1 - snapshot.storage.k8s.io/v1beta1 The my-kubernetes-cluster.yaml has information about the target cluster under the spec field, includin the storageClasses, Deployment, Service, Ingress and NetworkPolicy versions supported by the target cluster. Move2Kube will use this information and generate the Kubernetes YAMLs which are tailored for the given target cluster.\nDownload the kubernetes-to-kubernetes sample. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/kubernetes-to-kubernetes -r move2kube-demos $ ls kubernetes-to-kubernetes/ api-deployment.yaml api-service.yaml redis-deployment.yaml redis-service.yaml web-deployment.yaml web-ingress.yaml web-service.yaml Generate a plan file. Here, we provide the custom-cluster-selector transformer which is inside the downloaded customizations folder to Move2Kube using the -c flag. $ move2kube plan -s kubernetes-to-kubernetes -c customizations INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [CloudFoundry] Planning transformation INFO[0000] [CloudFoundry] Done INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [Base Directory] Identified 0 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in . INFO[0000] Identified 1 named services and 0 to-be-named services in . INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 1 named services and 1 to-be-named services INFO[0000] [Named Services] Identified 1 named services INFO[0000] No of services identified : 1 INFO[0000] Plan can be found at [/Users/user/m2k.plan]. View the generated plan file in YAML format. Notice Move2Kube has detected our custom-cluster-selector as customizationsDir which will be used during the Transform phase. $ cat m2k.plan apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: kubernetes-to-kubernetes customizationsDir: custom-cluster-selector apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject Click to see the rest of the yaml. spec: sourceDir: kubernetes-to-kubernetes customizationsDir: custom-cluster-selector services: move2kube-transformers: - transformerName: Parameterizer paths: KubernetesYamls: - . ServiceDirPath: - . - transformerName: KubernetesVersionChanger type: KubernetesOrgYamlsInSource paths: KubernetesYamls: - . ServiceDirPath: - . transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/transformer.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/transformer.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/transformer.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/transformer.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimagespushscript/transformer.yaml CustomClusterSelector: m2kassets/custom/transformer.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/transformer.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/transformer.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/transformer.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/transformer.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/transformer.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/transformer.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/transformer.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/transformer.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/transformer.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/transformer.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/transformer.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/transformer.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/transformer.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/transformer.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/transformer.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/transformer.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/transformer.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/transformer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/transformer.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/transformer.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/transformer.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/transformer.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/transformer.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/transformer.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/transformer.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/transformer.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/transformer.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/transformer.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/transformer.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/transformer.yaml Run the transformation using move2kube transform. $ move2kube transform INFO[0000] Detected a plan file at path /home/user/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: ID: move2kube.transformers.types Hints: [Services that don\u0026#39;t support any of the transformer types you are interested in will be ignored.] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] \u0026gt; [] Maven [] Rust-Dockerfile [] Tomcat [] ZuulAnalyser [] CustomClusterSelector [] Jboss [] Parameterizer [] ReadMeGenerator [] WarRouter [] DockerfileDetector [] Gradle [] Python-Dockerfile [] WinConsoleApp-Dockerfile [] Buildconfig [] EarAnalyser [] Knative [] Nodejs-Dockerfile [] WinSLWebApp-Dockerfile [] ContainerImagesPushScriptGenerator [] DockerfileImageBuildScript [] Tekton [] DotNetCore-Dockerfile [] Liberty [] DockerfileParser [] Golang-Dockerfile [] WinWebApp-Dockerfile [] CloudFoundry [] ComposeGenerator [] Kubernetes [] Ruby-Dockerfile [] WarAnalyser [] ComposeAnalyser [] Jar [] PHP-Dockerfile [] EarRouter [] KubernetesVersionChanger Accept the default by pressing the return or enter key. ? Select all services that are needed: ID: move2kube.services.[].enable Hints: [The services unselected here will be ignored.] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] \u0026gt; [] myproject Accept the detected service. INFO[0017] Starting Plan Transformation INFO[0017] Iteration 1 INFO[0017] Iteration 2 - 1 artifacts to process INFO[0017] Transformer CustomClusterSelector processing 1 artifacts ? Choose the cluster type: ID: move2kube.target.clustertype Hints: [Choose the cluster type you would like to target] [Use arrows to move, type to filter] GCP-GKE IBM-IKS IBM-Openshift Kubernetes \u0026gt; my-kubernetes-cluster Openshift AWS-EKS Openshift Select my-kubernetes-cluster as the target cluster type to deploy to. INFO[0393] Transformer CustomClusterSelector Done INFO[0393] Transformer KubernetesVersionChanger processing 1 artifacts INFO[0393] Transformer KubernetesVersionChanger Done INFO[0393] Created 1 pathMappings and 1 artifacts. Total Path Mappings : 1. Total Artifacts : 1. INFO[0393] Iteration 3 - 1 artifacts to process INFO[0393] Transformer Parameterizer processing 1 artifacts INFO[0393] Transformer Parameterizer Done INFO[0393] Plan Transformation done INFO[0393] Transformed target artifacts can be found at [/Users/user/myproject]. The transformation has completed and generated a directory called myproject. Note: The name of the output directory is the same as the project name (by default myproject). The project name can be changed using the -n flag.\n$ ls customizations kubernetes-to-kubernetes m2k.plan m2kconfig.yaml m2kqacache.yaml myproject $ ls myproject/source kubernetes-to-kubernetes-versionchanged kubernetes-to-kubernetes-versionchanged-parameterized The applications can now be deployed to Kubernetes using these generated artifacts.\nExploring the output The full structure of the output directory can be seen by executing the tree command.\n$ tree myproject/ myproject  source  kubernetes-to-kubernetes-versionchanged   api-deployment.yaml   api-service.yaml   redis-deployment.yaml   redis-service.yaml   web-deployment.yaml   web-ingress.yaml   web-service.yaml  kubernetes-to-kubernetes-versionchanged-parameterized  helm-chart   move2kube-transformers   Chart.yaml   templates    api-deployment.yaml    api-service.yaml    redis-deployment.yaml    redis-service.yaml    web-deployment.yaml    web-ingress.yaml    web-service.yaml   values-dev.yaml   values-prod.yaml   values-staging.yaml  kustomize   base    api-deployment.yaml    api-service.yaml    kustomization.yaml    redis-deployment.yaml    redis-service.yaml    web-deployment.yaml    web-ingress.yaml    web-service.yaml   overlays   dev    apps-v1-deployment-api.yaml    apps-v1-deployment-redis.yaml    apps-v1-deployment-web.yaml    kustomization.yaml   prod    apps-v1-deployment-api.yaml    apps-v1-deployment-redis.yaml    apps-v1-deployment-web.yaml    kustomization.yaml   staging   apps-v1-deployment-api.yaml   apps-v1-deployment-redis.yaml   apps-v1-deployment-web.yaml   kustomization.yaml  openshift-template  parameters-dev.yaml  parameters-prod.yaml  parameters-staging.yaml  template.yaml 13 directories, 42 files The myproject/source/kubernetes-to-kubernetes-versionchanged directory has the new Kubernetes YAMLs (deployment/service/ingress/etc.) which are tailored to meet the target cluster requirements.\nSome other things we can observe:\nThe Helm chart in the source/kubernetes-to-kubernetes-versionchanged-parameterized/helm-chart directory. The Kustomize YAMLs in the source/kubernetes-to-kubernetes-versionchanged-parameterized/kustomize directory. The Openshift Template in the source/kubernetes-to-kubernetes-versionchanged-parameterized/openshift-template directory. For more details on how to customize the parameterization look at the documentation.\nConclusion First, run move2kube collect -a k8s after logging in to your cluster to collect the cluster-related information inside the m2k_collect folder. Then, copy-paste the generated YAML file inside m2k_collect/cluster/ to custom-cluster-selector/clusters/. While running move2kube plan -s src -c custom-cluster-selector (or transform move2kube transform -s src -c custom-cluster-selector), provide the customization folder to Move2Kube using the -c flag. Finally, select the target cluster in the QA during Transform phase. Move2Kube will generate the Kubernetes YAMLs for the target cluster.\nGiven Kubernetes YAMLs, we saw how Move2Kube can create/transform the Kubernetes YAMLs to target a specific cluster.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/createhelmchartskustomize/","title":"Create Helm-charts, Kustomize overlays from Kubernetes Yamls","tags":[],"description":"","content":"In this tutorial we will see how to transform a set of Kubernetes YAMLs by parameterizing them. We can use Move2Kube to generate parameterized Helm charts, Kustomize and Openshift Templates from the Kubernetes YAMLs. Move2Kube can also change the version of Kubernetes resources to target particular clusters.\nPrerequisites\nMove2Kube CLI tool is installed Use a kubernetes-to-kubernetes sample. The kubernetes-to-kubernetes directory has some Kubernetes YAMLs that deploy a web app with multiple services. There are three services: a frontend website in PHP, a backend API in Node.JS, and a cache service using Redis. Procedure\nDownload the kubernetes-to-kubernetes sample. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/kubernetes-to-kubernetes -r move2kube-demos $ ls kubernetes-to-kubernetes/ api-deployment.yaml api-service.yaml redis-deployment.yaml redis-service.yaml web-deployment.yaml web-ingress.yaml web-service.yaml Run move2kube plan -s kubernetes-to-kubernetes/ to generate a plan file. $ move2kube plan -s kubernetes-to-kubernetes/ INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [CloudFoundry] Planning transformation INFO[0000] [CloudFoundry] Done INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [Base Directory] Identified 0 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in . INFO[0000] Identified 1 named services and 0 to-be-named services in . INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 1 named services and 1 to-be-named services INFO[0000] [Named Services] Identified 1 named services INFO[0000] No of services identified : 1 INFO[0000] Plan can be found at [/home/user/m2k.plan]. Look at the generated plan-file in YAML format. $ ls kubernetes-to-kubernetes\tm2k.plan $ cat m2k.plan Notice that Move2Kube has detected all the different services, one for each web app.\naapiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: kubernetes-to-kubernetes services: myproject: - transformerName: KubernetesVersionChanger type: KubernetesOrgYamlsInSource paths: KubernetesYamls: - . ServiceDirPath: - . - transformerName: Parameterizer paths: KubernetesYamls: - . ServiceDirPath: - . transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/buildconfig.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/cloudfoundry.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/clusterselector.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/composeanalyser.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/composegenerator.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimage/containerimagespushscript/containerimagespushscript.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/dockerfiledetector.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/dockerfilebuildscriptgenerator.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/dockerfileparser.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/dotnetcore.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/ear.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/earrouter.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/golang.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/gradle.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/jar.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/jboss.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/knative.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/kubernetes.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/kubernetesversionchanger.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/liberty.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/maven.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/nodejs.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/php.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/parameterizer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/python.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/readmegenerator.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/ruby.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/rust.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/tekton.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/tomcat.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/war.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/warrouter.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/winconsole.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/winsilverlightweb.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/winweb.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/zuulanalyser.yaml Run the transformation using move2kube transform. $ move2kube transform INFO[0000] Detected a plan file at path /home/user/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: ID: move2kube.transformers.types Hints: [Services that don\u0026#39;t support any of the transformer types you are interested in will be ignored.] ComposeAnalyser, PHP-Dockerfile, ReadMeGenerator, Ruby-Dockerfile, Tekton, Buildconfig, Golang-Dockerfile, Jar, Knative, Nodejs-Dockerfile, Parameterizer, CloudFoundry, DockerfileDetector, Kubernetes, Maven, WinWebApp-Dockerfile, Gradle, KubernetesVersionChanger, WarAnalyser, Rust-Dockerfile, WarRouter, ZuulAnalyser, DotNetCore-Dockerfile,EarRouter, Liberty, Python-Dockerfile, Tomcat, ContainerImagesPushScriptGenerator, DockerfileImageBuildScript, DockerfileParser, ClusterSelector, ComposeGenerator, EarAnalyser, Jboss, WinConsoleApp-Dockerfile, WinSLWebApp-Dockerfile ? Select all services that are needed: ID: move2kube.services.[].enable Hints: [The services unselected here will be ignored.] Click to see the remaining output. myproject INFO[0005] Starting Plan Transformation INFO[0005] Iteration 1 INFO[0005] Iteration 2 - 1 artifacts to process INFO[0005] Transformer ClusterSelector processing 1 artifacts ? Choose the cluster type: ID: move2kube.target.clustertype Hints: [Choose the cluster type you would like to target] Kubernetes INFO[0006] Transformer ClusterSelector Done INFO[0006] Transformer KubernetesVersionChanger processing 1 artifacts INFO[0006] Transformer KubernetesVersionChanger Done INFO[0006] Created 1 pathMappings and 1 artifacts. Total Path Mappings : 1. Total Artifacts : 1. INFO[0006] Iteration 3 - 1 artifacts to process INFO[0006] Transformer Parameterizer processing 1 artifacts INFO[0006] Transformer Parameterizer Done INFO[0006] Plan Transformation done INFO[0006] Transformed target artifacts can be found at [/home/user/myproject]. When the questions are complete, wait a few minutes for it to finish processing and view it has generated a directory called myproject. The name of the output directory is the same as the project name (by default myproject). The project name be changed using the -n flag. $ ls kubernetes-to-kubernetes m2k.plan m2kconfig.yaml m2kqacache.yaml myproject $ ls myproject/ source The applications can now be deployed to Kubernetes using these generated artifacts.\nExploring the output The full structure of the output directory can be seen by executing the tree command.\n$ cd myproject/ $ tree .  source  kubernetes-to-kubernetes-versionchanged   api-deployment.yaml   api-service.yaml   redis-deployment.yaml   redis-service.yaml   web-deployment.yaml   web-ingress.yaml   web-service.yaml  kubernetes-to-kubernetes-versionchanged-parameterized  helm-chart   myproject   Chart.yaml   templates    api-deployment.yaml    api-service.yaml    redis-deployment.yaml    redis-service.yaml    web-deployment.yaml    web-ingress.yaml    web-service.yaml   values-dev.yaml   values-prod.yaml   values-staging.yaml  kustomize   base    api-deployment.yaml    api-service.yaml    kustomization.yaml    redis-deployment.yaml    redis-service.yaml    web-deployment.yaml    web-ingress.yaml    web-service.yaml   overlays   dev    apps-v1-deployment-api.yaml    apps-v1-deployment-redis.yaml    apps-v1-deployment-web.yaml    kustomization.yaml   prod    apps-v1-deployment-api.yaml    apps-v1-deployment-redis.yaml    apps-v1-deployment-web.yaml    kustomization.yaml   staging   apps-v1-deployment-api.yaml   apps-v1-deployment-redis.yaml   apps-v1-deployment-web.yaml   kustomization.yaml  openshift-template  parameters-dev.yaml  parameters-prod.yaml  parameters-staging.yaml  template.yaml 13 directories, 42 files Some things to observe:\nThe Helm chart in the source/kubernetes-to-kubernetes-versionchanged-parameterized/helm-chart directory. The Kustomize YAMLs in the source/kubernetes-to-kubernetes-versionchanged-parameterized/kustomize directory. The Openshift Template in the source/kubernetes-to-kubernetes-versionchanged-parameterized/openshift-template directory. In each case, there are three environments dev, staging and prod. It is possible to have different parameterizations for each environment. Notice that the directory name has versionchanged in it. This is because two transformers are currently in play here,\nThe KubernetesVersionChanger transformer, which was asking for the Kubernetes version to target, and which created the version changed yamls to suit the target cluster supported Kinds and versions. The Parameterizer transformer, which was taking the version changed YAMLs and creating Helm charts, Kustomize overlays, and OC templates. If the intention was to retain the kind and verisons of Kubernetes YAMLs, disable the KubernetesVersionChanger transformer either in the QA, configuration, or in plan, and then the YAMLs will be parameterized as is.\nFor more details on how to customize the parameterization that Move2Kube does look at the documentation.\nConclusion Given Kubernetes YAMLs, we saw how Move2Kube can help us parameterize them and generate Helm charts, Kustomize, Openshift Templates, etc. Move2Kube is also capable of changing the versions of various Kubernetes resources to match the target cluster.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/createwincontainersnet/","title":"Create and deploy Windows .NET containers","tags":[],"description":"","content":".NET applications in 4.x framework In this tutorial, we will learn how containerize .NET applications developed for 4.x versions of .NET framework using Windows containers and deploy them to Kubernetes cluster using Move2Kube. Here, we are going to use the sample WCF service from samples/wcfservice.\nPrerequisites Install Move2Kube.\nConfigure Kubernetes with windows node support.\nDownload the samples/wcfservice sample from move2kube-demos repository.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/wcfservice -r move2kube-demos $ tree -L 2 wcfservice/ wcfservice/  wcfservice   App.config   IWindowsSampleService.cs   Properties   WindowsSampleService.cs   wcfservice.cs   wcfservice.csproj  wcfservice.sln 2 directories, 6 files Generating target artifacts We will be using a two stage process for the transformation: plan and transform. Run these steps from the directory containing the ./wcfservice/ directory:\nFirst create a plan of how to transform the applications to run on Kubernetes. In the plan phase, Move2Kube is going to go through the source artifacts and come up with a plan. $ move2kube plan -s wcfservice INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [CloudFoundry] Planning transformation INFO[0000] [CloudFoundry] Done INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [Base Directory] Identified 0 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in . WARN[0000] Unable to find compatible ASP.NET Core target framework hence skipping. INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 1 named services and 0 to-be-named services INFO[0000] [Named Services] Identified 1 named services INFO[0000] No of services identified : 1 INFO[0000] Plan can be found at [/Users/padmanabha/go/src/github.com/seshapad/workdir/dotnet-legacy-test/m2k.plan]. Move2Kube has created a m2k.plan which is essentially a YAML file. See what is inside the plan file. apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: wcfservice services: wcfservice: - transformerName: WinConsoleApp-Dockerfile paths: AppConfigFilePathList: Click to see the rest of the yaml. - wcfservice/App.config ServiceDirPath: - . transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/transformer.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/transformer.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/transformer.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/transformer.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/transformer.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimagespushscript/transformer.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/transformer.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/transformer.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/transformer.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/transformer.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/transformer.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/transformer.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/transformer.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/transformer.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/transformer.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/transformer.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/transformer.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/transformer.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/transformer.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/transformer.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/transformer.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/transformer.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/transformer.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/transformer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/transformer.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/transformer.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/transformer.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/transformer.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/transformer.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/transformer.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/transformer.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/transformer.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/transformer.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/transformer.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/transformer.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/transformer.yaml In the plan, notice that Move2Kube has detected the WCF services (wcfservice) and the relative path of the detected /App.config. The plan file indicates that the applications can be transformed using Move2Kube\u0026rsquo;s built-in WinConsoleApp-Dockerfile transformer. Invoke move2kube transform on this plan. $ move2kube transform INFO[0000] Detected a plan file at path /Users/padmanabha/go/src/github.com/seshapad/workdir/dotnet-legacy-test/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: ID: move2kube.transformers.types Hints: [Services that don\u0026#39;t support any of the transformer types you are interested in will be ignored.] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] [] PHP-Dockerfile [] Gradle [] Kubernetes [] Knative [] Nodejs-Dockerfile [] Tekton [] ZuulAnalyser [] ComposeAnalyser [] Jar [] Liberty [] Python-Dockerfile [] ContainerImagesPushScriptGenerator [] Jboss [] Parameterizer [] WinSLWebApp-Dockerfile [] Buildconfig [] CloudFoundry [] Ruby-Dockerfile [] WinWebApp-Dockerfile [] DockerfileDetector [] Golang-Dockerfile [] WinConsoleApp-Dockerfile [] EarAnalyser [] KubernetesVersionChanger [] Maven [] ClusterSelector [] EarRouter [] DockerfileParser [] DotNetCore-Dockerfile [] ReadMeGenerator [] Rust-Dockerfile [] Tomcat [] WarAnalyser [] ComposeGenerator [] DockerfileImageBuildScript [] WarRouter Accept the default by pressing return or enter key. Hints: [Services that don\u0026#39;t support any of the transformer types you are interested in will be ignored.] PHP-Dockerfile, Gradle, Kubernetes, Knative, Nodejs-Dockerfile, Tekton, ZuulAnalyser, ComposeAnalyser, Jar, Liberty, Python-Dockerfile, ContainerImagesPushScriptGenerator, Jboss, Parameterizer, WinSLWebApp-Dockerfile, Buildconfig, CloudFoundry, Ruby-Dockerfile, WinWebApp-Dockerfile, DockerfileDetector, Golang-Dockerfile, WinConsoleApp-Dockerfile, EarAnalyser, KubernetesVersionChanger, Maven, ClusterSelector, EarRouter, DockerfileParser, DotNetCore-Dockerfile, ReadMeGenerator, Rust-Dockerfile, Tomcat, WarAnalyser, ComposeGenerator, DockerfileImageBuildScript, WarRouter ? Select all services that are needed: ID: move2kube.services.[].enable Hints: [The services unselected here will be ignored.] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] [] wcfservice Select all the services. ? Select all services that are needed: ID: move2kube.services.[].enable Hints: [The services unselected here will be ignored.] wcfservice INFO[0233] Starting Plan Transformation INFO[0233] Iteration 1 INFO[0233] Iteration 2 - 1 artifacts to process INFO[0233] Transformer WinConsoleApp-Dockerfile processing 1 artifacts INFO[0233] Transformer WinConsoleApp-Dockerfile Done INFO[0233] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 2. Total Artifacts : 1. INFO[0233] Iteration 3 - 2 artifacts to process INFO[0233] Transformer DockerfileParser processing 1 artifacts WARN[0233] Unable to find ports in Dockerfile : /var/folders/45/5wf_qgcs06gd_xpg6rzvbx0r0000gn/T/move2kube2980808479/environment-DockerfileParser-1624209065/2318519572/source/Dockerfile. Using default port INFO[0233] Transformer ZuulAnalyser processing 2 artifacts INFO[0233] Transformer ZuulAnalyser Done INFO[0233] Transformer DockerfileParser Done INFO[0233] Transformer DockerfileImageBuildScript processing 2 artifacts ? Select the container runtime to use : ID: move2kube.containerruntime Hints: [The container runtime selected will be used in the scripts] [Use arrows to move, type to filter] \u0026gt; docker podman Select runtime of choice. In this case, we select docker. Note: At this point, the default port 8080 is detected and the user is prompted whether to expose this port.\nINFO[0346] Transformer DockerfileImageBuildScript Done INFO[0346] Created 1 pathMappings and 4 artifacts. Total Path Mappings : 3. Total Artifacts : 3. INFO[0346] Iteration 4 - 4 artifacts to process INFO[0346] Transformer ComposeGenerator processing 2 artifacts ? What URL/path should we expose the service wcfservice\u0026#39;s 8080 port on? ID: move2kube.services.\u0026#34;wcfservice\u0026#34;.\u0026#34;8080\u0026#34;.urlpath Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/wcfservice) wcfservice Leave out the leading / to use the first part wcfservice as subdomain (as specified in the Hints). wcfservice ? Provide the minimum number of replicas each service should have ID: move2kube.minreplicas Hints: [If the value is 0 pods won\u0026#39;t be started by default] (2) Accept the default answer for two replicas for the given service. 2 ? Enter the URL of the image registry : Hints: [You can always change it later by changing the yamls.] [Use arrows to move, type to filter] Other (specify custom option) index.docker.io \u0026gt; quay.io us.icr.io Enter quay.io for the image registry host. Select \u0026lsquo;Other\u0026rsquo; if your registry name is not here. quay.io ? Enter the namespace where the new images should be pushed : Hints: [Ex : myproject] (myproject) m2k-tutorial No authentication INFO[0793] Transformer ComposeGenerator Done INFO[0793] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: ID: move2kube.target.clustertype Hints: [Choose the cluster type you would like to target] [Use arrows to move, type to filter] Openshift AWS-EKS Azure-AKS GCP-GKE IBM-IKS IBM-Openshift \u0026gt; Kubernetes Select the Kubernetes cluster type to deploy to. Kubernetes INFO[0863] Transformer ClusterSelector Done INFO[0863] Transformer Knative processing 2 artifacts INFO[0863] Transformer Knative Done INFO[0863] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[0863] Transformer ContainerImagesPushScriptGenerator Done INFO[0863] Transformer ClusterSelector processing 2 artifacts INFO[0863] Transformer ClusterSelector Done INFO[0863] Transformer Buildconfig processing 2 artifacts INFO[0863] Transformer Buildconfig Done INFO[0863] Transformer ClusterSelector processing 2 artifacts INFO[0863] Transformer ClusterSelector Done INFO[0863] Transformer Kubernetes processing 2 artifacts ? Provide the ingress host domain ID: move2kube.target.ingress.host Hints: [Ingress host domain is part of service URL] my-cluster-ingress-host-domain.com Indicate the ingress hosting domain. It can be grabbed from the cluster you are deploying to. The ingress hosting domain will differ based on the cluster you are fetching from. my-cluster-ingress-host-domain.com ? Provide the TLS secret for ingress ID: move2kube.target.ingress.tls Hints: [Leave empty to use http] Accept the by-default for the TLS secret by pressing the \u0026lsquo;return\u0026rsquo; key. INFO[0934] Transformer Kubernetes Done INFO[0934] Transformer ClusterSelector processing 2 artifacts INFO[0934] Transformer ClusterSelector Done INFO[0934] Transformer Tekton processing 2 artifacts INFO[0934] Transformer Tekton Done INFO[0934] Created 27 pathMappings and 7 artifacts. Total Path Mappings : 30. Total Artifacts : 7. INFO[0934] Iteration 5 - 7 artifacts to process INFO[0934] Transformer Parameterizer processing 4 artifacts INFO[0934] Transformer Parameterizer Done INFO[0934] Transformer ReadMeGenerator processing 5 artifacts INFO[0934] Transformer ReadMeGenerator Done INFO[0935] Plan Transformation done INFO[0935] Transformed target artifacts can be found at [/Users/padmanabha/go/src/github.com/seshapad/workdir/dotnet-legacy-test/myproject]. The transformation is successful and the target artifacts can be found inside the ./myproject directory. The overview of the structure of the ./myproject directory can be seen by executing the below command.\n$ tree -L 3 myproject myproject/  Readme.md  deploy   cicd    tekton    tekton-parameterized   compose    docker-compose.yaml   knative    wcfservice-service.yaml   knative-parameterized    helm-chart    kustomize    openshift-template   yamls    myproject-ingress.yaml    wcfservice-deployment.yaml    wcfservice-service.yaml   yamls-parameterized   helm-chart   kustomize   openshift-template  scripts   builddockerimages.bat   builddockerimages.sh   pushimages.bat   pushimages.sh  source  Dockerfile  wcfservice   App.config   IWindowsSampleService.cs   Properties   WindowsSampleService.cs   wcfservice.cs   wcfservice.csproj  wcfservice.sln 19 directories, 17 files Move2Kube has created all the deployment artifacts which are present inside the ./myproject directory. The ./myproject/source directory looks very similar to the wcfservice directory given as input to Move2Kube. But, Move2Kube has placed additional files in the source code. For example, it has added the Dockerfile for each of the transformed services, and with these Dockerfiles, the applications can be containerized and then deployed to a Kubernetes cluster.\nDifferences in Windows applications There are two main differences in Move2Kube Windows application transformations.\nFirst difference is in the deployment YAML of Windows container images (see myproject/deploy/yamls/wcfservice-deployment.yaml in the above example) . The nodeSelector and tolerations ensure that the image is instantiated on a Windows node in the Kubernetes cluster. nodeSelector: kubernetes.io/os: windows restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: os value: Windows Second difference is in the Dockerfile (see myproject/source/Dockerfile in the above example) for the Windows service as shown below. The FROM instruction refers to Windows container images and the --platform indicates that these images have to be built for a Windows platform. FROM --platform=windows/amd64 mcr.microsoft.com/dotnet/framework/sdk:4.8 As builder WORKDIR /app COPY . . RUN msbuild /p:Configuration=Release /p:OutputPath=/app/output FROM --platform=windows/amd64 mcr.microsoft.com/dotnet/framework/runtime:4.8 WORKDIR /app COPY --from=builder /app/output/ . CMD wcfservice.exe Deploying the application to Kubernetes with the generated target artifacts The steps involved to deploy a Windows application is the same as any other application except the container images have to be built on a Windows machine with Docker Desktop set to Windows container mode. Refer to .NET core deployment section for details.\nConclusion In this tutorial, we learned how to migrate multiple .NET applications developed on 4.x .NET framework to Kubernetes using the target artifacts generated by Move2Kube.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/migratedeploynetcore/","title":"Migrate and deploy .NET Core applications to Kubernetes","tags":[],"description":"","content":"In this tutorial, we will learn how to migrate and deploy .NET Core applications to a Kubernetes cluster using the target artifacts generated by Move2Kube. We are going to use the data from samples/dotnet5.\nPrerequisites Install Move2Kube.\nInstall a container runtime: Docker or Podman.\nInstall Kubectl.\nVerify the dependencies were correctly installed.\n$ docker version or\n$ podman info $ kubectl version Download the samples/dotnet5 sample from move2kube-demos repository.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/dotnet5 -r move2kube-demos View the structure inside the ./dotnet5 directory and the four different applications: dotnetwebapp - an ASP.NET Core web application.\ndotnetangular - an ASP.NET Core application with Angular for client-side code.\ndotnetreact - an ASP.NET Core application with React for client-side code.\ndotnetreact-redux - an ASP.NET Core application with React and Redux for client-side code.\n$ tree dotnet5 -L 2 dotnet5  dotnet5angular   ClientApp   Controllers   Pages   Program.cs   Properties   Startup.cs   WeatherForecast.cs   appsettings.Development.json   appsettings.json   dotnet5angular.csproj   dotnet5angular.sln   wwwroot  dotnet5react   ClientApp   Controllers   Pages   Program.cs   Properties   Startup.cs   WeatherForecast.cs   appsettings.Development.json   appsettings.json   dotnet5react.csproj   dotnet5react.sln   log.cs   obj  dotnet5react-redux   ClientApp   Controllers   Pages   Program.cs   Properties   Startup.cs   WeatherForecast.cs   appsettings.Development.json   appsettings.json   dotnet5react-redux.csproj   dotnet5react-redux.sln  dotnet5webapp  Pages  Program.cs  Properties  Startup.cs  appsettings.Development.json  appsettings.json  dotnet5webapp.csproj  dotnet5webapp.sln  wwwroot 21 directories, 28 files Generating target artifacts We will be using the two stage process (plan and transform) for the transformation. Run these steps from the directory containing the ./dotnet5/ directory:\nCreate a plan on how to transform the applications to run on Kubernetes. In the plan phase, Move2Kube will go through the source artifacts and generate a plan. $ cd samples $ move2kube plan -s dotnet5 INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [CloudFoundry] Planning transformation INFO[0000] [CloudFoundry] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [Base Directory] Identified 0 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in dotnet5angular INFO[0000] Identified 1 named services and 0 to-be-named services in dotnet5react INFO[0000] Identified 1 named services and 0 to-be-named services in dotnet5react-redux INFO[0000] Identified 1 named services and 0 to-be-named services in dotnet5webapp INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 4 named services and 0 to-be-named services INFO[0000] [Named Services] Identified 4 named services INFO[0000] No of services identified : 4 INFO[0000] Plan can be found at [/Users/user/github/move2kube-demos/samples/m2k.plan]. Move2Kube has created a m2k.plan which is essentially a yaml file.\nView what is inside the plan file. apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: dotnet5 services: dotnet5angular: - transformerName: DotNetCore-Dockerfile paths: DotNetCoreCsprojPathType: - dotnet5angular/dotnet5angular.csproj DotNetCoreSolutionPathType: - dotnet5angular/dotnet5angular.sln ServiceDirPath: - dotnet5angular dotnet5react: - transformerName: DotNetCore-Dockerfile paths: DotNetCoreCsprojPathType: - dotnet5react/dotnet5react.csproj DotNetCoreSolutionPathType: - dotnet5react/dotnet5react.sln ServiceDirPath: - dotnet5react dotnet5react-redux: - transformerName: DotNetCore-Dockerfile paths: DotNetCoreCsprojPathType: - dotnet5react-redux/dotnet5react-redux.csproj DotNetCoreSolutionPathType: - dotnet5react-redux/dotnet5react-redux.sln ServiceDirPath: - dotnet5react-redux dotnet5webapp: - transformerName: DotNetCore-Dockerfile paths: DotNetCoreCsprojPathType: - dotnet5webapp/dotnet5webapp.csproj DotNetCoreSolutionPathType: - dotnet5webapp/dotnet5webapp.sln ServiceDirPath: - dotnet5webapp transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/buildconfig.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/cloudfoundry.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/clusterselector.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/composeanalyser.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/composegenerator.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimage/containerimagespushscript/containerimagespushscript.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/dockerfiledetector.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/dockerfilebuildscriptgenerator.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/dockerfileparser.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/dotnetcore.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/ear.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/earrouter.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/golang.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/gradle.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/jar.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/jboss.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/knative.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/kubernetes.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/kubernetesversionchanger.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/liberty.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/maven.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/nodejs.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/php.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/parameterizer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/python.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/readmegenerator.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/ruby.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/rust.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/tekton.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/tomcat.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/war.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/warrouter.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/winconsole.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/winsilverlightweb.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/winweb.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/zuulanalyser.yaml In the plan, notice that Move2Kube has detected all the four services (dotnet5webapp, dotnet5angular, dotnet5react-redux, dotnet5react) and relative paths of the detected .csproj and/or .sln files for each of the services. The plan file indicates the applications can be transformed using Move2Kube\u0026rsquo;s built-in DotNetCore-Dockerfile transformer. Run move2kube transform on this plan. $ move2kube transform INFO[0000] Detected a plan file at path /Users/username/github/move2kube-demos/samples/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: Hints: [Services that don\u0026#39;t support any of the transformer types you are interested in will be ignored.] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] \u0026gt; [] Jboss [] Kubernetes [] Liberty [] ZuulAnalyser [] CloudFoundry [] DockerfileParser [] Golang-Dockerfile [] Gradle [] Python-Dockerfile [] ReadMeGenerator [] ContainerImagesPushScriptGenerator [] DotNetCore-Dockerfile [] Parameterizer [] Tekton [] WinSLWebApp-Dockerfile [] Buildconfig [] ClusterSelector [] ComposeAnalyser [] PHP-Dockerfile [] Ruby-Dockerfile [] WinConsoleApp-Dockerfile [] DockerfileImageBuildScript [] EarRouter [] Rust-Dockerfile [] Tomcat [] WarAnalyser [] EarAnalyser [] Knative [] Maven [] WinWebApp-Dockerfile [] ComposeGenerator [] Jar [] DockerfileDetector [] KubernetesVersionChanger [] Nodejs-Dockerfile [] WarRouter Accpet the default by pressing the return or enter key. Jboss, Kubernetes, Liberty, ZuulAnalyser, CloudFoundry, DockerfileParser, Golang-Dockerfile, Gradle, Python-Dockerfile, ReadMeGenerator, ContainerImagesPushScriptGenerator, DotNetCore-Dockerfile, Parameterizer, Tekton, WinSLWebApp-Dockerfile, Buildconfig, ClusterSelector, ComposeAnalyser, PHP-Dockerfile, Ruby-Dockerfile, WinConsoleApp-Dockerfile, DockerfileImageBuildScript, EarRouter, Rust-Dockerfile, Tomcat, WarAnalyser, EarAnalyser, Knative, Maven, WinWebApp-Dockerfile, ComposeGenerator, Jar, DockerfileDetector, KubernetesVersionChanger, Nodejs-Dockerfile, WarRouter ? Select all services that are needed: Hints: [The services unselected here will be ignored.] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] \u0026gt; [] dotnet5angular [] dotnet5react [] dotnet5react-redux [] dotnet5webapp Select all the services. dotnet5angular, dotnet5react, dotnet5react-redux, dotnet5webapp INFO[0068] Starting Plan Transformation INFO[0068] Iteration 1 INFO[0068] Iteration 2 - 4 artifacts to process INFO[0068] Transformer DotNetCore-Dockerfile processing 4 artifacts ? Select ports to be exposed for the service dotnet5angular : Hints: [Select Other if you want to add more ports] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] \u0026gt; [] 5000 [ ] Other (specify custom option) Select port 5000 detected in the source code to expose the dotnet5angular service. 5000 ? Select ports to be exposed for the service dotnet5react : Hints: [Select Other if you want to add more ports] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] \u0026gt; [] 5000 [ ] Other (specify custom option) Select port 5000 for the dotnet5react service. 5000 ? Select ports to be exposed for the service dotnet5react-redux : Hints: [Select Other if you want to add more ports] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] \u0026gt; [] 5000 [ ] Other (specify custom option) Accept the default for the dotnet5react-redux service. 5000 ? Select ports to be exposed for the service dotnet5webapp : Hints: [Select Other if you want to add more ports] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] \u0026gt; [] 5000 [ ] Other (specify custom option) Accept the default for the dotnetwebapp service. 5000 INFO[1152] Transformer DotNetCore-Dockerfile Done INFO[1152] Created 8 pathMappings and 8 artifacts. Total Path Mappings : 8. Total Artifacts : 4. INFO[1152] Iteration 3 - 8 artifacts to process INFO[1152] Transformer DockerfileImageBuildScript processing 5 artifacts ? Select the container runtime to use : Hints: [The container runtime selected will be used in the scripts] [Use arrows to move, type to filter] docker \u0026gt; podman Select the container runtime you want to use. For this tutorial, select podman as the container runtime. podman INFO[1274] Transformer DockerfileImageBuildScript Done INFO[1274] Transformer DockerfileParser processing 4 artifacts INFO[1274] Transformer ZuulAnalyser processing 2 artifacts INFO[1274] Transformer ZuulAnalyser Done INFO[1274] Transformer DockerfileParser Done INFO[1275] Created 1 pathMappings and 7 artifacts. Total Path Mappings : 9. Total Artifacts : 12. INFO[1275] Iteration 4 - 7 artifacts to process INFO[1275] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: Hints: [Choose the cluster type you would like to target] [Use arrows to move, type to filter] IBM-IKS IBM-Openshift \u0026gt; Kubernetes Openshift AWS-EKS Azure-AKS GCP-GKE Select the Kubernetes cluster type. Kubernetes INFO[1331] Transformer ClusterSelector Done INFO[1331] Transformer Kubernetes processing 2 artifacts ? What URL/path should we expose the service dotnet5webapp\u0026#39;s 5000 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/dotnet5webapp) dotnetwebapp Leave out the leading / to use the first part dotnetwebapp as subdomain (as specified in the Hints). dotnetwebapp ? What URL/path should we expose the service dotnet5react-redux\u0026#39;s 5000 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/dotnet5react-redux) dotnet5react-redux Leave out the leading / to use the first part dotnetreact-redux as subdomain. dotnet5react-redux ? What URL/path should we expose the service dotnet5react\u0026#39;s 5000 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/dotnet5react) dotnet5react Leave out the leading / for the dotnet5react service to use the first part dotnetreact as subdomain. dotnet5react ? What URL/path should we expose the service dotnet5angular\u0026#39;s 5000 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/dotnet5angular) dotnet5angular Leave out the leading / to use the first part dotnetangular as subdomain. dotnet5angular ? Provide the minimum number of replicas each service should have Hints: [If the value is 0 pods won\u0026#39;t be started by default] (2) Accept the default for two replicas for each service. 2 ? Enter the URL of the image registry : Hints: [You can always change it later by changing the yamls.] [Use arrows to move, type to filter] Other (specify custom option) index.docker.io \u0026gt; quay.io us.icr.io Select quay.io for the image registry host. Select \u0026lsquo;Other\u0026rsquo; if your registry name is not here. quay.io ? Enter the namespace where the new images should be pushed : Hints: [Ex : myproject] (myproject) m2k-tutorial Input the namespace to deploy- m2k-tutorial. (For example, namespace m2k-tutorial is in quay.io) m2k-tutorial ? [quay.io] What type of container registry login do you want to use? Hints: [Docker login from config mode, will use the default config from your local machine.] [Use arrows to move, type to filter] Use existing pull secret \u0026gt; No authentication UserName/Password Select the container registry login type. No authentication ? Provide the ingress host domain Hints: [Ingress host domain is part of service URL] (myproject.com) my-cluster-ingress-host-domain.com Input the ingress hosting domain. It can be grabbed from the cluster you are deploying to. The ingress hosting domain will differ based on the cluster you are fetching from. my-cluster-ingress-host-domain.com ? Provide the TLS secret for ingress Hints: [Leave empty to use http] Accept the by-default TLS secret by pressing the \u0026lsquo;return\u0026rsquo; key. INFO[1850] Transformer Kubernetes Done INFO[1850] Transformer ComposeGenerator processing 2 artifacts INFO[1851] Transformer ComposeGenerator Done INFO[1851] Transformer ClusterSelector processing 2 artifacts INFO[1851] Transformer ClusterSelector Done INFO[1851] Transformer Buildconfig processing 2 artifacts INFO[1851] Transformer Buildconfig Done INFO[1851] Transformer ClusterSelector processing 2 artifacts INFO[1851] Transformer ClusterSelector Done INFO[1851] Transformer Knative processing 2 artifacts INFO[1851] Transformer Knative Done INFO[1851] Transformer ClusterSelector processing 2 artifacts INFO[1851] Transformer ClusterSelector Done INFO[1851] Transformer Tekton processing 2 artifacts INFO[1851] Transformer Tekton Done INFO[1851] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[1851] Transformer ContainerImagesPushScriptGenerator Done INFO[1852] Created 33 pathMappings and 7 artifacts. Total Path Mappings : 42. Total Artifacts : 19. INFO[1852] Iteration 5 - 7 artifacts to process INFO[1852] Transformer Parameterizer processing 4 artifacts INFO[1852] Transformer Parameterizer Done INFO[1852] Transformer ReadMeGenerator processing 5 artifacts INFO[1852] Transformer ReadMeGenerator Done INFO[1852] Plan Transformation done INFO[1852] Transformed target artifacts can be found at [/Users/username/github/move2kube-demos/samples/myproject]. The transformation is successful and the target artifacts can be found inside the ./myproject directory.\nView the ./myproject directory structure. $ tree myproject -L 3 myproject  Readme.md  deploy   cicd    tekton    tekton-parameterized   compose    docker-compose.yaml   knative    dotnet5angular-service.yaml    dotnet5react-redux-service.yaml    dotnet5react-service.yaml    dotnet5webapp-service.yaml   knative-parameterized    helm-chart    kustomize    openshift-template   yamls    dotnet5angular-deployment.yaml    dotnet5angular-service.yaml    dotnet5react-deployment.yaml    dotnet5react-redux-deployment.yaml    dotnet5react-redux-service.yaml    dotnet5react-service.yaml    dotnet5webapp-deployment.yaml    dotnet5webapp-service.yaml    myproject-ingress.yaml   yamls-parameterized   helm-chart   kustomize   openshift-template  scripts   builddockerimages.bat   builddockerimages.sh   pushimages.bat   pushimages.sh  source  dotnet5angular   ClientApp   Controllers   Dockerfile   Pages   Program.cs   Properties   Startup.cs   WeatherForecast.cs   appsettings.Development.json   appsettings.json   dotnet5angular.csproj   dotnet5angular.sln   wwwroot  dotnet5react   ClientApp   Controllers   Dockerfile   Pages   Program.cs   Properties   Startup.cs   WeatherForecast.cs   appsettings.Development.json   appsettings.json   dotnet5react.csproj   dotnet5react.sln   log.cs   obj  dotnet5react-redux   ClientApp   Controllers   Dockerfile   Pages   Program.cs   Properties   Startup.cs   WeatherForecast.cs   appsettings.Development.json   appsettings.json   dotnet5react-redux.csproj   dotnet5react-redux.sln  dotnet5webapp  Dockerfile  Pages  Program.cs  Properties  Startup.cs  appsettings.Development.json  appsettings.json  dotnet5webapp.csproj  dotnet5webapp.sln  wwwroot 38 directories, 51 files Move2Kube has created all the deployment artifacts which inside the ./myproject directory. The ./myproject/source directory looks very similar to the directory dotnet5 that we gave as input but Move2Kube has added files to the source code. For example, it has added the Dockerfile for each of the transformed services, and with these dockerfiles, the applications can be containerized and then deployed to a Kubernetes cluster.\nDeploying the application to Kubernetes with the generated target artifacts View the ./myproject directory. $ cd myproject/ $ ls Readme.md deploy scripts source Run the builddockerimages.sh script inside the ./myproject/scripts directory. This step may take some time to complete. $ cd scripts $ ./builddockerimages.sh [1/2] STEP 1/7: FROM mcr.microsoft.com/dotnet/sdk:5.0 AS builder [1/2] STEP 2/7: WORKDIR /src --\u0026gt; Using cache d1926570d7a610945da9057c04ddf60c23a1030f344dc62eb82a31ba0d42bed2 --\u0026gt; d1926570d7a [1/2] STEP 3/7: COPY . . --\u0026gt; Using cache 3593fc41a77a76df0e1bc15715990cecffdf4ae24dcd57df772b3465d9d10f53 --\u0026gt; 3593fc41a77 [1/2] STEP 4/7: RUN mkdir app --\u0026gt; Using cache 6b5c375d46df984c99adabf28bedab6afd5a309b8ce3a372fad5929da741ba37 --\u0026gt; 6b5c375d46d [1/2] STEP 5/7: RUN dotnet restore dotnet5angular.csproj --\u0026gt; Using cache d9f3086339e4fa9510696cc33d6af9b3e022c3ab6a3797b20744c3616b821d0c --\u0026gt; d9f3086339e [1/2] STEP 6/7: RUN curl https://deb.nodesource.com/setup_10.x -o setup_10.x \u0026amp;\u0026amp; bash setup_10.x \u0026amp;\u0026amp; apt-get install -y build-essential nodejs --\u0026gt; Using cache 71a9b96d0549f313f09b5e90c23cd580b66ec1d829e0a380a8db41e6d4198c10 --\u0026gt; 71a9b96d054 [1/2] STEP 7/7: RUN dotnet publish dotnet5angular.csproj -c Release -o /src/app/publish --\u0026gt; Using cache 7f5deecfba502398540b90a73541874bf7a4a50a07524cc8320fcf46f8c82ee9 --\u0026gt; 7f5deecfba5 [2/2] STEP 1/6: FROM mcr.microsoft.com/dotnet/aspnet:5.0 [2/2] STEP 2/6: WORKDIR /app --\u0026gt; Using cache c843aafccaaed89f76c5b5f906d811f5bc2c4316856e3daa9b0761c2672f222c --\u0026gt; c843aafccaa [2/2] STEP 3/6: EXPOSE 5000 --\u0026gt; Using cache 54a3b726dcfb236ec8a6b474d45354fe9076015389503e09f5f356226bf459cc --\u0026gt; 54a3b726dcf [2/2] STEP 4/6: ENV ASPNETCORE_URLS=http://+:5000 --\u0026gt; Using cache f821c90b7bea4c8473d0f0565c8afcc1709a85de1bfe4458211956c6b79a67ef --\u0026gt; f821c90b7be [2/2] STEP 5/6: COPY --from=builder /src/app/publish . --\u0026gt; Using cache 4a7ffdd712359b6b018b26f2ebb09e01ba400b6862507bd70223852a1285c642 --\u0026gt; 4a7ffdd7123 [2/2] STEP 6/6: CMD [\u0026#34;dotnet\u0026#34;, \u0026#34;dotnet5angular.dll\u0026#34;] --\u0026gt; Using cache bfaead65def42103a1d5aaf4fa1b06d6be874b216e190614618a37ead13248fb [2/2] COMMIT dotnet5angular --\u0026gt; bfaead65def Successfully tagged localhost/dotnet5angular:latest bfaead65def42103a1d5aaf4fa1b06d6be874b216e190614618a37ead13248fb /Users/username/github/move2kube-demos/samples/myproject [1/2] STEP 1/7: FROM mcr.microsoft.com/dotnet/sdk:5.0 AS builder [1/2] STEP 2/7: WORKDIR /src --\u0026gt; Using cache d1926570d7a610945da9057c04ddf60c23a1030f344dc62eb82a31ba0d42bed2 --\u0026gt; d1926570d7a [1/2] STEP 3/7: COPY . . --\u0026gt; Using cache 92870100a90703664900aebacef35d0104f40154dd24df7d227c3de8af935d97 --\u0026gt; 92870100a90 [1/2] STEP 4/7: RUN mkdir app --\u0026gt; Using cache 2c049f9bd9efa5d3c847e1de3b68514158c45e1c5754f5180a3a1cf4b418dd92 --\u0026gt; 2c049f9bd9e [1/2] STEP 5/7: RUN dotnet restore dotnet5react.csproj --\u0026gt; Using cache 8f181b1b25a49666a999edd22a7f8d5b7b334d2961cb795133e372c78c0eae96 --\u0026gt; 8f181b1b25a [1/2] STEP 6/7: RUN curl https://deb.nodesource.com/setup_10.x -o setup_10.x \u0026amp;\u0026amp; bash setup_10.x \u0026amp;\u0026amp; apt-get install -y build-essential nodejs --\u0026gt; Using cache deba3a713958ae310b04f93c92b138ab33d39c5198c03ff9f2c62dda1a5e6095 --\u0026gt; deba3a71395 [1/2] STEP 7/7: RUN dotnet publish dotnet5react.csproj -c Release -o /src/app/publish --\u0026gt; Using cache a44da7d550a108d66cf8311f1c3f4813520b777df9d69d7b7c8e0ebebd5b92d1 --\u0026gt; a44da7d550a [2/2] STEP 1/6: FROM mcr.microsoft.com/dotnet/aspnet:5.0 [2/2] STEP 2/6: WORKDIR /app --\u0026gt; Using cache c843aafccaaed89f76c5b5f906d811f5bc2c4316856e3daa9b0761c2672f222c --\u0026gt; c843aafccaa [2/2] STEP 3/6: EXPOSE 5000 --\u0026gt; Using cache 54a3b726dcfb236ec8a6b474d45354fe9076015389503e09f5f356226bf459cc --\u0026gt; 54a3b726dcf [2/2] STEP 4/6: ENV ASPNETCORE_URLS=http://+:5000 --\u0026gt; Using cache f821c90b7bea4c8473d0f0565c8afcc1709a85de1bfe4458211956c6b79a67ef --\u0026gt; f821c90b7be [2/2] STEP 5/6: COPY --from=builder /src/app/publish . --\u0026gt; Using cache 219fed27ab510fc9767f69f733443ef07483027be1f96fcc252e46ac4f5b1b70 --\u0026gt; 219fed27ab5 [2/2] STEP 6/6: CMD [\u0026#34;dotnet\u0026#34;, \u0026#34;dotnet5react.dll\u0026#34;] --\u0026gt; Using cache a2f7eae2981cc0c7a8c843c017457e761b55afc26321616e6d99b31a2eadffa4 [2/2] COMMIT dotnet5react --\u0026gt; a2f7eae2981 Successfully tagged localhost/dotnet5react:latest a2f7eae2981cc0c7a8c843c017457e761b55afc26321616e6d99b31a2eadffa4 /Users/username/github/move2kube-demos/samples/myproject [1/2] STEP 1/7: FROM mcr.microsoft.com/dotnet/sdk:5.0 AS builder [1/2] STEP 2/7: WORKDIR /src --\u0026gt; Using cache d1926570d7a610945da9057c04ddf60c23a1030f344dc62eb82a31ba0d42bed2 --\u0026gt; d1926570d7a [1/2] STEP 3/7: COPY . . --\u0026gt; Using cache 8ebf36b68a9eba59618911849554f61a5040b11102fb498ed54d110549b4a1dc --\u0026gt; 8ebf36b68a9 [1/2] STEP 4/7: RUN mkdir app --\u0026gt; Using cache 8336566b56a2d64f6198107edbfd29a62fe34f9e3827a4fdad7a268b61302d8c --\u0026gt; 8336566b56a [1/2] STEP 5/7: RUN dotnet restore dotnet5react-redux.csproj --\u0026gt; Using cache f27fbe5bd7809033734ed557ea7b180266865ea62756bc5160faacc0fc68f2e0 --\u0026gt; f27fbe5bd78 [1/2] STEP 6/7: RUN curl https://deb.nodesource.com/setup_10.x -o setup_10.x \u0026amp;\u0026amp; bash setup_10.x \u0026amp;\u0026amp; apt-get install -y build-essential nodejs --\u0026gt; Using cache 7891b1551dc508625ef6ca1767e63fc745e1fd2fd1194ec3b66075b259ad1368 --\u0026gt; 7891b1551dc [1/2] STEP 7/7: RUN dotnet publish dotnet5react-redux.csproj -c Release -o /src/app/publish --\u0026gt; Using cache f3f81341502db6f921de5d49f10d9ef8295de7b288021559b58f681cd33a5151 --\u0026gt; f3f81341502 [2/2] STEP 1/6: FROM mcr.microsoft.com/dotnet/aspnet:5.0 [2/2] STEP 2/6: WORKDIR /app --\u0026gt; Using cache c843aafccaaed89f76c5b5f906d811f5bc2c4316856e3daa9b0761c2672f222c --\u0026gt; c843aafccaa [2/2] STEP 3/6: EXPOSE 5000 --\u0026gt; Using cache 54a3b726dcfb236ec8a6b474d45354fe9076015389503e09f5f356226bf459cc --\u0026gt; 54a3b726dcf [2/2] STEP 4/6: ENV ASPNETCORE_URLS=http://+:5000 --\u0026gt; Using cache f821c90b7bea4c8473d0f0565c8afcc1709a85de1bfe4458211956c6b79a67ef --\u0026gt; f821c90b7be [2/2] STEP 5/6: COPY --from=builder /src/app/publish . --\u0026gt; Using cache ec3f3ff41508d0f77d2295ec1b8a1aba5e87dffe277a0624fda4564c0ad67f54 --\u0026gt; ec3f3ff4150 [2/2] STEP 6/6: CMD [\u0026#34;dotnet\u0026#34;, \u0026#34;dotnet5react-redux.dll\u0026#34;] --\u0026gt; Using cache ca7f612ddd72d54352a86780e5d2afe90d19a3a28bca0c2dcaf308d51f38f745 [2/2] COMMIT dotnet5react-redux --\u0026gt; ca7f612ddd7 Successfully tagged localhost/dotnet5react-redux:latest ca7f612ddd72d54352a86780e5d2afe90d19a3a28bca0c2dcaf308d51f38f745 /Users/username/github/move2kube-demos/samples/myproject [1/2] STEP 1/6: FROM mcr.microsoft.com/dotnet/sdk:5.0 AS builder [1/2] STEP 2/6: WORKDIR /src --\u0026gt; Using cache d1926570d7a610945da9057c04ddf60c23a1030f344dc62eb82a31ba0d42bed2 --\u0026gt; d1926570d7a [1/2] STEP 3/6: COPY . . --\u0026gt; Using cache 595b1552996ad3e8b3645b83871fc31bd6fe414a4225f25354f9c64e1aaeb05b --\u0026gt; 595b1552996 [1/2] STEP 4/6: RUN mkdir app --\u0026gt; Using cache dc8a29f1f85b391c9eb80c04b3d70c45491fef8ff6d7bb28a56fcb1aee4053e9 --\u0026gt; dc8a29f1f85 [1/2] STEP 5/6: RUN dotnet restore dotnet5webapp.csproj --\u0026gt; Using cache e6041315f657d10dabd5efe12c3f870486906750219405007b02464521aa3950 --\u0026gt; e6041315f65 [1/2] STEP 6/6: RUN dotnet publish dotnet5webapp.csproj -c Release -o /src/app/publish --\u0026gt; Using cache f50e0d617960c1c3f4cfdfbb39d413cd537b89c5422a45cf8738464c8e5b1f3d --\u0026gt; f50e0d61796 [2/2] STEP 1/6: FROM mcr.microsoft.com/dotnet/aspnet:5.0 [2/2] STEP 2/6: WORKDIR /app --\u0026gt; Using cache c843aafccaaed89f76c5b5f906d811f5bc2c4316856e3daa9b0761c2672f222c --\u0026gt; c843aafccaa [2/2] STEP 3/6: EXPOSE 5000 --\u0026gt; Using cache 54a3b726dcfb236ec8a6b474d45354fe9076015389503e09f5f356226bf459cc --\u0026gt; 54a3b726dcf [2/2] STEP 4/6: ENV ASPNETCORE_URLS=http://+:5000 --\u0026gt; Using cache f821c90b7bea4c8473d0f0565c8afcc1709a85de1bfe4458211956c6b79a67ef --\u0026gt; f821c90b7be [2/2] STEP 5/6: COPY --from=builder /src/app/publish . --\u0026gt; Using cache fc48c345e77eadbf994b567db6cae594c1212cb768b392b86470521be13fbe1e --\u0026gt; fc48c345e77 [2/2] STEP 6/6: CMD [\u0026#34;dotnet\u0026#34;, \u0026#34;dotnet5webapp.dll\u0026#34;] --\u0026gt; Using cache 564f1f4a6cb6437c428d72f0c607281bacd9ffe3913c8dde6b64ad9c32832c7a [2/2] COMMIT dotnet5webapp --\u0026gt; 564f1f4a6cb Successfully tagged localhost/dotnet5webapp:latest 564f1f4a6cb6437c428d72f0c607281bacd9ffe3913c8dde6b64ad9c32832c7a /Users/username/github/move2kube-demos/samples/myproject done Run the pushimages.sh script to push the application images to the registry specified during the transform phase. For this step, you are required to log in to your Docker registry. To log in to quay.io, run podman login quay.io (or docker login quay.io, depending upon the container runtime selected during transform phase). To log in to IBM Cloud us.icr.io registry refer here. $ ./pushimages.sh Note: If you have pushed the image repository to quay.io, then in the Repository\u0026rsquo;s Visibility in quay.io repository\u0026rsquo;s Settings, select whether you want the repository to be public or private so that it can be properly accessed by the Kubernetes cluster.\nDeploy the application with kubectl apply command using the yaml files which Move2Kube created inside the ./myproject/deploy/yamls directory including deployment artifacts, service artifacts and ingress artifact for all the services. $ cd .. $ kubectl apply -f deploy/yamls deployment.apps/dotnet5angular created service/dotnet5angular created deployment.apps/dotnet5react created deployment.apps/dotnet5react-redux created service/dotnet5react-redux created service/dotnet5react created deployment.apps/dotnet5webapp created service/dotnet5webapp created ingress.networking.k8s.io/myproject created The application is now accessible on the cluster.\nCheck the pod status. $ kubectl get pods NAME READY STATUS RESTARTS AGE dotnet5angular-746b886f85-jjprw 1/1 Running 0 45s dotnet5angular-746b886f85-tfv8r 1/1 Running 0 45s dotnet5react-866bdb54cf-87n6v 1/1 Running 0 38s dotnet5react-866bdb54cf-g9vbk 1/1 Running 0 38s dotnet5react-redux-5d95d8789-v4dfn 1/1 Running 0 34s dotnet5react-redux-5d95d8789-vz47d 1/1 Running 0 34s dotnet5webapp-5c88c67bf-cdmf2 1/1 Running 0 26s dotnet5webapp-5c88c67bf-j68hd 1/1 Running 0 26s The deployment and service and ingress have been created.\nGet the ingress to see the URL where the app has been deployed to. $ kubectl get ingress myproject Conclusion In this tutorial, we learned how to migrate multiple .NET Core applications to Kubernetes using the target artifacts generated by Move2Kube.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/migratedockercomposekube/","title":"Migrate from Docker Compose to Kubernetes","tags":[],"description":"","content":"Summary $ move2kube transform -s docker-compose Move2Kube automatically analyzes all the yaml files in the docker-compose directory and transforms and creates all artifacts required for deploying the application in Kubernetes.\nPrerequisites Install the Move2Kube CLI tool. Note: This tutorial has been created with v0.3.3-rc.2 version of Move2Kube.\n$ MOVE2KUBE_TAG=\u0026#39;v0.3.3-rc.2\u0026#39; bash \u0026lt;(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Install a Kubernetes cluster from MiniKube. Overview In this tutorial we will migrate an application written for Docker Compose to run on Kubernetes using the two Docker Compose samples from the move2kube-demos repo.\nSample 1 is a web app with a single service using Nginx and a prebuilt image.\nSample 2 is more complicated. It is also a web app but it has three services.\nA frontend written in PHP for Apache An API backend written for NodeJS A service for caching the calculations performed by the backend. For the cache service we use a prebuilt Redis image.\nBelow are the steps for migrating the second sample. The steps for the first sample are similar except that since it uses prebuilt images, you can skip the build and push the images portion.\nProcedure\nDownload the samples/docker-compose/multiple-services sample. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/docker-compose/multiple-services -r move2kube-demos $ ls multiple-services Run the planning phase. $ move2kube plan -s multiple-services/ INFO[0000] Configuration loading done INFO[0000] Start planning INFO[0000] Planning started on the base directory INFO[0000] [CloudFoundry] Planning INFO[0000] [CloudFoundry] Done INFO[0000] [ComposeAnalyser] Planning INFO[0000] Identified 3 named services and 0 to-be-named services INFO[0000] [ComposeAnalyser] Done INFO[0000] [DockerfileDetector] Planning INFO[0000] Identified 1 named services and 1 to-be-named services INFO[0000] [DockerfileDetector] Done INFO[0000] [Base Directory] Identified 4 named services and 1 to-be-named services INFO[0000] Planning finished on the base directory INFO[0000] Planning started on its sub directories INFO[0000] Identified 1 named services and 0 to-be-named services in api INFO[0000] Identified 1 named services and 0 to-be-named services in web INFO[0000] Planning finished on its sub directories INFO[0000] [Directory Walk] Identified 4 named services and 2 to-be-named services INFO[0000] [Named Services] Identified 3 named services INFO[0000] Planning done INFO[0000] No of services identified : 3 INFO[0000] Plan can be found at [/Users/user/Desktop/tutorial/m2k.plan] Inspect the plan to verify all three services were detected. $ cat m2k.plan apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: multiple-services services: api: - transformerName: ComposeAnalyser paths: DockerCompose: - docker-compose.yaml Dockerfile: - api/Dockerfile ServiceDirectories: - api configs: ComposeService: serviceName: api - transformerName: Nodejs-Dockerfile paths: ServiceDirectories: - api - transformerName: DockerfileDetector paths: Dockerfile: - api/Dockerfile ServiceDirectories: - api redis: - transformerName: ComposeAnalyser paths: DockerCompose: - docker-compose.yaml configs: ComposeService: serviceName: redis web: - transformerName: ComposeAnalyser paths: DockerCompose: - docker-compose.yaml Dockerfile: - web/Dockerfile ServiceDirectories: - web configs: ComposeService: serviceName: web - transformerName: DockerfileDetector paths: Dockerfile: - web/Dockerfile ServiceDirectories: - web - transformerName: PHP-Dockerfile paths: ServiceDirectories: - web transformers: ArgoCD: m2kassets/built-in/transformers/kubernetes/argocd/transformer.yaml Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/transformer.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/transformer.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/transformer.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/transformer.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/transformer.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimagespushscript/transformer.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/transformer.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/transformer.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/transformer.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/transformer.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/transformer.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/transformer.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/transformer.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/transformer.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/transformer.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/transformer.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/transformer.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/transformer.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/transformer.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/transformer.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/transformer.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/transformer.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/transformer.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/transformer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/transformer.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/transformer.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/transformer.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/transformer.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/transformer.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/transformer.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/transformer.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/transformer.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/transformer.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/transformer.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/transformer.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/transformer.yaml Run the transformation phase. Important: For most prompts we accept the default in this tutorial. However, some prompts to watch out for are:\nKind of service/ingress created for the redis service:. Here, select ClusterIP so the service port will not be exposed via the Ingress. Exposed \u0026lsquo;web\u0026rsquo; service URL path: Since most website frontends are built to be served under / we can use that here instead of /web. Image registry URL and image registry namespace: The image registry URL is where the container images will be pushed after building Docker Hub (index.docker.io), Quay (quay.io), IBM Cloud Container Registry (us.icr.io), etc. The namespace here means the username on your target image registry and not the Kubernetes cluster namespace. Ingress host and TLS secret: If you are deploying to MiniKube, use localhost as the ingress host domain. If you are deploying to Kubernetes cluster on IBM Cloud, then you can find your ingress subdomain on your cluster on IBM Cloud as shown here. You can leave the TLS secret blank. $ move2kube transform INFO[0000] Detected a plan file at path /Users/user/Desktop/tutorial/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: ID: move2kube.transformers.types Hints: - Services that don\u0026#39;t support any of the transformer types you are interested in will be ignored. ArgoCD, Buildconfig, CloudFoundry, ClusterSelector, ComposeAnalyser, ComposeGenerator, ContainerImagesPushScriptGenerator, DockerfileDetector, DockerfileImageBuildScript, DockerfileParser, DotNetCore-Dockerfile, EarAnalyser, EarRouter, Golang-Dockerfile, Gradle, Jar, Jboss, Knative, Kubernetes, KubernetesVersionChanger, Liberty, Maven, Nodejs-Dockerfile, PHP-Dockerfile, Parameterizer, Python-Dockerfile, ReadMeGenerator, Ruby-Dockerfile, Rust-Dockerfile, Tekton, Tomcat, WarAnalyser, WarRouter, WinConsoleApp-Dockerfile, WinSLWebApp-Dockerfile, WinWebApp-Dockerfile, ZuulAnalyser ? Select all services that are needed: ID: move2kube.services.[].enable Hints: - The services unselected here will be ignored. api, redis, web INFO[0133] Iteration 1 INFO[0133] Iteration 2 - 3 artifacts to process INFO[0133] Transformer ComposeAnalyser processing 3 artifacts INFO[0133] Transformer ZuulAnalyser processing 2 artifacts INFO[0133] Transformer ZuulAnalyser Done INFO[0133] Transformer ComposeAnalyser Done INFO[0133] Created 2 pathMappings and 4 artifacts. Total Path Mappings : 2. Total Artifacts : 3. INFO[0133] Iteration 3 - 4 artifacts to process INFO[0133] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: ID: move2kube.target.clustertype Hints: - Choose the cluster type you would like to target Kubernetes INFO[0179] Transformer ClusterSelector Done INFO[0179] Transformer ArgoCD processing 2 artifacts ? What kind of service/ingress should be created for the service redis\u0026#39;s 6379 port? ID: move2kube.services.\u0026#34;redis\u0026#34;.\u0026#34;6379\u0026#34;.servicetype Hints: - Choose Ingress if you want a ingress/route resource to be created ClusterIP ? What kind of service/ingress should be created for the service api\u0026#39;s 1234 port? ID: move2kube.services.\u0026#34;api\u0026#34;.\u0026#34;1234\u0026#34;.servicetype Hints: - Choose Ingress if you want a ingress/route resource to be created Ingress ? Specify the ingress path to expose the service api\u0026#39;s 1234 port on? ID: move2kube.services.\u0026#34;api\u0026#34;.\u0026#34;1234\u0026#34;.urlpath Hints: - Leave out leading / to use first part as subdomain /api ? What kind of service/ingress should be created for the service web\u0026#39;s 8080 port? ID: move2kube.services.\u0026#34;web\u0026#34;.\u0026#34;8080\u0026#34;.servicetype Hints: - Choose Ingress if you want a ingress/route resource to be created Ingress ? Specify the ingress path to expose the service web\u0026#39;s 8080 port on? ID: move2kube.services.\u0026#34;web\u0026#34;.\u0026#34;8080\u0026#34;.urlpath Hints: - Leave out leading / to use first part as subdomain / ? Provide the minimum number of replicas each service should have ID: move2kube.minreplicas Hints: - If the value is 0 pods won\u0026#39;t be started by default 2 ? Enter the URL of the image registry : ID: move2kube.target.imageregistry.url Hints: - You can always change it later by changing the yamls. quay.io ? Enter the namespace where the new images should be pushed : ID: move2kube.target.imageregistry.namespace Hints: - Ex : myproject move2kube ? [quay.io] What type of container registry login do you want to use? ID: move2kube.target.imageregistry.logintype Hints: - Docker login from config mode, will use the default config from your local machine. No authentication INFO[1487] Transformer ArgoCD Done INFO[1487] Transformer ClusterSelector processing 2 artifacts INFO[1487] Transformer ClusterSelector Done INFO[1487] Transformer Buildconfig processing 2 artifacts INFO[1487] Transformer Buildconfig Done INFO[1487] Transformer ComposeGenerator processing 2 artifacts INFO[1487] Transformer ComposeGenerator Done INFO[1487] Transformer DockerfileImageBuildScript processing 3 artifacts ? Select the container runtime to use : ID: move2kube.containerruntime Hints: - The container runtime selected will be used in the scripts docker INFO[1492] Transformer DockerfileImageBuildScript Done INFO[1492] Transformer ClusterSelector processing 2 artifacts INFO[1492] Transformer ClusterSelector Done INFO[1492] Transformer Knative processing 2 artifacts INFO[1492] Transformer Knative Done INFO[1492] Transformer ClusterSelector processing 2 artifacts INFO[1492] Transformer ClusterSelector Done INFO[1492] Transformer Kubernetes processing 2 artifacts ? Provide the ingress host domain ID: move2kube.target.ingress.host Hints: - Ingress host domain is part of service URL localhost ? Provide the TLS secret for ingress ID: move2kube.target.ingress.tls Hints: - Leave empty to use http INFO[1499] Transformer Kubernetes Done INFO[1499] Transformer ClusterSelector processing 2 artifacts INFO[1499] Transformer ClusterSelector Done INFO[1499] Transformer Tekton processing 2 artifacts INFO[1499] Transformer Tekton Done INFO[1499] Created 33 pathMappings and 11 artifacts. Total Path Mappings : 35. Total Artifacts : 7. INFO[1499] Iteration 4 - 11 artifacts to process INFO[1499] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[1499] Transformer ContainerImagesPushScriptGenerator Done INFO[1499] Transformer Parameterizer processing 5 artifacts INFO[1499] Transformer Parameterizer Done INFO[1499] Transformer ReadMeGenerator processing 5 artifacts INFO[1500] Transformer ReadMeGenerator Done INFO[1500] Created 17 pathMappings and 1 artifacts. Total Path Mappings : 52. Total Artifacts : 18. INFO[1500] Iteration 5 - 1 artifacts to process INFO[1500] Transformer ReadMeGenerator processing 2 artifacts INFO[1500] Transformer ReadMeGenerator Done INFO[1500] Transformation done INFO[1500] Transformed target artifacts can be found at [/Users/user/Desktop/tutorial/myproject]. The tranformatin is complete.\nView the transformation output. # click to see the output $ ls docker-compose\tm2k.plan\tm2kqacache.yaml\tmyproject docker-compose.zip\tm2kconfig.yaml\tmultiple-services $ tree myproject/ myproject/  Readme.md  deploy   cicd    argocd     myproject-deploy-application.yaml    argocd-parameterized     helm-chart      myproject      Chart.yaml      templates      myproject-deploy-application.yaml     kustomize      base      kustomization.yaml      myproject-deploy-application.yaml     openshift-template     template.yaml    tekton     myproject-clone-build-push-pipeline.yaml     myproject-clone-push-serviceaccount.yaml     myproject-git-event-triggerbinding.yaml     myproject-git-repo-eventlistener.yaml     myproject-image-registry-secret.yaml     myproject-ingress.yaml     myproject-run-clone-build-push-triggertemplate.yaml     myproject-tekton-triggers-admin-role.yaml     myproject-tekton-triggers-admin-rolebinding.yaml     myproject-tekton-triggers-admin-serviceaccount.yaml    tekton-parameterized    helm-chart     myproject     Chart.yaml     templates     myproject-clone-build-push-pipeline.yaml     myproject-clone-push-serviceaccount.yaml     myproject-git-event-triggerbinding.yaml     myproject-git-repo-eventlistener.yaml     myproject-image-registry-secret.yaml     myproject-ingress.yaml     myproject-run-clone-build-push-triggertemplate.yaml     myproject-tekton-triggers-admin-role.yaml     myproject-tekton-triggers-admin-rolebinding.yaml     myproject-tekton-triggers-admin-serviceaccount.yaml    kustomize     base     kustomization.yaml     myproject-clone-build-push-pipeline.yaml     myproject-clone-push-serviceaccount.yaml     myproject-git-event-triggerbinding.yaml     myproject-git-repo-eventlistener.yaml     myproject-image-registry-secret.yaml     myproject-ingress.yaml     myproject-run-clone-build-push-triggertemplate.yaml     myproject-tekton-triggers-admin-role.yaml     myproject-tekton-triggers-admin-rolebinding.yaml     myproject-tekton-triggers-admin-serviceaccount.yaml    openshift-template    template.yaml   compose    docker-compose.yaml   knative    api-service.yaml    redis-service.yaml    web-service.yaml   knative-parameterized    helm-chart     myproject     Chart.yaml     templates     api-service.yaml     redis-service.yaml     web-service.yaml    kustomize     base     api-service.yaml     kustomization.yaml     redis-service.yaml     web-service.yaml    openshift-template    template.yaml   yamls    api-deployment.yaml    api-service.yaml    myproject-ingress.yaml    redis-deployment.yaml    redis-service.yaml    web-deployment.yaml    web-service.yaml   yamls-parameterized   helm-chart    myproject    Chart.yaml    templates     api-deployment.yaml     api-service.yaml     myproject-ingress.yaml     redis-deployment.yaml     redis-service.yaml     web-deployment.yaml     web-service.yaml    values-dev.yaml    values-prod.yaml    values-staging.yaml   kustomize    base     api-deployment.yaml     api-service.yaml     kustomization.yaml     myproject-ingress.yaml     redis-deployment.yaml     redis-service.yaml     web-deployment.yaml     web-service.yaml    overlays    dev     apps-v1-deployment-api.yaml     apps-v1-deployment-redis.yaml     apps-v1-deployment-web.yaml     kustomization.yaml    prod     apps-v1-deployment-api.yaml     apps-v1-deployment-redis.yaml     apps-v1-deployment-web.yaml     kustomization.yaml    staging    apps-v1-deployment-api.yaml    apps-v1-deployment-redis.yaml    apps-v1-deployment-web.yaml    kustomization.yaml   openshift-template   parameters-dev.yaml   parameters-prod.yaml   parameters-staging.yaml   template.yaml  scripts   builddockerimages.bat   builddockerimages.sh   pushimages.bat   pushimages.sh  source  api   Dockerfile   index.js   package-lock.json   package.json  docker-compose.yaml  web  Dockerfile  fib.php  index.php 43 directories, 107 files Inside the scripts directory we see some helpful scripts that Move2Kube has generated to help us build and push the container images we need.\nBuild all the images using the builddockerimages.sh script. # click to see the output $ cd myproject/ $ ./builddockerimages.sh [+] Building 4.3s (10/10) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 133B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/node:14 2.5s =\u0026gt; [auth] library/node:pull token for registry-1.docker.io 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 3.69kB 0.0s =\u0026gt; [1/4] FROM docker.io/library/node:14@sha256:e5c6aac226819f88d6431a56f502972d323d052b1b6108094ba7e6b07154a542 0.0s =\u0026gt; CACHED [2/4] WORKDIR /app 0.0s =\u0026gt; [3/4] COPY . . 0.0s =\u0026gt; [4/4] RUN npm install 1.5s =\u0026gt; exporting to image 0.1s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:d5a8e3d3f05592f6edefe5df286c31c2327dbde4ad3d5832fc059f1a9381157a 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/fibonacci-api:latest 0.0s Use \u0026#39;docker scan\u0026#39; to run Snyk tests against images to find vulnerabilities and learn how to fix them /Users/user/Desktop/tutorial/myproject [+] Building 2.5s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 82B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/php:7-apache 2.2s =\u0026gt; [auth] library/php:pull token for registry-1.docker.io 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 1.51kB 0.0s =\u0026gt; CACHED [1/2] FROM docker.io/library/php:7-apache@sha256:729ad01c7d8e10fd992a6d4f3eb05dce3fb69bdf5c4fb4a9de4be4f4f5ae4dcc 0.0s =\u0026gt; [2/2] COPY . /var/www/html/ 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:f5d91c6d96de3f8bb4c2c5d8bf6cde84985b7ee29d00ad21fad07e05cbe5ddca 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/fibonacci-web:latest 0.0s Use \u0026#39;docker scan\u0026#39; to run Snyk tests against images to find vulnerabilities and learn how to fix them /Users/user/Desktop/tutorial/myproject done Push the images to the registry and namespace selected using the pushimages.sh script. # click to see the output $ ./pushimages.sh The push refers to repository [quay.io/move2kube/fibonacci-web] 29db8d44d6a6: Pushed 10dfb82106c4: Layer already exists 7446d340e7f8: Layer already exists 55d40777afe6: Layer already exists 56543a169be6: Layer already exists b299cffd87cb: Layer already exists 23946094ff3f: Layer already exists 6c39776a30a0: Layer already exists 564928686313: Layer already exists 6e4300c6b758: Layer already exists ee0ca96d307e: Layer already exists 0fdfbbf7aebd: Layer already exists 2a3138346faa: Layer already exists 2edcec3590a4: Layer already exists latest: digest: sha256:b34a669c75afda3dd4b8d5ef264a6f818cb394bb147d754d6e1a8699798a4c70 size: 3242 The push refers to repository [quay.io/move2kube/fibonacci-api] aef80d5c2943: Pushed 4471bdef8049: Pushed 5825d126ab35: Layer already exists d48d998e8307: Layer already exists 1f95b68fc83b: Layer already exists c1a45f6975fa: Layer already exists be099ea57c79: Layer already exists 2b2dfe091b20: Layer already exists df74cf750cc8: Layer already exists 75a95a2ddc29: Layer already exists e8fb9c1faa8f: Layer already exists 9d1a9278f26b: Layer already exists latest: digest: sha256:521be8d409c29414274c912600dc7606b7db591f69abb2fbfb5e402ccb547878 size: 2840 Note: If you are using Quay.io, change the pushed repositories visibility to Public or the Kubernetes pods may fail to pull the images from the registry and could fail to start due to ErrImagePullBack.\nIf you have already have a Kubernetes cluster, log in to your Kubernetes cluster. Or, start MiniKube to start a local Kubernetes cluster. $ minikube start  minikube v1.24.0 on Darwin 12.0.1  Using the docker driver based on existing profile  Starting control plane node minikube in cluster minikube  Pulling base image ...  Updating the running docker \u0026#34;minikube\u0026#34; container ...  Preparing Kubernetes v1.22.3 on Docker 20.10.8 ...  Verifying Kubernetes components...  Using image gcr.io/k8s-minikube/storage-provisioner:v5  After the addon is enabled, please run \u0026#34;minikube tunnel\u0026#34; and your ingress resources would be available at \u0026#34;127.0.0.1\u0026#34;  Using image k8s.gcr.io/ingress-nginx/controller:v1.0.4  Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1  Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1  Verifying ingress addon...  Enabled addons: storage-provisioner, default-storageclass, ingress  Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; cluster and \u0026#34;default\u0026#34; namespace by default Also enable the ingress addon\n$ minikube addons enable ingress  After the addon is enabled, please run \u0026#34;minikube tunnel\u0026#34; and your ingress resources would be available at \u0026#34;127.0.0.1\u0026#34;  Using image k8s.gcr.io/ingress-nginx/controller:v1.0.4  Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1  Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1  Verifying ingress addon...  The \u0026#39;ingress\u0026#39; addon is enabled Check if you are able to run the kubectl related command. $ kubectl get pods Deploy the Kubernetes YAMLs to our Kubernetes/MiniKube cluster. $ kubectl apply -f deploy/yamls deployment.apps/api created service/api created ingress.networking.k8s.io/myproject created deployment.apps/redis created service/redis created deployment.apps/web created service/web created View all the Kubernetes resources that were created. $ kubectl get all NAME READY STATUS RESTARTS AGE pod/api-84fc6cf59f-6z4nl 1/1 Running 0 8h pod/api-84fc6cf59f-72lmx 1/1 Running 0 8h pod/redis-5c94584bb-c9zk5 1/1 Running 0 8h pod/redis-5c94584bb-sv2zx 1/1 Running 0 8h pod/web-999d4cc74-6ckbj 1/1 Running 0 8h pod/web-999d4cc74-97hnc 1/1 Running 0 8h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/api ClusterIP 10.103.24.55 \u0026lt;none\u0026gt; 1234/TCP 8h service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 12h service/redis ClusterIP None \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 8h service/web ClusterIP 10.100.18.139 \u0026lt;none\u0026gt; 8080/TCP 8h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/api 2/2 2 2 8h deployment.apps/redis 2/2 2 2 8h deployment.apps/web 2/2 2 2 8h NAME DESIRED CURRENT READY AGE replicaset.apps/api-84fc6cf59f 2 2 2 8h replicaset.apps/redis-5c94584bb 2 2 2 8h replicaset.apps/web-999d4cc74 2 2 2 8h Important: This step is required only if the app has been deployed on MiniKube cluster.\nAccess the running application using the Ingress we created by starting a tunnel to the MiniKube cluster. $ minikube tunnel  The service/ingress myproject requires privileged ports to be exposed: [80 443]  sudo permission will be asked for it.  Starting tunnel for service myproject. Password: Access the app on the ingress specified during the ingress host domain QA. (For MiniKube, it will be http://localhost). Conclusion In this tutorial we transformed a Docker Compose application with multiple services. We used Move2Kube to come up with a plan for migration, transform the input using the plan, generate the appropriate build scripts, Kubernetes YAMLs, etc. and deployed them to MiniKube.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/migratedeploycfapps/","title":"Migrate and deploy Cloud Foundry applications to Kubernetes","tags":[],"description":"","content":"This document steps through installing Move2Kube and using Move2Kube\u0026rsquo;s the step process (collect, plan and transform) to create deployment artifacts for Cloud Foundry apps using the data from samples/cloud-foundry.\nSummary $ move2kube transform -s cloud-foundry Move2Kube will automatically analyze all the artifacts in the cloud-foundry directory and transform and create all the artifacts required for deploying the application in Kubernetes.\nPrerequisites A source directory which contains the source code files and/or the manifest.yml file of a Cloud Foundry application.\nA sample of this is present in the move2kube-demos repository. In this tutorial, we will be using the cloud-foundry sample from this repository.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/cloud-foundry -r move2kube-demos View the structure inside the ./cloud-foundry directory. The cloud-foundry directory contains the source code files and the manifest.yml file.\n$ tree cloud-foundry cloud-foundry/  cfnodejsapp   main.js   manifest.yml   package-lock.json   package.json  m2k_collect  cf  cfapps.yaml Install Move2Kube.\nInstall a container runtime: Docker or Podman.\nInstall Kubectl.\nTo verify that dependencies were correctly installed you can run the below commands.\n$ move2kube version $ docker version or\n$ podman info $ kubectl version Install the Cloud Foundry CLI.\nTo demonstrate how to use Move2Kube to migrate a Cloud Foundry (CF) application to Kubernetes, we will be using the source code inside the cloud-foundry/cfnodejsapp directory. If you want to try out Move2Kube on your CF application, then in place of our sample cloud-foundry directory, provide the correct path of the source directory (containing the source code and/or manifest files) of your CF application to Move2Kube during the plan phase.\nOptional: We will deploy a simple NodeJS application into CF. If you have a running CF app already you may use that instead. Provision a CF app with the name cfnodejsapp using your cloud provider (Ex: IBM Cloud).\nMake note of the API endpoint (API endpoints for the IBM Cloud Foundry service can be found here).\nLogin to CF.\n$ cf login -a \u0026lt;YOUR CF API endpoint\u0026gt; Deploy the sample application to Cloud Foundry. The source code of the sample application is present inside ./cloud-foundry/cfnodejsapp folder.\n$ cf push -f ./cloud-foundry/cfnodejsapp -p ./cloud-foundry/cfnodejsapp Visit the URL of the application (you can get this by running cf apps) to see it running.\nGenerating target artifacts Now that we have a running CF app we can transform it using Move2Kube\u0026rsquo;s three stage process: collect, plan and transform. Run these steps from the directory where cloud-foundry directory is present:\nOptional: This step is required only if you are interested in metadata such as environment variables from a running instance. If you don\u0026rsquo;t have a running app, you can use the m2k_collect directoy that comes with the sample. Collect some data about your running CF application. Limit the collection to only Cloud Foundry information using the -a cf annotation flag.\n```console $ cf login -a \u0026lt;YOUR CF API endpoint\u0026gt; $ move2kube collect -a cf INFO[0000] Begin collection INFO[0000] [*collector.CfAppsCollector] Begin collection INFO[0013] [*collector.CfAppsCollector] Done INFO[0013] [*collector.CfServicesCollector] Begin collection INFO[0027] [*collector.CfServicesCollector] Done INFO[0027] Collection done INFO[0027] Collect Output in [/Users/username/m2k_collect]. Copy this directory into the source directory to be used for planning. ``` The data we collected will be stored in a new directory called ./m2k_collect.\n```console $ ls m2k_collect cf ``` The ./m2k_collect/cf directory contains the yaml file which has the runtime information of the particular application being transformed including the buildpacks that are supported, the memory, the number of instances and the ports that are supported. If there are environment variables, it collects that information too.\n* Move the `./m2k_collect/cf` directory into the source directory `./cloud-foundry`. ```console $ mv m2k_collect cloud-foundry/ ``` Create a plan on how to transform your app to run on Kubernetes. In the plan phase, it is going to combine the runtime metadata (if present) with source artifacts and come up with a plan. Provide the path to the source directory (containing the source code and/or manifest files of CF application) using the -s flag.\n```console $ move2kube plan -s cloud-foundry INFO[0000] Loading Configuration INFO[0000] [*configuration.ClusterMDLoader] Loading configuration INFO[0000] [*configuration.ClusterMDLoader] Done INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [ZuulAnalyser] Planning transformation INFO[0000] [ZuulAnalyser] Done INFO[0000] [CloudFoundry] Planning transformation INFO[0000] Identified 1 namedservices and 0 unnamed transformer plans INFO[0000] [CloudFoundry] Done INFO[0000] [Base Directory] Identified 1 namedservices and 0 unnamed transformer plans INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 namedservices and 0 unnamed transformer plans in . INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 1 namedservices and 0 unnamed transformer plans INFO[0000] [Named Services] Identified 1 namedservices INFO[0000] No of services identified : 1 INFO[0000] Plan can be found at [/Users/username/m2k.plan]. ``` Move2Kube has created a m2k.plan which is essentially a YAML file. View what is inside the plan file.\n```console $ cat m2k.plan ``` ```yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: cloud-foundry services: cfnodejsapp: - transformerName: CloudFoundry paths: CfManifest: - cfnodejsapp/manifest.yml CfRunningManifest: - m2k_collect/cf/cfapps.yaml ServiceDirPath: - cfnodejsapp configs: CloudFoundryService: serviceName: cfnodejsapp ContainerizationOptions: - Nodejs-Dockerfile - transformerName: Nodejs-Dockerfile paths: ServiceDirPath: - cfnodejsapp transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/buildconfig.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/cloudfoundry.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/clusterselector.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/composeanalyser.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/composegenerator.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimage/containerimagespushscript/containerimagespushscript.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/dockerfiledetector.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/dockerfilebuildscriptgenerator.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/dockerfileparser.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/dotnetcore.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/ear.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/earrouter.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/golang.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/gradle.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/jar.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/jboss.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/knative.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/kubernetes.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/kubernetesversionchanger.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/liberty.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/maven.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/nodejs.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/php.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/parameterizer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/python.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/readmegenerator.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/ruby.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/rust.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/tekton.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/tomcat.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/war.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/warrouter.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/winconsole.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/winsilverlightweb.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/winweb.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/zuulanalyser.yaml ``` \u0026lt;/details\u0026gt; * Notice that Move2Kube has detected the `cfnodejsapp` service, which is the name of the sample CF application from its manifest.yml. * The plan file indicates the application can be transformed using the `CloudFoundry` or `Nodejs-Dockerfile` transformers. * Move2Kube can use the source artifacts `manifest.yaml` and also the runtime information from `cfapps.yaml` and combine all of them to do the transformation. Run move2kube transform on this plan. $ move2kube transform INFO[0000] Detected a plan file at path /Users/username/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: Hints: [Services that don\u0026#39;t support any of the transformer types you are interested in will be ignored.] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] \u0026gt; [] Jboss [] Kubernetes [] Liberty [] ZuulAnalyser [] CloudFoundry [] DockerfileParser [] Golang-Dockerfile [] Gradle [] Python-Dockerfile [] ReadMeGenerator [] ContainerImagesPushScriptGenerator [] DotNetCore-Dockerfile [] Parameterizer [] Tekton [] WinSLWebApp-Dockerfile [] Buildconfig [] ClusterSelector [] ComposeAnalyser [] PHP-Dockerfile [] Ruby-Dockerfile [] WinConsoleApp-Dockerfile [] DockerfileImageBuildScript [] EarRouter [] Rust-Dockerfile [] Tomcat [] WarAnalyser [] EarAnalyser [] Knative [] Maven [] WinWebApp-Dockerfile [] ComposeGenerator [] Jar [] DockerfileDetector [] KubernetesVersionChanger [] Nodejs-Dockerfile [] WarRouter Accept the default by pressing the return or enter key. Jboss, Kubernetes, Liberty, ZuulAnalyser, CloudFoundry, DockerfileParser, Golang-Dockerfile, Gradle, Python-Dockerfile, ReadMeGenerator, ContainerImagesPushScriptGenerator, DotNetCore-Dockerfile, Parameterizer, Tekton, WinSLWebApp-Dockerfile, Buildconfig, ClusterSelector, ComposeAnalyser, PHP-Dockerfile, Ruby-Dockerfile, WinConsoleApp-Dockerfile, DockerfileImageBuildScript, EarRouter, Rust-Dockerfile, Tomcat, WarAnalyser, EarAnalyser, Knative, Maven, WinWebApp-Dockerfile, ComposeGenerator, Jar, DockerfileDetector, KubernetesVersionChanger, Nodejs-Dockerfile, WarRouter ? Select all services that are needed: Hints: [The services unselected here will be ignored.] [Use arrows to move, space to select, \u0026lt;right\u0026gt; to all, \u0026lt;left\u0026gt; to none, type to filter] \u0026gt; [] cfnodejsapp Select the cfnodejsapp service. cfnodejsapp INFO[0275] Starting Plan Transformation INFO[0275] Iteration 1 INFO[0275] Iteration 2 - 1 artifacts to process INFO[0275] Transformer CloudFoundry processing 1 artifacts INFO[0275] Transformer ZuulAnalyser processing 2 artifacts INFO[0275] Transformer ZuulAnalyser Done INFO[0275] Transformer CloudFoundry Done INFO[0275] Created 0 pathMappings and 3 artifacts. Total Path Mappings : 0. Total Artifacts : 1. INFO[0275] Iteration 3 - 3 artifacts to process INFO[0275] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: Hints: [Choose the cluster type you would like to target] [Use arrows to move, type to filter] \u0026gt; Kubernetes Openshift AWS-EKS Azure-AKS GCP-GKE IBM-IKS IBM-Openshift Now, it asks to select the cluster type you want to deploy to. Here, we select the Kubernetes cluster type.\n```console Kubernetes INFO[0351] Transformer ClusterSelector Done INFO[0351] Transformer Kubernetes processing 2 artifacts ? What URL/path should we expose the service cfnodejsapp's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/cfnodejsapp) ``` Accpet the default by pressing the return or enter key. /cfnodejsapp ? Provide the minimum number of replicas each service should have Hints: [If the value is 0 pods won\u0026#39;t be started by default] (2) Let\u0026rsquo;s go ahead with the default answer again, which means 2 replicas for each service.\n```console 2 ? Enter the URL of the image registry : Hints: [You can always change it later by changing the yamls.] [Use arrows to move, type to filter] Other (specify custom option) index.docker.io \u0026gt; quay.io us.icr.io ``` Select quay.io as the image registry host. Select \u0026lsquo;Other\u0026rsquo; if your registry name is not here. quay.io ? Enter the namespace where the new images should be pushed : Hints: [Ex : myproject] (myproject) m2k-tutorial Input the namespace under which you want to deploy- m2k-tutorial. (Say, you have namespace m2k-tutorial in quay.io) m2k-tutorial ? [quay.io] What type of container registry login do you want to use? Hints: [Docker login from config mode, will use the default config from your local machine.] [Use arrows to move, type to filter] Use existing pull secret \u0026gt; No authentication UserName/Password Select the container registry login type. No authentication INFO[0841] Optimization done INFO[0841] Begin Optimization INFO[0841] Optimization done INFO[0841] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 2. Total Artifacts : 3. INFO[0841] Transformer Knative Done INFO[0841] Transformer Tekton processing 2 artifacts INFO[0841] Begin Optimization INFO[0841] Optimization done INFO[0841] Generating Tekton pipeline for CI/CD INFO[0841] No remote git repos detected. You might want to configure the git repository links manually. ? Provide the ingress host domain Hints: [Ingress host domain is part of service URL] (myproject.com) my-cluster-ingress-host-domain.com Grab the ingress hosting domain from the cluster you are deploying to. The ingress hosting domain will differ based on the cluster you are fetching from. ? Provide the TLS secret for ingress Hints: [Leave empty to use http] Accept the by-default TLS secret by pressing the \u0026lsquo;return\u0026rsquo; key. INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Generating Tekton pipeline for CI/CD INFO[1094] No remote git repos detected. You might want to configure the git repository links manually. INFO[1094] Created 20 pathMappings and 2 artifacts. Total Path Mappings : 22. Total Artifacts : 3. INFO[1094] Transformer Tekton Done INFO[1094] Transformer Buildconfig processing 2 artifacts INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Created 0 pathMappings and 0 artifacts. Total Path Mappings : 22. Total Artifacts : 3. INFO[1094] Transformer Buildconfig Done INFO[1094] Transformer ComposeGenerator processing 2 artifacts INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Created 2 pathMappings and 0 artifacts. Total Path Mappings : 24. Total Artifacts : 3. INFO[1094] Transformer ComposeGenerator Done INFO[1094] Transformer Kubernetes processing 2 artifacts INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Total transformed objects : 3 INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Total transformed objects : 3 INFO[1094] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 26. Total Artifacts : 3. INFO[1094] Transformer Kubernetes Done INFO[1094] Transformer Nodejs-Dockerfile processing 1 artifacts ? Select port to be exposed for the service cfnodejsapp : Hints: [Select Other if you want to expose the service cfnodejsapp to some other port] [Use arrows to move, type to filter] \u0026gt; 8080 Other (specify custom option) Select the port to expose the cfnodejsapp service. 8080 INFO[1184] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 28. Total Artifacts : 3. INFO[1184] Transformer Nodejs-Dockerfile Done INFO[1184] Iteration 4 INFO[1184] Transformer ReadMeGenerator processing 4 artifacts INFO[1184] Created 1 pathMappings and 0 artifacts. Total Path Mappings : 29. Total Artifacts : 11. INFO[1184] Transformer ReadMeGenerator Done INFO[1184] Transformer DockerfileImageBuildScript processing 2 artifacts ? Select the container runtime to use : Hints: [The container runtime selected will be used in the scripts] [Use arrows to move, type to filter] \u0026gt; docker podman Select the container runtime to use. docker INFO[1184] Created 1 pathMappings and 2 artifacts. Total Path Mappings : 30. Total Artifacts : 11. INFO[1184] Transformer DockerfileImageBuildScript Done INFO[1184] Transformer DockerfileParser processing 1 artifacts INFO[1184] Created 0 pathMappings and 1 artifacts. Total Path Mappings : 30. Total Artifacts : 11. INFO[1184] Transformer DockerfileParser Done INFO[1184] Transformer Parameterizer processing 4 artifacts INFO[1184] Created 12 pathMappings and 0 artifacts. Total Path Mappings : 42. Total Artifacts : 11. INFO[1184] Transformer Parameterizer Done INFO[1184] Iteration 5 INFO[1184] Transformer Tekton processing 2 artifacts INFO[1184] Begin Optimization INFO[1184] Optimization done INFO[1184] Generating Tekton pipeline for CI/CD INFO[1184] No remote git repos detected. You might want to configure the git repository links manually. INFO[1184] Begin Optimization INFO[1184] Optimization done INFO[1184] Generating Tekton pipeline for CI/CD INFO[1184] No remote git repos detected. You might want to configure the git repository links manually. INFO[1184] Created 20 pathMappings and 2 artifacts. Total Path Mappings : 62. Total Artifacts : 14. INFO[1184] Transformer Tekton Done INFO[1184] Transformer Buildconfig processing 2 artifacts INFO[1184] Begin Optimization INFO[1185] Optimization done INFO[1185] Created 0 pathMappings and 0 artifacts. Total Path Mappings : 62. Total Artifacts : 14. INFO[1185] Transformer Buildconfig Done INFO[1185] Transformer ComposeGenerator processing 2 artifacts INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Created 2 pathMappings and 0 artifacts. Total Path Mappings : 64. Total Artifacts : 14. INFO[1185] Transformer ComposeGenerator Done INFO[1185] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[1185] Created 1 pathMappings and 1 artifacts. Total Path Mappings : 65. Total Artifacts : 14. INFO[1185] Transformer ContainerImagesPushScriptGenerator Done INFO[1185] Transformer Kubernetes processing 2 artifacts INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Total transformed objects : 3 INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Total transformed objects : 3 INFO[1185] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 67. Total Artifacts : 14. INFO[1185] Transformer Kubernetes Done INFO[1185] Transformer ContainerImagesBuildScriptGenerator processing 2 artifacts INFO[1185] Created 2 pathMappings and 1 artifacts. Total Path Mappings : 69. Total Artifacts : 14. INFO[1185] Transformer ContainerImagesBuildScriptGenerator Done INFO[1185] Transformer Knative processing 2 artifacts INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 71. Total Artifacts : 14. INFO[1185] Transformer Knative Done INFO[1185] Iteration 6 INFO[1185] Transformer Parameterizer processing 4 artifacts INFO[1185] Created 12 pathMappings and 0 artifacts. Total Path Mappings : 83. Total Artifacts : 22. INFO[1185] Transformer Parameterizer Done INFO[1185] Transformer ReadMeGenerator processing 6 artifacts INFO[1185] Created 1 pathMappings and 0 artifacts. Total Path Mappings : 84. Total Artifacts : 22. INFO[1185] Transformer ReadMeGenerator Done INFO[1185] Plan Transformation done INFO[1185] Transformed target artifacts can be found at [/Users/username/github/move2kube-demos/samples/myproject]. The transformation is successful and the target artifacts can be found inside the ./myproject directory. The structure of the ./myproject directory can be seen by running:\n$ tree myproject myproject/  Readme.md  deploy   cicd    tekton     cfnodejsapp-vcapasenv-secret.yaml     myproject-clone-build-push-pipeline.yaml     myproject-clone-push-serviceaccount.yaml     myproject-git-event-triggerbinding.yaml     myproject-git-repo-eventlistener.yaml     myproject-image-registry-secret.yaml     myproject-ingress.yaml     myproject-run-clone-build-push-triggertemplate.yaml     myproject-tekton-triggers-admin-role.yaml     myproject-tekton-triggers-admin-rolebinding.yaml     myproject-tekton-triggers-admin-serviceaccount.yaml    tekton-parameterized    helm-chart     myproject     Chart.yaml     templates     cfnodejsapp-vcapasenv-secret.yaml     myproject-clone-build-push-pipeline.yaml     myproject-clone-push-serviceaccount.yaml     myproject-git-event-triggerbinding.yaml     myproject-git-repo-eventlistener.yaml     myproject-image-registry-secret.yaml     myproject-ingress.yaml     myproject-run-clone-build-push-triggertemplate.yaml     myproject-tekton-triggers-admin-role.yaml     myproject-tekton-triggers-admin-rolebinding.yaml     myproject-tekton-triggers-admin-serviceaccount.yaml    kustomize     base     cfnodejsapp-vcapasenv-secret.yaml     kustomization.yaml     myproject-clone-build-push-pipeline.yaml     myproject-clone-push-serviceaccount.yaml     myproject-git-event-triggerbinding.yaml     myproject-git-repo-eventlistener.yaml     myproject-image-registry-secret.yaml     myproject-ingress.yaml     myproject-run-clone-build-push-triggertemplate.yaml     myproject-tekton-triggers-admin-role.yaml     myproject-tekton-triggers-admin-rolebinding.yaml     myproject-tekton-triggers-admin-serviceaccount.yaml    openshift-template    template.yaml   compose    docker-compose.yaml   knative    cfnodejsapp-service.yaml   knative-parameterized    helm-chart     myproject     Chart.yaml     templates     cfnodejsapp-service.yaml    kustomize     base     cfnodejsapp-service.yaml     kustomization.yaml    openshift-template    template.yaml   yamls    cfnodejsapp-deployment.yaml    cfnodejsapp-service.yaml    cfnodejsapp-vcapasenv-secret.yaml    myproject-ingress.yaml   yamls-parameterized   helm-chart    myproject    Chart.yaml    templates     cfnodejsapp-deployment.yaml     cfnodejsapp-service.yaml     cfnodejsapp-vcapasenv-secret.yaml     myproject-ingress.yaml    values-dev.yaml    values-prod.yaml    values-staging.yaml   kustomize    base     cfnodejsapp-deployment.yaml     cfnodejsapp-service.yaml     cfnodejsapp-vcapasenv-secret.yaml     kustomization.yaml     myproject-ingress.yaml    overlays    dev     apps-v1-deployment-cfnodejsapp.yaml     kustomization.yaml    prod     apps-v1-deployment-cfnodejsapp.yaml     kustomization.yaml    staging    apps-v1-deployment-cfnodejsapp.yaml    kustomization.yaml   openshift-template   parameters-dev.yaml   parameters-prod.yaml   parameters-staging.yaml   template.yaml  scripts   builddockerimages.bat   builddockerimages.sh   pushimages.bat   pushimages.sh  source  cfnodejsapp   Dockerfile   main.js   manifest.yml   package-lock.json   package.json  m2k_collect  cf  cfapps.yaml Move2Kube has created all the deployment artifacts present inside the ./myproject directory.\nDeploying the application to Kubernetes with the generated target artifacts Open the ./myproject directory. $ cd myproject/ $ ls Readme.md deploy scripts source Run the builddockerimages.sh script inside the ./myproject/scripts directory. Note: This step may take some time to complete.\n$ cd scripts $ ./builddockerimages.sh [+] Building 7.1s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.1s =\u0026gt; =\u0026gt; transferring dockerfile: 37B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for registry.access.redha 2.7s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 354B 0.0s =\u0026gt; CACHED [1/3] FROM registry.access.redhat.com/ubi8/ 0.0s =\u0026gt; [2/3] COPY . . 0.1s =\u0026gt; [3/3] RUN npm install 4.0s =\u0026gt; exporting to image 0.1s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:7bd59bff8763073644bd68cd3f 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/cfnodejsapp 0.0s /Users/username/github/move2kube-demos/samples/myproject done Run the pushimages.sh script to push the applications images to the registry specified during the transform phase. For this step, you are required to log in to your Docker registry. To log in to quay.io run docker login quay.io.\nTo log in to IBM Cloud us.icr.io registry refer here.\n$ ./pushimages.sh Using default tag: latest The push refers to repository [quay.io/m2k-tutorial/cfnodejsapp] d98785f949ff: Layer already exists 2234a0b938d7: Layer already exists 967d006c4be4: Layer already exists 90c628b74ee4: Layer already exists e7f8a4365a01: Layer already exists a7be0896acef: Layer already exists 73eb3b4bebc5: Layer already exists latest: digest: sha256:75283b09042b1454567b5e99d6d99374daad07fe46ee6843ace7dca29f085fd7 size: 1789 Note: If you have pushed the image repository to quay.io, open the Repository\u0026rsquo;s Visibility in quay.io cfnodejsapp repository\u0026rsquo;s Settings, select whether you want the repository cfnodejsapp to be public or private so that it can be properly accessed by the Kubernetes cluster.\nDeploy the application with kubectl apply command using the YAML files Move2Kube created inside the ./myproject/deploy/yamls directory. $ cd .. $ kubectl apply -f deploy/yamls deployment.apps/cfnodejsapp created service/cfnodejsapp created ingress.networking.k8s.io/myproject created The application is now accessible on the cluster.\nCheck the pods status by running: $ kubectl get pods NAME READY STATUS RESTARTS AGE cfnodejsapp-58d777bd44-8ct2m 1/1 Running 0 7s cfnodejsapp-58d777bd44-hq6lf 1/1 Running 0 7s View the ingress to see the URL where the app has been deployed to. $ kubectl get ingress myproject Conclusion That is a simple way to combine multiple types of information like runtime, source, and cluster, and do a holistic transformation of your Cloud Foundry app to Kubernetes.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/runnoninteractively/","title":"Run transforms non-interactively","tags":[],"description":"","content":"Move2Kube interacts with users through a series of questions during the transformation phase. After looking at the output, you may want to rerun it and give different answers to some of the questions. In order to avoid answering all of the same questions over and over, Move2Kube provides a simple configuration file.\nIn the directory where we ran the move2kube transform command, we see a file called m2kconfig.yaml which contains the answers provided to all of the questions that were asked. There is also a m2kqacache.yaml file which contains both the questions and the answers in more detail and can be used when we run the transform using the --config flag\nSummary command $ move2kube transform --config path/to/m2kconfig.yaml Running a tranform non-interactively Download the language platforms sample. Each directory contains a simple web application written in different languages. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/language-platforms -r move2kube-demos $ ls language-platforms django\tgolang\tjava-gradle\tjava-gradle-war\tjava-maven\tjava-maven-war\tnodejs\tphp\tpython\truby\trust Important: If you already have a m2kconfig.yaml from a previous run, skip the next step.\nRun the plan and transform on the language-platforms source, answer all the questions as appropriate. $ ls language-platforms $ move2kube plan -s language-platforms INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory ... INFO[0000] Plan can be found at [/Users/user/Desktop/tutorial/m2k.plan]. $ move2kube transform INFO[0000] Detected a plan file at path /Users/user/Desktop/tutorial/m2k.plan. Will transform using this plan. ... INFO[0095] Plan Transformation done INFO[0095] Transformed target artifacts can be found at [/Users/user/Desktop/tutorial/myproject]. View the m2kconfig.yaml file in the output. $ ls m2k.plan\tm2kconfig.yaml\tm2kqacache.yaml\tmyproject\tlanguage-platforms $ cat m2kconfig.yaml Your `m2kconfig.yaml` might look different depending on what questions were asked and what answers you gave. move2kube: containerruntime: docker minreplicas: \u0026#34;2\u0026#34; services: golang: \u0026#34;8080\u0026#34;: urlpath: /golang enable: true ports: - \u0026#34;8080\u0026#34; java-gradle: \u0026#34;9080\u0026#34;: urlpath: /java-gradle enable: true wartransformer: Liberty java-maven: \u0026#34;9080\u0026#34;: urlpath: /java-maven enable: true wartransformer: Liberty myproject-django: \u0026#34;8080\u0026#34;: urlpath: /myproject-django enable: true port: \u0026#34;8080\u0026#34; myproject-java-war: \u0026#34;9080\u0026#34;: urlpath: /myproject-java-war enable: true wartransformer: Liberty myproject-php: \u0026#34;8082\u0026#34;: urlpath: /myproject-php enable: true myproject-python: \u0026#34;8080\u0026#34;: urlpath: /myproject-python enable: true port: \u0026#34;8080\u0026#34; nodejs: \u0026#34;8080\u0026#34;: urlpath: /nodejs enable: true port: \u0026#34;8080\u0026#34; ruby: \u0026#34;8080\u0026#34;: urlpath: /ruby enable: true port: \u0026#34;8080\u0026#34; rust: \u0026#34;8085\u0026#34;: urlpath: /rust enable: true port: \u0026#34;8085\u0026#34; target: clustertype: Kubernetes imageregistry: logintype: No authentication namespace: myproject url: quay.io ingress: host: myproject.com tls: \u0026#34;\u0026#34; transformers: types: - EarRouter - WinConsoleApp-Dockerfile - Gradle - Jar - WinSLWebApp-Dockerfile - DockerfileImageBuildScript - DotNetCore-Dockerfile - EarAnalyser - Golang-Dockerfile - Ruby-Dockerfile - Tomcat - Buildconfig - ComposeGenerator - ContainerImagesPushScriptGenerator - DockerfileParser - WarAnalyser - WinWebApp-Dockerfile - WarRouter - ZuulAnalyser - KubernetesVersionChanger - Tekton - Maven - PHP-Dockerfile - Parameterizer - Python-Dockerfile - ClusterSelector - DockerfileDetector - Kubernetes - Liberty - Rust-Dockerfile - Nodejs-Dockerfile - ReadMeGenerator - CloudFoundry - ComposeAnalyser - Jboss - Knative transformerselector: \u0026#34;\u0026#34; The config file only contains the answers we provided to the questions. We can better understand the config file by looking at its companion file m2kqacache.yaml\n$ cat m2kqacache.yaml ``` ```yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: QACache spec: solutions: - id: move2kube.transformers.types type: MultiSelect description: \u0026#39;Select all transformer types that you are interested in:\u0026#39; hints: - Services that don\u0026#39;t support any of the transformer types you are interested in will be ignored. options: - Golang-Dockerfile - Nodejs-Dockerfile - ContainerImagesPushScriptGenerator - ReadMeGenerator - ComposeAnalyser - EarAnalyser - Ruby-Dockerfile - WarAnalyser - DotNetCore-Dockerfile - Python-Dockerfile - Liberty - Knative - CloudFoundry - Tomcat - ZuulAnalyser - Tekton - Rust-Dockerfile - KubernetesVersionChanger - DockerfileParser - Parameterizer - PHP-Dockerfile - Kubernetes - WinWebApp-Dockerfile - Maven - Jboss - ComposeGenerator - Gradle - WinConsoleApp-Dockerfile - Buildconfig - DockerfileDetector - WarRouter - EarRouter - ClusterSelector - Jar - DockerfileImageBuildScript - WinSLWebApp-Dockerfile default: - Golang-Dockerfile - Nodejs-Dockerfile - ContainerImagesPushScriptGenerator - ReadMeGenerator - ComposeAnalyser - EarAnalyser - Ruby-Dockerfile - WarAnalyser - DotNetCore-Dockerfile - Python-Dockerfile - Liberty - Knative - CloudFoundry - Tomcat - ZuulAnalyser - Tekton - Rust-Dockerfile - KubernetesVersionChanger - DockerfileParser - Parameterizer - PHP-Dockerfile - Kubernetes - WinWebApp-Dockerfile - Maven - Jboss - ComposeGenerator - Gradle - WinConsoleApp-Dockerfile - Buildconfig - DockerfileDetector - WarRouter - EarRouter - ClusterSelector - Jar - DockerfileImageBuildScript - WinSLWebApp-Dockerfile answer: - EarRouter - WinConsoleApp-Dockerfile - Gradle - Jar - WinSLWebApp-Dockerfile - DockerfileImageBuildScript - DotNetCore-Dockerfile - EarAnalyser - Golang-Dockerfile - Ruby-Dockerfile - Tomcat - Buildconfig - ComposeGenerator - ContainerImagesPushScriptGenerator - DockerfileParser - WarAnalyser - WinWebApp-Dockerfile - WarRouter - ZuulAnalyser - KubernetesVersionChanger - Tekton - Maven - PHP-Dockerfile - Parameterizer - Python-Dockerfile - ClusterSelector - DockerfileDetector - Kubernetes - Liberty - Rust-Dockerfile - Nodejs-Dockerfile - ReadMeGenerator - CloudFoundry - ComposeAnalyser - Jboss - Knative - id: move2kube.transformerselector type: Input hints: - Set the transformer selector config. default: \u0026#34;\u0026#34; answer: \u0026#34;\u0026#34; - id: move2kube.services.[].enable type: MultiSelect description: \u0026#39;Select all services that are needed:\u0026#39; hints: - The services unselected here will be ignored. options: - java-maven - myproject-django - myproject-python - rust - golang - java-gradle - myproject-java-war - myproject-php - nodejs - ruby default: - java-maven - myproject-django - myproject-python - rust - golang - java-gradle - myproject-java-war - myproject-php - nodejs - ruby answer: - java-maven - myproject-django - myproject-python - rust - golang - java-gradle - myproject-java-war - myproject-php - nodejs - ruby - id: move2kube.services.java-gradle.wartransformer type: Select description: Select the transformer to use for service java-gradle options: - Liberty - Tomcat - Jboss default: Liberty answer: Liberty - id: move2kube.services.ruby.port type: Select description: \u0026#39;Select port to be exposed for the service ruby :\u0026#39; hints: - Select Other if you want to expose the service ruby to some other port options: - \u0026#34;8080\u0026#34; - Other (specify custom option) default: \u0026#34;8080\u0026#34; answer: \u0026#34;8080\u0026#34; - id: move2kube.services.nodejs.port type: Input description: \u0026#39;Enter the port to be exposed for the service nodejs: \u0026#39; hints: - The service nodejs will be exposed to the specified port default: \u0026#34;8080\u0026#34; answer: \u0026#34;8080\u0026#34; - id: move2kube.services.myproject-python.port type: Select description: \u0026#39;Select port to be exposed for the service myproject-python :\u0026#39; hints: - Select Other if you want to expose the service myproject-python to some other port options: - \u0026#34;8080\u0026#34; - Other (specify custom option) default: \u0026#34;8080\u0026#34; answer: \u0026#34;8080\u0026#34; - id: move2kube.services.myproject-django.port type: Select description: \u0026#39;Select port to be exposed for the service myproject-django :\u0026#39; hints: - Select Other if you want to expose the service myproject-django to some other port options: - \u0026#34;8080\u0026#34; - Other (specify custom option) default: \u0026#34;8080\u0026#34; answer: \u0026#34;8080\u0026#34; - id: move2kube.services.java-maven.wartransformer type: Select description: Select the transformer to use for service java-maven options: - Liberty - Tomcat - Jboss default: Liberty answer: Liberty - id: move2kube.services.rust.port type: Select description: \u0026#39;Select port to be exposed for the service rust :\u0026#39; hints: - Select Other if you want to expose the service rust to some other port options: - \u0026#34;8085\u0026#34; - Other (specify custom option) default: \u0026#34;8085\u0026#34; answer: \u0026#34;8085\u0026#34; - id: move2kube.services.myproject-java-war.wartransformer type: Select description: Select the transformer to use for service myproject-java-war options: - Liberty - Tomcat - Jboss default: Liberty answer: Liberty - id: move2kube.services.golang.ports type: MultiSelect description: \u0026#39;Select ports to be exposed for the service golang :\u0026#39; hints: - Select Other if you want to add more ports options: - \u0026#34;8080\u0026#34; - Other (specify custom option) default: - \u0026#34;8080\u0026#34; answer: - \u0026#34;8080\u0026#34; - id: move2kube.containerruntime type: Select description: \u0026#39;Select the container runtime to use :\u0026#39; hints: - The container runtime selected will be used in the scripts options: - docker - podman default: docker answer: docker - id: move2kube.services.\u0026#34;rust\u0026#34;.\u0026#34;8085\u0026#34;.urlpath type: Input description: What URL/path should we expose the service rust\u0026#39;s 8085 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /rust answer: /rust - id: move2kube.services.\u0026#34;myproject-django\u0026#34;.\u0026#34;8080\u0026#34;.urlpath type: Input description: What URL/path should we expose the service myproject-django\u0026#39;s 8080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /myproject-django answer: /myproject-django - id: move2kube.services.\u0026#34;golang\u0026#34;.\u0026#34;8080\u0026#34;.urlpath type: Input description: What URL/path should we expose the service golang\u0026#39;s 8080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /golang answer: /golang - id: move2kube.services.\u0026#34;nodejs\u0026#34;.\u0026#34;8080\u0026#34;.urlpath type: Input description: What URL/path should we expose the service nodejs\u0026#39;s 8080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /nodejs answer: /nodejs - id: move2kube.services.\u0026#34;ruby\u0026#34;.\u0026#34;8080\u0026#34;.urlpath type: Input description: What URL/path should we expose the service ruby\u0026#39;s 8080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /ruby answer: /ruby - id: move2kube.services.\u0026#34;myproject-python\u0026#34;.\u0026#34;8080\u0026#34;.urlpath type: Input description: What URL/path should we expose the service myproject-python\u0026#39;s 8080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /myproject-python answer: /myproject-python - id: move2kube.services.\u0026#34;myproject-php\u0026#34;.\u0026#34;8082\u0026#34;.urlpath type: Input description: What URL/path should we expose the service myproject-php\u0026#39;s 8082 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /myproject-php answer: /myproject-php - id: move2kube.minreplicas type: Input description: Provide the minimum number of replicas each service should have hints: - If the value is 0 pods won\u0026#39;t be started by default default: \u0026#34;2\u0026#34; answer: \u0026#34;2\u0026#34; - id: move2kube.target.imageregistry.url type: Select description: \u0026#39;Enter the URL of the image registry : \u0026#39; hints: - You can always change it later by changing the yamls. options: - Other (specify custom option) - us.icr.io - quay.io - registry.ng.bluemix.net default: quay.io answer: quay.io - id: move2kube.target.imageregistry.namespace type: Input description: \u0026#39;Enter the namespace where the new images should be pushed : \u0026#39; hints: - \u0026#39;Ex : myproject\u0026#39; default: myproject answer: myproject - id: move2kube.target.imageregistry.logintype type: Select description: \u0026#39;[quay.io] What type of container registry login do you want to use?\u0026#39; hints: - Docker login from config mode, will use the default config from your local machine. options: - Use existing pull secret - No authentication - UserName/Password default: No authentication answer: No authentication - id: move2kube.target.clustertype type: Select description: \u0026#39;Choose the cluster type:\u0026#39; hints: - Choose the cluster type you would like to target options: - AWS-EKS - Azure-AKS - GCP-GKE - IBM-IKS - IBM-Openshift - Kubernetes - Openshift default: Kubernetes answer: Kubernetes - id: move2kube.target.ingress.host type: Input description: Provide the ingress host domain hints: - Ingress host domain is part of service URL default: myproject.com answer: myproject.com - id: move2kube.target.ingress.tls type: Input description: Provide the TLS secret for ingress hints: - Leave empty to use http default: \u0026#34;\u0026#34; answer: \u0026#34;\u0026#34; - id: move2kube.services.\u0026#34;myproject-java-war\u0026#34;.\u0026#34;9080\u0026#34;.urlpath type: Input description: What URL/path should we expose the service myproject-java-war\u0026#39;s 9080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /myproject-java-war answer: /myproject-java-war - id: move2kube.services.\u0026#34;java-maven\u0026#34;.\u0026#34;9080\u0026#34;.urlpath type: Input description: What URL/path should we expose the service java-maven\u0026#39;s 9080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /java-maven answer: /java-maven - id: move2kube.services.\u0026#34;java-gradle\u0026#34;.\u0026#34;9080\u0026#34;.urlpath type: Input description: What URL/path should we expose the service java-gradle\u0026#39;s 9080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /java-gradle answer: /java-gradle The cache file contains both the questions and the answers along with additional information about each question, such as the default answer, the type of the question, the question IDs, any hints that were provided, etc.\nThe config file stores the answer to a question under the key specified by the question\u0026rsquo;s ID.\nFor example, the question What URL/path should we expose the service java-maven's 9080 port on? has the id move2kube.services.\u0026quot;java-maven\u0026quot;.\u0026quot;9080\u0026quot;.urlpath. So we find the answer in the config file /java-maven stored as:\nmove2kube: services: java-maven: \u0026#34;9080\u0026#34;: urlpath: /java-maven Every time Move2Kube goes to ask a question, it first checks the config file to see if it has already been answered using the question\u0026rsquo;s ID. If the ID is not present in the config file, Move2Kube will usually ask the user for the answer meaning we can provide the answer to any question by storing it in the config file.\nRun the transform again with the generated config file. $ mv myproject old # rename the output directory from the previous run to avoid conflicts $ ls m2k.plan\tm2kconfig.yaml\tm2kqacache.yaml\told\tlanguage-platforms $ move2kube transform --config m2kconfig.yaml INFO[0000] Detected a plan file at path /Users/user/Desktop/tutorial/m2k.plan. Will transform using this plan. INFO[0000] Starting Plan Transformation ... INFO[0007] Plan Transformation done INFO[0007] Transformed target artifacts can be found at [/Users/user/Desktop/tutorial/myproject]. $ ls m2k.plan\tm2kconfig.yaml\tm2kqacache.yaml\tmyproject\told\tlanguage-platforms This time Move2Kube didn\u0026rsquo;t ask any questions because we provided all of the answers by editing the config file directly to change the answer to a question. We can also remove some of the answers from the config file if we want Move2Kube to ask us those questions again. This gives us a nice way to iterate quickly, as well as a way to run Move2Kube non-interatively.\n"},{"uri":"http://konveyor.github.io/move2kube/tutorials/","title":"Tutorials","tags":[],"description":"","content":"These are some of the source platforms that Move2Kube supports.\n"},{"uri":"http://konveyor.github.io/move2kube/installation/installweb/","title":"Web Interface","tags":[],"description":"","content":"Minikube can be installed using Docker or Podman web interfaces.\nInstalling Move2Kube using Docker Follow the steps below to install Move2Kube with options of persistence by mounting to the current directory and advanced features by mounting to the Docker socket allowing Move2Kube to run container based transformers.\nMove2Kube can also be installed as a Helm Chart from ArtifactHub. For more information on Helm Chart and Operator see Move2Kube Operator\nProcedure .\nNote For bleeding edge features, development, and testing use, follow the steps below, but replace v0.3.0 with latest.\nInstall Move2Kube. $ docker run --rm -it -p 8080:8080 quay.io/konveyor/move2kube-ui:v0.3.0 Set persistence (optional). $ docker run --rm -it -p 8080:8080 \\ -v \u0026#34;${PWD}/move2kube-api-data:/move2kube-api/data\u0026#34; \\ quay.io/konveyor/move2kube-ui:v0.3.0 Set advanced features (optional). $ docker run --rm -it -p 8080:8080 \\ -v \u0026#34;${PWD}/move2kube-api-data:/move2kube-api/data\u0026#34; \\ -v //var/run/docker.sock:/var/run/docker.sock \\ quay.io/konveyor/move2kube-ui:v0.3.0 Installing Move2Kube using Podman Run the command below to install Move2Kube. $ podman run --rm -it -p 8080:8080 quay.io/konveyor/move2kube-ui:v0.3.0 Accessing the Move2Kube UI Open a Web browser and navigate to http://localhost:8080/ to access the UI.\n**Note: There is a known issue when mounting directories in WSL. Some empty directories may be created in the root directory. If you are using Windows, use Powershell instead of WSL until this is fixed.\n"},{"uri":"http://konveyor.github.io/move2kube/installation/installcli/","title":"Command line tool","tags":[],"description":"","content":"Installing on Linux / MacOS / Windows WSL (Recommended): The easiest way to install Move2Kube is to download the pre-built binaries for Linux, MacOS, and Windows from the list of releases on Github: https://github.com/konveyor/move2kube/releases. Follow the steps below to install the latest stable version.\nProcedure\nInstall Move2Kube with one of the following options. Latest stable version: bash \u0026lt;(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) A specific version (for example version v0.3.0-beta.0): MOVE2KUBE_TAG=\u0026#39;v0.3.0-beta.0\u0026#39; bash \u0026lt;(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Bleeding edge version: BLEEDING_EDGE=\u0026#39;true\u0026#39; bash \u0026lt;(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Without sudo: USE_SUDO=false bash \u0026lt;(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Consider setting the following options. The script installs to /usr/local/bin by default. To install to a different directory: MOVE2KUBE_INSTALL_DIR=/my/new/install/dir bash \u0026lt;(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Make the installation directory in the PATH to run Move2Kube as move2kube instead of /my/new/install/dir/move2kube. Combine the above two environment variables to install without sudo and install to a different directory. USE_SUDO=false MOVE2KUBE_INSTALL_DIR=/my/new/install/dir bash \u0026lt;(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Alternate installations Move2Kube can be installed using Homebrew and Go.\nHomebrew\nbrew tap konveyor/move2kube brew install move2kube To install a specific version (for example version v0.3.0-beta.0):\nbrew install move2kube@0.3.0-beta.0 Go\nInstall using go get pulls from the main branch of Move2Kube with the latest development changes.\ngo get -u github.com/konveyor/move2kube "},{"uri":"http://konveyor.github.io/tackle/overview/","title":"Overview","tags":[],"description":"","content":"Tackle is a collection of tools that support modernizing and migrating applications to Kubernetes. These tools assess applications to determine which option is the appropriate migration strategy for each application:\nRehosting Replatforming Refactoring Tackle uses an interactive questionnaire for the assessment which enables key stakeholders to gather information about applications, discuss risks flagged by Tackle, and reach a consensus in formulating recommendations for each application.\nTackle Refactoring Tools The tools are cloud-native micro-services that are accessible from a common Tackle UI.\nApplication Inventory Pathfinder Controls Application Inventory Tackle Application Inventory is the vehicle which selects applications for assessment by Pathfinder. It provides users four main functions:\nMaintain a portfolio of applications. Link applications to the business services they support. Define interdependencies. Add metadata using an extensible tagging model to describe and categorize applications in multiple dimensions. Pathfinder Tackle Pathfinder is an interactive questionnaire based tool that assesses the suitability of applications for modernization in order to deploy them into containers on an enterprise Kubernetes platform. The tool output includes::\nApplications suitability for Kubernetes Associated risks Adoption plan with the applications prioritization, business criticality and dependencies. Controls Tackle Controls are a collection of entities that add value to Application Inventory and the Pathfinder assessment. They comprise business services, stakeholders, stakeholder groups, job functions, tag types, and tags.\nTackle Controls are a collection of entities that add value to the Application Inventory and the Pathfinder assessment including:\nBusiness services Stakeholders Stakeholder groups Job functions Tag types Tag Tackle Projects Tackle Web UI Tackle Application Inventory Tackle Pathfinder Tackle Controls Tackle Documentation Tackle Commons REST Tackle Keycloak Theme Tackle DiVA Tackle Test Generator Tackle Container Advisor Source\n"},{"uri":"http://konveyor.github.io/crane/overview/","title":"Overview","tags":[],"description":"","content":"The Crane tool helps application owners migrate Kubernetes workloads and their states between clusters, remove environment-specific configuration, and automate application deployments along the way.\nThe process uses a few tools:\ncrane: The command line tool that migrates applications to the terminal. crane-lib: The brains behind Crane functionality responsible for transforming resources. crane-plugin-openshift: Plugin specifically tailored to manage OpenShift migration workloads and an example of a repeatable best-practice. crane-plugins: Collection of plugins from the Konveyor community based on experience from performing Kube migrations. Why crane is needed? Crane is the product of several years of experience performing large-scale production Kubernetes migrations that are large, complex, error-prone, and usually peformed in a limited window of time.\nTo face those challenges, the Crane migration tool is designed with transparency and ease-of-diagnostics in mind. It drives migration through a pipeline of non-destructive tasks that output results to disk so the operation can be easily audited and versioned without ever impacting live workloads. The tasks can be run repeatedly and will output consistent results given the same inputs without side-effects on the system at large.\nSource\n"},{"uri":"http://konveyor.github.io/forklift/overview/","title":"Overview","tags":[],"description":"","content":"Forklift provides a simplistic way of large scale migration virtual machines at scale to Kubernetes from traditional hypervisors like VMware (vSphere) and Red Hat Virtualization (oVirt) to KubeVirt. Forklift is an open source tool based on proven core linux v2v technologies that can rehost a few or hundreds of VMs while minimizing downtime as it lifts and shifts the applications. The benefits of Forklift include:\nMigration Analytics: Validate the compatibility of the virtual machine before executing the migration. Warm migrations: Reduce downtime by using warm migration capabilities, which will pre-copy the data before finalizing the migration. Migration at Scale: Migrate small or large amount of virtual machines at once. Forklift operates using the functionality of multiple projects:\nForklift Operator. The Forklift Operator deploys and maintains Forklift. Forklift UI. The Forklift UI is based on Patternfly 4. Forklift Controller. The Forklift Controller orchestrates the migration. Forklift Validation. The Forklift Validation service checks the virtual machines for possible issues before migration. Runs with Open Policy Agent. Forklift must-gather. Support tool for gathering information about the Forklift environment. Additional resources\nPerformance recommendations for migrating from VMware vSphere to OpenShift Virtualization. Performance recommendations for migrating from Red Hat Virtualization to OpenShift Virtualization. Migration Types Forklift supports cold migration from oVirt (oVirt) and warm migration from both VMware vSphere and from oVirt.\nCold migration Cold migration is the default migration type where the source\u0026rsquo;s virtual machines are shut down while the data is copied.\nWarm migration Warm migration copies most of the data during the precopy stage while the source virtual machines (VMs) are running. During a time of minimal data transactions, the cutover stage shuts down the VMs and the remaining data is copied.\nMigration Stages Forklift operates in two stages, precopy and cutover.\nPrecopy stage The VMs are not shut down during the precopy stage. Instead the VM disks are copied incrementally using changed block tracking (CBT) snapshots. The snapshots are created at one-hour intervals by default but can be changed by updating the forklift-controller deployment.\nImportant CBT must be enabled for each source VM and each VM disk.\nA VM can support up to 28 CBT snapshots. If the source VM has too many CBT snapshots and the Migration Controller service is not able to create a new snapshot, warm migration might fail. The Migration Controller service deletes each snapshot when it is no longer required.\nThe precopy stage runs until the cutover stage is started manually or is scheduled to start.\nCutover stage The VMs are shut down during the cutover stage and the remaining data is migrated. Data stored in RAM is not migrated.\nStart the cutover stage manually by using the Forklift console or by scheduling a cutover time in the Migration manifest.\nSource\n"},{"uri":"http://konveyor.github.io/tackle/upgradeto2/","title":"Upgrading to 2.0","tags":[],"description":"","content":" For additional information, see the Tackle CLI tool directory in the tackle2-hub repository.\nA CLI tool is available to upgrade Tackle that will migrate application data seeds from a running 1.2 instance to a running 2.0 instance, or export the data to be imported at a later time. The tool has the following functions:\nexport-tackle1: Exports Tackle 1.2 API objects into local JSON files (tags, tag types, and job functions). tackle --skip-destination-check export-tackle1: Exports Tackle 1.2 API objects into a local directory. import: Creates objects in Tackle 2 from local JSON files. clean: Deletes objects uploaded to Tackle 2 from local JSON files. clean-all: Deletes all data returned by Tackle 2 including seeds and additional data to clean, and skips some Pathfinder items without index action. Export/Import To export data from a running 1.2 Tackle instance and import it into a running 2.0 instance, the following commands are used.\n* tackle export-tackle1 * tackle import The export-tackle1 command identifies seed resources in the Tackle2 instance first, then downloads all resources from Tackle 1.2 API, transforms it to the format expected by the Tackle 2 Hub, and lastly re-maps resources to the seeds already existing in the destination Tackle2 Hub API. The result is serialized and stored into local JSON files.\nThe tackle import command connects to Tackle2 Hub, checks existing objects for possible collisions (by IDs) and uploads resources from local JSON files.\nDelayed Export If a Tackle 2.0 instance is not available or one needs to be cleaned up, all 1.2 instance data is exported to a local directory regardless of what already exists in Tackle 2.0. To export data for delayed import, the following commands are used.\n* tackle --skip-destination-check export-tackle1 * tackle clean-all * tackle import See the CLI options section below for more tool functions.\nGit Python 3 with a YAML parser RHEL8/Python 3.6: Parser included RHEL9/Python 3.9: PyYAML module needs to be installed Verifying and installing YAML Parser Follow the steps below to install a YAML parser.\nNote: Use the python3 -m pip install pyyaml command to verify a parser is installed or to install the parser on non-RHEL-like distributions that have the dnf command.\nInstall parser for RHEL-like Linux systems. dnf install python39 python3-pyyaml git Using the Upgrade tool Follow these procedures to download and start the upgrade tool, then export the 1.2 Tackle instance objects and import them into the Tackle 2.0 instance.\nGetting started Follow the steps below to clone the tool repository, set credentials, and run the tool.\nUse the tackle-config.yml.example file as a template to set your Tackle endpoints and credentials and save it as tackle-config.yml.\nProcedure\nClone Github repository. git clone https://github.com/konveyor/tackle2-hub.git Change to the tool directory. cd hack/tool Set API endpoints and credentials in a config file (tackle-config.yml by default). # Main Tackle 2 endpoint and credentials url: https://tackle-konveyor-tackle.apps.cluster.local username: admin password: Passw0rd! # Tackle 1.2 endpoint and credentials, e.g. to dump data and migrate to Tackle2 tackle1: url: https://tackle-tackle.apps.mta01.cluster.local username: tackle password: password Run the tackle tool. ./tackle Export Tackle 1.2 Run tackle export-tackle1 to create a data dump of Tackle 1.2 objects into JSON files in a default local directory: ./tackle-data.\nNote: Unverified HTTPS warnings from Python could be hidden by export PYTHONWARNINGS=\u0026ldquo;ignore:Unverified HTTPS request\u0026rdquo; or with the -w command option.\nProcedure\nRun the tackle export on the 1.2 instance. tackle export-tackle1 Review the JSON files in the ./tackle-data directory and modify if necessary. Import to Tackle 2 Hub After checking the local JSON dump files in ./tackle-data directory (if needed), use the command below to create objects in Tackle 2 Hub by running tackle import.\nProcedure\nRun the tackle export on the 1.2 instance. tackle import Import troubleshooting The tackle import command could fail in a pre-import check phase which ensures there are no resources of a given type with the same ID in the Tackle 2.0 destination (error after checking tagtypes in destination Tackle2.. etc.). In this case, run the tackle clean command, to remove these objects from the destination API, or remove it manually either from the destination or from the JSON data files.\nIf the import failed in the upload phase (error after Uploading tagtypes.. etc.), try running tackle clean to remove already imported objects followed by tackle clean-all which lists all resources of all known data types in the destination Tackle 2 API and deletes it without looking to local data files.\nCleaning objects Warning: All clean actions may delete objects already present in the Tackle 2.0 that are unrelated to the import data.\nClean objects In the event existing objects in the Tackle 2.0 instance are in conflict with the Tackle 1.2 JSON dump files, the following command will delete objects previously created by the import command.\nProcedure\nRun the tackle export on the 1.2 instance. tackle clean Clean all objects Follow the steps below to list objects from all data types and delete those resources to clean the Tackle 2.0 instance.\nNote: The clean-all command deletes all resources from the Tackle 2.0 Hub API, however, the Pathfinder API does not support listing assessments without providing an applicationID. The applications may not be present in Hub, so an \u0026ldquo;orphaned\u0026rdquo; assessments could stay in Pathfinder. In order to resolve potential collision with imported data, run tackle clean together with the clean-all command.\nProcedure\nRun the tackle export on the 1.2 instance. tackle clean-all Review the JSON files in the ./tackle-data directory. Command line options The upgrade tool has the following command line options.\n-c / --config: This path specifies a YAML file with configuration options including endpoints and credentials for Tackle APIs (tackle-config.yml by default).\n-v / --verbose: Verbose output option logs all API requests and responses providing more information for possible debugging (off by default).\n-d / --data-dir: Data directory specifies a path to a local directory for Tackle database records in JSON format (./tackle-data by default).\n-s / --skip-destination-check: Creates a full export without having access to the Tackle 2.0 destination and executes all seed objects with this option. When importing this data, the destination needs to be empty (run clean-all first).\n-w / --disable-ssl-warnings: Supress SSL warnings option for api requests.\n-i / --ignore-import-errors: Import errors can be skipped.\nWarning: This option is not recommended. Only use with high attention to avoid data inconsistency. If the import has failed, it is recommended use the tackle clean command to only remove imported resources.\nExample:\n$ tackle --help usage: tackle [-h] [-c [CONFIG]] [-d [DATA_DIR]] [-v] [-s] [action ...] Konveyor Tackle maintenance tool. positional arguments: action One or more Tackle commands that should be executed, options: export-tackle1 import clean clean-all options: -h, --help show this help message and exit -c [CONFIG], --config [CONFIG] A config file path (tackle-config.yml by default). -d [DATA_DIR], --data-dir [DATA_DIR] Local Tackle data directory path (tackle-data by default). -v, --verbose Print verbose output (including all API requests). -s, --skip-destination-check Skip connection and data check of Tackle 2 destination. -w, --disable-ssl-warnings Do not display warnings during ssl check for api requests. -i, --ignore-import-errors Skip to next item if an item fails load. "},{"uri":"http://konveyor.github.io/tackle/admintasks/","title":"Administrator view tasks","tags":[],"description":"","content":"The administrator view is intended to be used by administrators to set up the Tackle instance environment. Credentials This management module enables administrators to create and manage credentials for access to private repositories. It also allows for the architects to assign the credentials to applications without knowing their contents. Configuring source control credentials Follow the steps below to create new credentials for a source control repository.\nProcedure\nClick Credentials in the left menu of the Administrator view. Click the Create new button. Enter the following information. Name Description (Optional) Select the Source Control in the Type drop-down list. Select the credential type in the User Credentials drop-down list and enter the requested information. Username/Password Username Password (Hidden) Create credentials SCM Private Key/Passphrase SCM Private Key Private Key Passphrase (Hidden) Note: Type specific credential information such as keys and passphrases will be hidden or shown as [Encrypted].\nClick Create. Tackle validates the input and creates a new credential. SCM keys must be parsed and checked for validity. If the validation fails, an error message displaying not a valid key/XML file is displayed.\nConfiguring Maven credentials Follow the steps below to create new credentials for a Maven repository.\nProcedure\nClick Credentials in the left menu of the Administrator view. Click the Create new button. Enter the following information. Name Description (Optional) Select Maven Settings File in the Type drop-down list. Upload the settings file. Click Create. Tackle validates the input and creates a new credential. Maven settings.xml files must be parsed and checked for validity. If the validation fails, an error message displaying not a valid key/XML file is displayed.\nConfiguring proxy credentials Follow the steps below to create new credentials for a proxy repository.\nProcedure\nClick Credentials in the left menu of the Administrator view. Click the Create new button. Enter the following information. Name Description (Optional) Select Proxy in the Type drop-down list. Enter the following information. Username Password Note: Type specific credential information such as keys and passphrases will be hidden or shown as [Encrypted].\nClick Create. Repositories This management module configures the repositories used by Tackle with the following options.\nConfiguring Git repositories Follow the steps below to configure a Git repository.\nClick Repositories and then Git in the left menu of the Administrator view. Click the Consume insecure Git repositories toggle switch to enable. Configuring Subversion repositories Follow the steps below to configure a Subversion repository.\nClick Repositories and then Subversion in the left menu of the Administrator view. Click the Consume insecure Subversion repositories toggle switch to enable. Subversion Configuring Maven repositories Follow the steps below to configure a Maven repository or clear the local artifact repository.\nConfiguring the repository Click Repositories and then Maven in the left menu of the Administrator view. Click the Force update of dependencies toggle switch to enable. Click the Consume insecure artifact repositories toggle switch to enable. Managing repository size Click Repositories and then Maven in the left menu of the Administrator view. Click the Clear repository link. Note Due to the size of the repository, the size change may not be evident despite the function working properly.\nProxy This management module configures HTTP \u0026amp; HTTPS proxy settings. To configure the proxies click the radio button and enter the following information.\nClick Proxy in the left menu of the Administrator view. Click the HTTP proxy or HTTPS proxy toggle switch to enable the proxy connection. Enter the following information Proxy host Proxy port Click the HTTP proxy credentials or HTTPS proxy credentials toggle switch to enable authentication (optional). Select the credential from the drop-down list. "},{"uri":"http://konveyor.github.io/tackle/views/","title":"Views","tags":[],"description":"","content":"The latest version of Tackle has the Developers view and the new Architects view to support the three main roles of users:\nAdministrators: Has access to some application-wide configuration parameters that other users can consume but not change or browse.\nExample actions: Define Git credentials, Maven settings, .xml files.\nArchitects: Often the technical leads for the migration project that can create and modify applications and information related to it. The Architects do not need to have access to sensitive information, but can consume it.\nExample actions: Associate an existing credential to access the repository of a given application.\nMigrators: Developers that can run assessments and analysis, but cannot create or modify applications in the portfolio. Maybe an example action?\nDeveloper view The developer view is intended to be used by migrators and stakeholders and has three pages with different functionalities. Application inventory The Application inventory page manages the applications being migrated and is where the assessment and analysis processes are performed. The application list provides a holistic view of the application portfolio using an extensible tagging model to classify application types. The applications can be expanded to show more detailed information which can be edited and managed.\nThis page has two tabs with different information and functionality: Assessment and Analysis.\nApplication Assessments Use the Assessment tab to facilitate a conversation before migrating an application with stakeholders such as technical subject matter experts and application owners or users. Tackle does not prescribe solutions but instead provides a script of discussion points to identify potential migration risks.\nAssessment categories include:\nApplication details Application dependencies Application architecture Application observability Application cross-cutting concerns Reviewing the results will help stakeholders develop suitable migration strategies for applications or application types. Only one assessment can be done at once, but assessment results can be applied to other applications.\nAdditional functionality includes:\nCopy assessment Copy assessment and review Discard assessment Manage dependencies Manage credentials Delete Analysis Use the Analysis tab to perform an automated examination of the application that views binaries, source code, and dependencies to find possible issues that might prevent the application from running on the new platform. The process starts by retrieving and then analyzing one of the following repositories:\nBinary: Provides a full picture of the application and library dependencies that are embedded into the code. Source code: Only provides a view of the application. Source code + dependencies: Provides a full picture by downloading the source code from the repository, then parses and downloads the dependencies from Maven and adds them to the source code. There is an option to upload a locally stored binary. This only works if a single application is selected.\nAdditional functionality includes:\nManage dependencies Manage credentials Analysis details Cancel analysis Delete Reports The Reports page provides an overview of the assessments and reviews for the entire application inventory. The Reports page contains the following sections:\nCurrent landscape: Displays all applications according to their risk levels. Adoption candidate distribution: Lists the assessed applications with the following columns: Criticality is based on the Business criticality value of the review. Priority is based on the Work priority value of the review. Effort is based on the Effort estimate value of the review. Decision is based on the Proposed action value of the review. By default, all applications are selected. You can clear some of the application check boxes to filter the report.\nSuggested adoption plan: Displays a suggested adoption plan based on effort, priority, and dependencies. Identified risks: Lists the severe risks identified in the assessments for all applications. Controls The Controls page is where the application parameters are managed by the architect or developers as the instance is configured and edited as the project progresses.\nThe parameters include:\nStakeholders Stakeholder groups Job functions Business services Tags (Tag Types) Administrator view The administrator view is intended to be used by administrators to set up the Tackle instance environment. Credentials This management module enables administrators to create and manage credentials for access to private repositories. It also allows for the architects to assign the credentials to applications without knowing their contents. The credentials page displays the available credentials with an Edit and Delete buttons and the following fields:\nName Description Type Source Control Username/Password Source Private Key/Passphrase Maven Settings File Proxy Type specific information Created by Note: Type specific credential information such as keys and passphrases will be hidden or shown as [Encrypted].\nRepositories This management module configures the repositories used by Tackle with the following options. Git Consume insecure Git repositories Subversion Consume insecure Subversion repositories Maven Manage repository size Force update of dependencies Consume insecure artifact repositories Proxy This management module configures HTTP \u0026amp; HTTPS proxy settings and credentials. To configure the proxies click the radio button and enter the following information. Proxy host Proxy port Authentication (optional) Proxy credentials Source\n"},{"uri":"http://konveyor.github.io/tackle/instances/","title":"Seeding Instances","tags":[],"description":"","content":"Tackle instances have key parameters that are configured in the Controls window prior to migration by the project architect and can be added and edited as needed.\nThese parameters define applications and individuals, teams, verticals or areas within an organization affected or participating in the migration.\nStakeholders Stakeholder groups Job functions Business services Tag types Tags Seeding Tackle instance The steps to creating and configuring a Tackle instance can be performed in any order, but the suggested order below is the most efficient for creating stakeholders and tags.\nStakeholders\nCreate stakeholder groups Create job functions Create stakeholders Tags\nCreate tag types Create tags Tackle stakeholders and defined by:\nEmail Name Job function Stakeholder groups Creating a new stakeholder group Follow the steps below to create a new stakeholder group. There are no default stakeholder groups defined.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Stakeholder groups tab. Click the Create new button. Enter the following information: Name Description Member(s) Click the Create button. Creating a new job function Tackle uses the job function attribute to classify stakeholders and provides a list of default values that can be expanded. Follow the steps below to create a new job function not included in the default list.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Job functions tab. Click the Create new button. Enter a job function title into the Name text box. Click the Create button. Creating a new stakeholder Follow the steps below to create a new migration project stakeholder.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Stakeholders tab. Click the Create new button. Enter the following information: Email Name Job function: Addition beyond the standard can be created Stakeholder group Click the Create button. Business services Tackle uses the business services attribute to define the departments within the organization that use the application and will be affected by the migration.\nImportant: Business services must be created prior to creating or importing applications.\nCreating a new business service Follow the steps below to create a new business service. There are no default business services defined and must be created prior to creating applications.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Business services tab. Click the Create new button. Enter the following information: Name Description Owner Click the Create button. Creating new tag types Tackle uses tags to classify applications in multiple categories. Follow the steps below to create a new tag type not included in the default list.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Tags tab. Click the Create tag type button. Enter the following information: Name Rank - order the tags appear on applications Color Click the Create button. Creating new tags Follow the steps below to create a new tag not included in the default list.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Tags tab. Click the Create tag types button. Enter the following information: Name Tag Type Click the Create button. Source\n"},{"uri":"http://konveyor.github.io/tackle/installation/","title":"Installing Tackle","tags":[],"description":"","content":"Follow the steps below to provision the minikube cluster and install Tackle 2.0.\nProvisioning Minikube Follow the steps below to provision minikube for single users deploying Tackle on a workstation. These steps will configure minikube and enable:\nIngress so the Tackle tool can publish outside of the Kubernetes cluster. Operator lifecycle manager (OLM) addon. (OpenShift has OLM installed out of the box but Kubernetes does not.) Procedure\nProvision the minikube cluster with these recommended parameters. Replace \u0026lt;profile name\u0026gt; with your choice of minikube profile name. [user@user ~]$ minikube start --memory=10g -p \u0026lt;profile name\u0026gt; Enable the ingress addon. [user@user ~]$ minikube addons enable ingress -p \u0026lt;profile name\u0026gt; Install Operator Lifecycle Manager (OLM), a tool to help manage the Operators running on your cluster. [user@user ~]$ curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.21.2/install.sh | bash -s v0.21.2 Installing Tackle Operator Operators are a structural layer that manages resources deployed on Kubernetes (database, front end, back end) to automatically create a Tackle instance instead of doing it manually.\nRequirements Tackle requires a total of 5 persistent volumes (PVs) used by different components to successfully deploy, 3 RWO volumes and 2 RWX volumes will be requested via PVCs.\nName Default Size Access Mode Description hub database 5Gi RWO Hub DB hub bucket 100Gi RWX Hub file storage keycloak postgresql 1Gi RWO Keycloak backend DB pathfinder postgresql 1Gi RWO Pathfinder backend DB maven 100Gi RWX maven m2 repository Follow the steps below to install the Tackle Operator in the my-tackle-operator namespace (default) on any Kubernetes distribution, including minikube.\nProcedure\nInstall the Tackle Operator. [user@user ~]$ kubectl create -f https://operatorhub.io/install/tackle-operator.yaml Verify Tackle was installed. [user@user ~]$ kubectl get pods -n my-tackle-operator Repeat this step until konveyor-tackle-XXX and tackle-operator-XXX show 1/1 Running. Create the Tackle instance Follow the steps below to initiate the Tackle instance and set a custom resource (CR) with the tackle_hub.yaml file. CRs can be customized to meet the project needs.\nProcedure\nCreate the instance pointing to the CR file. [user@user ~]$ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - kind: Tackle apiVersion: tackle.konveyor.io/v1alpha1 metadata: name: tackle namespace: my-tackle-operator spec: EOF Verify the instance [user@user ~]$ kubectl get pods -n my-tackle-operator Repeat this step until all components are Completed or Running. Note: This can take one to five minutes depending on the cluster resources.\nLogging in to Tackle Web Console Follow the steps below to log into the Tackle web console.\nProcedure\nAccess the minikube dashboard. This will enable the dashboard add-on, and open the proxy in the default web browser. [user@user ~]$ minikube dashboard -p \u0026lt;profile name\u0026gt; Ensure the top dropdown namespace selector is set to the my-tackle-operator Click Service then Ingresses Click the endpoint IP for the tackle ingress ingress to launch the Tackle web console in a new browser window. Note: This may default to http://$IP_ADDR and fail to load, if so switch to https://$IP_ADDR\nThe default auth enabled credentials are: admin/Passw0rd! Source\n"},{"uri":"http://konveyor.github.io/tackle2/","title":"Welcome to Tackle 2.0","tags":[],"description":"","content":"Tackle documentation\nTackle 2.0 includes the seamless integration with the analysis capabilities of the Windup project, also known downstream as Migration Toolkit for Applications, enabling Tackle to effectively manage, assess and now also analyze applications to have a holistic view at your portfolio when dealing with large scale modernization and Kubernetes adoption initiatives. This is essential to provide key insights that allow Architects leading these massive projects to make informed decisions, thus reducing risks and making the required effort measurable and predictable.\nThe following is a list of the exciting new features included in Tackle 2.0:\nAdministrator perspective: Dedicated perspective to manage tool-wide configuration, with a similar approach and design to the OpenShift Administrator Perspective.. Enhanced RBAC: Three new differentiated personas with different permissions - Administrator, Architect and Migrator Integration with repositories: Full integration with source code (Git, Subversion) and binaries (Maven) repositories to automate the retrieval of applications for analysis. Credentials management: Secure store for multiple credential types (source control, Maven settings files, proxy). Credentials are managed by Administrators and assigned by Architects to applications. Proxy integration: HTTP and HTTPS proxy configuration can be managed in the Tackle UI. Analysis module: Full integration with the Windup project to allow the execution of application analysis from the application inventory. Enhanced analysis modes: Aside from source and binary analysis modes, now Tackle includes the Source + Dependencies mode that parses the POM file available in the source repository to gather dependencies from corporate or public artifact repositories, adding them to the scope of the analysis. Analysis scope selection: Simplified user experience to configure the analysis scope, with the possibility to force the analysis of known Open Source libraries. Authless deployment: Tackle can now be optionally deployed without Keycloak, allowing full unauthenticated admin access to the tool. This is especially useful when deploying the tool in resource constrained environments like local instances of Minikube, where only a single user would have access to it. Seamless upgrades: Tackle lifecycle is now managed by a new operator with Capability Level II, allowing seamless upgrades between GA versions. Find more documentation here or by clicking Tackle 1.0 \u0026amp; 2.0 in the left pane. In case you find anything missing, please let us know by creating an issue in the Konveyor documentation repository.\n"},{"uri":"http://konveyor.github.io/tackle/assessanalyze/","title":"Assessing and analyzing applications","tags":[],"description":"","content":"Tackle core functions are assessing and analyzing the applications for migration and are performed on the Application inventory page. Assessing applications Follow the steps below to facilitate discussion of application migration.\nProcedure\nClick Application inventory in the left menu in the Development view. Click the checkbox to the left of the application being assessed. Note: Only one application can be assessed at a time.\nClick the Assess button. Select the Stakeholders and Stakeholder groups from the drop-down lists to track who contributed to the assessment for future reference. Note: Additional list options can be added in the Stakeholder Groups or Stakeholders tab on the Controls page in the Developer view.\nClick the Next button. Click the radio button next to the option that best answers the questions in each category and click Next to go to the next section when complete. Click Save and Review to view the risks that should be taken into account. Applying assessments to other applications Follow the steps below to apply an application assessment to similar applications being migrated.\nProcedure\nClick Application inventory in the left menu in the Development view. Click the checkbox of the application with the completed assessment to copy. Click the menu kebab at the right of the selected application. Select Copy assessment or Copy assessment and review. Click the checkbox of the application(s) to copy the assessment or assessment and review to. Click the Copy button. Running application analysis Follow the steps below to analyze an application for migration.\nProcedure\nClick Application inventory in the left menu in the Development view. Click the Analysis tab. Click the checkbox to the left of the application being analyzed. Note: More than one application can be analyzed at a time.\nCheck the credentials assigned to the application. Click the Analyze button. Select source for analysis from the drop-down list. Click the Next button. Set the target to one or more of the transformation targets. Application server migration to Containerization Quarkus OpenJDK OpenJDK 11 Linux Camel Click the Next button. Click a radio button to select one of the following scope options to narrow down and focus the analysis. Application and internal dependencies only Application and all dependencies, including known Open Source libraries Select the list of packages to be analyzed manually (Type the file name and click Add.) Exclude packages (Type the package name and click Add.) Click the Next button. Set custom rules by typing a name or searching and clicking the Add Rules button. Consider the following rules: Target Source Exclude tags: Rules with these tags are not processed. Note: Analysis engines use standard rules for a comprehensive set of migration targets, but if the target is not included or customized frameworks custom rules can be added. Custom rules files will be validated.\nClick the Next button. Add or remove targets and sources to narrow the analysis. Exclude rules tags as necessary. Click the Next button. Verify the analysis parameters. Click the Run button. Analysis status will show Scheduled as it downloads the image for the container to execute. When that is complete, it will show In-progress until complete.\nNote: The analysis will take minutes to hours to run depending on the size of the application and the cluster capacity and resources.\nTackle relies on Kubernetes scheduling capabilities to determine how many analyzer instances are created based on cluster capacity. If several applications are selected for analysis, by default, only one analyzer can be provisioned at a time. With more cluster capacity, more analysis processes can be executed in parallel. 19. Expand the application and click the Report link to the right of Analysis when completed.\nReviewing the Analysis Report Follow the steps below to review the analysis report. For more information see Chapter 3. Reviewing the reports of the CLI Guide Migration Toolkit For Applications 5.3.\nProcedure\nClick Application inventory in the left menu in the Development view. Expand the application with a completed analysis. Click the Report link. Click the dependencies or source links. Click the tabs to review the report. Source\n"},{"uri":"http://konveyor.github.io/tackle/addapps/","title":"Adding applications","tags":[],"description":"","content":"Applications can be added to Tackle by creating new applications from scratch manually or by importing them. Tackle applications are defined by manually entered and pre-defined attributes:\nName (manual) Description (manual) Business service (pre-defined) Tags (pre-defined) Source code Binary Creating a new application Follow the steps below to add a new application to the inventory for assessment and analysis.\nNote: Before starting this procedure, it is helpful to set up business services and check tags and tag types and create additions as needed.\nProcedure\nClick Application inventory in the left menu of the Developer view. Click the Create new button. Enter the following information: Name Description Business service Tags Comments Click the arrow to the right of Source Code to expand the section. Enter the following information: Repository type Source Repository Branch Root path Click the down arrow to the right of Binary to expand the section. Enter the following information: Group Artifact Version Packaging Click the Create button. Assigning application credentials Follow the steps below to assign credentials to one or more applications.\nProcedure\nClick Application inventory in the left menu of the Developer view. Click the Analysis tab. Click the menu kebab to the right of the Review button and select Manage credentials. Select the credential from the Source credentials or Maven setting drop-down list. Click the Save button. Importing application lists Tackle allows users to import a spreadsheet of existing applications instead of entering them manually. A recommended sample CSV template has been made available to users to fill in with the required information.\nDownloading the application list CSV template. Follow the steps below to download the sample CSV template.\nProcedure\nClick Application inventory in the left menu of the Developer view. Click the menu kebab button to the right of the Review button. Click Manage imports to open the Application imports page. Click the kebab menu to the right of the Import button and click Download CSV template. Importing an application list Follow the steps below to import a .csv file provided by the customer containing a list of applications and their attributes. Importing can be performed by using the Import or Manage import functions, but using the steps below is recommended for verifying the import was successful.\nNote: Importing will only add applications, it will not overwrite any existing ones.\nProcedure\nReview the import file for all the required information in the required format. Click Application inventory in the left menu of the Developer view. Click the menu kebab button to the right of the Review button. Click the Import button. Browse to the file and click the Import button. Verify the import has completed and check the number of rows accepted or rejected. Review the imported applications by clicking the arrow to the left of the check box to expand. Important: Rows accepted may not match the number of applications in the Application inventory list because some rows are dependencies. To verify, check the record type column of the CSV file for applications defined as 1 and dependencies as 2.\nSource\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/migrateoptions/","title":"Migration plan options","tags":[],"description":"","content":"On the Migration plans page of the Forklift web console, click the Options menu kebab beside a migration plan to access the following options:\nEdit: Edit the details of a migration plan. A migration plan cannot be edited while it is running or after it has completed successfully. Duplicate: Create a new migration plan with the same virtual machines (VMs), parameters, mappings, and hooks as an existing plan. Use this feature for the following tasks: Migrating VMs to a different namespace. Editing an archived migration plan. Editing a migration plan with a different status, for example, failed, canceled, running, critical, or ready. Archive: Delete the logs, history, and metadata of a migration plan. The plan cannot be edited or restarted. It can only be viewed. Important: The Archive option is irreversible, but an archived plan can be duplicated.\nDelete: Permanently remove a migration plan. A running migration plan cannot be deleted. Important The Delete option is irreversible.\nDeleting a migration plan does not remove temporary resources such as importer pods, conversion pods, config maps, secrets, failed VMs, and data volumes. (BZ#2018974) A migration plan must be archived before deleting it in order to clean up the temporary resources.\nView details: Display the details of a migration plan. Restart: Restart a failed or canceled migration plan. Cancel scheduled cutover: Cancel a scheduled cutover migration for a warm migration plan. Source\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/migratecli/","title":"Migrating virtual machines using the command line","tags":[],"description":"","content":"Follow the steps below to migrate virtual machines (VMs) to KubeVirt using the command line (CLI) by creating Forklift custom resources (CRs) and specifying:\nA name for cluster-scoped CRs A name and a namespace for namespace-scoped CRs Prerequisites\nMust be logged in as a user with cluster-admin privileges. VMware only: Must have the vCenter SHA-1 fingerprint. Must create a VMware Virtual Disk Development Kit (VDDK) image in a secure registry that is accessible to all clusters. Procedure\nCreate a Secret manifest for the source provider credentials: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: \u0026lt;secret\u0026gt; namespace: konveyor-forklift type: Opaque stringData: user: \u0026lt;user\u0026gt; (1) password: \u0026lt;password\u0026gt; (2) cacert: | (3) \u0026lt;oVirt_ca_certificate\u0026gt; thumbprint: \u0026lt;vcenter_fingerprint\u0026gt; (4) EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify the vCenter user or the oVirt Engine user. (2) Specify the user password. (3) oVirt only: Specify the CA certificate of the Engine. Retrieve it at https://\u0026lt;{rhv-short}_engine_host\u0026gt;/ovirt-engine/services/pki-resource?resource=ca-certificate\u0026amp;format=X509-PEM-CA. (4) VMware only: Specify the vCenter SHA-1 fingerprint. Create a Provider manifest for the source provider: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Provider metadata: name: \u0026lt;provider\u0026gt; namespace: konveyor-forklift spec: type: \u0026lt;provider_type\u0026gt; (1) url: \u0026lt;api_end_point\u0026gt; (2) settings: vddkInitImage: \u0026lt;registry_route_or_server_path\u0026gt;/vddk: (3) secret: name: \u0026lt;secret\u0026gt; (4) namespace: konveyor-forklift EOF The explanations below refer to the callouts in the sample code above.\n(1) Allowed values are ovirt and vsphere. (2) Specify the API end point URL, for example, https://\u0026lt;vCenter_host\u0026gt;/sdk for vSphere or https://\u0026lt;{rhv-short}_engine_host\u0026gt;/ovirt-engine/api/ for oVirt. (3) VMware only: Specify the VDDK image created. (4) Specify the name of provider Secret CR. VMware only: Create a Host manifest: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Host metadata: name: \u0026lt;vmware_host\u0026gt; namespace: konveyor-forklift spec: provider: namespace: konveyor-forklift name: \u0026lt;source_provider\u0026gt; (1) id: \u0026lt;source_host_mor\u0026gt; (2) ipAddress: \u0026lt;source_network_ip\u0026gt; (3) EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify the name of the VMware Provider CR. (2) Specify the managed object reference (MOR) of the VMware host. (3) Specify the IP address of the VMware migration network. Create a NetworkMap manifest to map the source and destination networks: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: NetworkMap metadata: name: \u0026lt;network_map\u0026gt; namespace: konveyor-forklift spec: map: - destination: name: \u0026lt;pod\u0026gt; namespace: konveyor-forklift type: pod (1) source: (2) id: \u0026lt;source_network_id\u0026gt; (3) name: \u0026lt;source_network_name\u0026gt; - destination: name: \u0026lt;network_attachment_definition\u0026gt; (4) namespace: \u0026lt;network_attachment_definition_namespace\u0026gt; (5) type: multus source: id: \u0026lt;source_network_id\u0026gt; name: \u0026lt;source_network_name\u0026gt; provider: source: name: \u0026lt;source_provider\u0026gt; namespace: konveyor-forklift destination: name: \u0026lt;destination_cluster\u0026gt; namespace: konveyor-forklift EOF The explanations below refer to the callouts in the sample code above.\n(1) Allowed values are pod and multus. (2) Use either the id or the name parameter to specify the source network. (3) Specify the VMware network MOR or oVirt network UUID. (4) Specify a network attachment definition for each additional KubeVirt network. (5) Specify the namespace of the KubeVirt network attachment definition. Create a StorageMap manifest to map source and destination storage: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: StorageMap metadata: name: \u0026lt;storage_map\u0026gt; namespace: konveyor-forklift spec: map: - destination: storageClass: \u0026lt;storage_class\u0026gt; accessMode: \u0026lt;access_mode\u0026gt; (1) source: id: \u0026lt;source_datastore\u0026gt; (2) - destination: storageClass: \u0026lt;storage_class\u0026gt; accessMode: \u0026lt;access_mode\u0026gt; source: id: \u0026lt;source_datastore\u0026gt; provider: source: name: \u0026lt;source_provider\u0026gt; namespace: konveyor-forklift destination: name: \u0026lt;destination_cluster\u0026gt; namespace: konveyor-forklift EOF The explanations below refer to the callouts in the sample code above.\n(1) Allowed values are ReadWriteOnce and ReadWriteMany. (2) Specify the VMware data storage MOR or oVirt storage domain UUID, for example, f2737930-b567-451a-9ceb-2887f6207009. Optional: Create a Hook manifest to run custom code on a VM during the phase specified in the Plan CR: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Hook metadata: name: \u0026lt;hook\u0026gt; namespace: konveyor-forklift spec: image: quay.io/konveyor/hook-runner (1) playbook: | (2) LS0tCi0gbmFtZTogTWFpbgogIGhvc3RzOiBsb2NhbGhvc3QKICB0YXNrczoKICAtIG5hbWU6IExv YWQgUGxhbgogICAgaW5jbHVkZV92YXJzOgogICAgICBmaWxlOiAiL3RtcC9ob29rL3BsYW4ueW1s IgogICAgICBuYW1lOiBwbGFuCiAgLSBuYW1lOiBMb2FkIFdvcmtsb2FkCiAgICBpbmNsdWRlX3Zh cnM6CiAgICAgIGZpbGU6ICIvdG1wL2hvb2svd29ya2xvYWQueW1sIgogICAgICBuYW1lOiB3b3Jr bG9hZAoK EOF The explanations below refer to the callouts in the sample code above.\n(1) Use the default hook-runner image or specify a custom image. A playbook does not need to be specified, if a custom image is. (2) Optional: Base64-encoded Ansible playbook. The image must be hook-runner if a playbook is specified. Create a Plan manifest for the migration: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Plan metadata: name: \u0026lt;plan\u0026gt; (1) namespace: konveyor-forklift spec: warm: true (2) provider: source: name: \u0026lt;source_provider\u0026gt; namespace: konveyor-forklift destination: name: \u0026lt;destination_cluster\u0026gt; namespace: konveyor-forklift map: network: (3) name: \u0026lt;network_map\u0026gt; (4) namespace: konveyor-forklift storage: name: \u0026lt;storage_map\u0026gt; (5) namespace: konveyor-forklift targetNamespace: konveyor-forklift vms: (6) - id: \u0026lt;source_vm\u0026gt; (7) - name: \u0026lt;source_vm\u0026gt; hooks: (8) - hook: namespace: konveyor-forklift name: \u0026lt;hook\u0026gt; (9) step: \u0026lt;step\u0026gt; (10) EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify the name of the Plan CR. (2) Specify whether the migration is warm or cold. Only the precopy stage will run in a warm migration if the cutover parameter in the Migration manifest value is not specified. (3) Add multiple network mappings if needed. (4) Specify the name of the NetworkMap CR. (5) Specify the name of the StorageMap CR. (6) Use either the id or the name parameter to specify the source VMs. (7) Specify the VMware VM MOR or oVirt VM UUID. (8) Optional: Specify up to two hooks for a VM. Each hook must run during a separate migration step. (9) Specify the name of the Hook CR. (10) Allowed values are PreHook, before the migation plan starts, or PostHook, after the migration is complete. Create a Migration manifest to run the Plan CR: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Migration metadata: name: \u0026lt;migration\u0026gt; (1) namespace: konveyor-forklift spec: plan: name: \u0026lt;plan\u0026gt; (2) namespace: konveyor-forklift cutover: \u0026lt;cutover_time\u0026gt; (3) EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify the name of the Migration CR. (2) Specify the name of the Plan CR being run. The Migration CR creates a VirtualMachine CR for each VM that is migrated. (3) Optional: Specify a cutover time according to the ISO 8601 format with the UTC time offset, for example, 2021-04-04T01:23:45.678+09:00. Multiple Migration CRs can be associated with a single Plan CR. If a migration does not complete, create a new Migration CR, without changing the Plan CR, to migrate the remaining VMs. Retrieve the Migration CR to monitor the progress of the migration: $ kubectl get migration/\u0026lt;migration\u0026gt; -n konveyor-forklift -o yaml Obtaining the SHA-1 fingerprint of a vCenter host Obtain the SHA-1 fingerprint of a vCenter host in order to create a Secret CR.\nProcedure\nRun the following command: $ openssl s_client \\ -connect \u0026lt;vcenter_host\u0026gt;:443 \\ (1) \u0026lt; /dev/null 2\u0026gt;/dev/null \\ | openssl x509 -fingerprint -noout -in /dev/stdin \\ | cut -d \u0026#39;=\u0026#39; -f 2 The explanations below refer to the callouts in the sample code above.\n(1) Specify the IP address or FQDN of the vCenter host. Example output:\n01:23:45:67:89:AB:CD:EF:01:23:45:67:89:AB:CD:EF:01:23:45:67 Canceling a migration Forklift functionality cancel cancel an entire migration or individual virtual machines (VMs) while a migration is in progress from the command line interface (CLI).\nCanceling an entire migration Follow the steps below to cancel an entire migration.\nProcedure\nDelete the Migration CR: $ kubectl delete migration \u0026lt;migration\u0026gt; -n konveyor-forklift (1) The explanations below refer to the callouts in the sample code above.\n(1) Specify the name of the Migration CR. Canceling individual VMs migration Follow the steps below to cancel migrating an individual VM.\nProcedure\nAdd the individual VMs to the spec.cancel block of the Migration manifest: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Migration metadata: name: \u0026lt;migration\u0026gt; namespace: konveyor-forklift ... spec: cancel: - id: vm-102 (1) - id: vm-203 - name: rhel8-vm EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify a VM by using the id key or the name key. The value of the id key is the managed object reference, for a VMware VM, or the VM UUID, for a oVirt VM. Retrieve the Migration CR to monitor the progress of the remaining VMs: $ kubectl get migration/\u0026lt;migration\u0026gt; -n konveyor-forklift -o yaml Source\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/migrateweb/","title":"Migrating virtual machines using the web console","tags":[],"description":"","content":"Migrate virtual machines (VMs) to KubeVirt using the Forklift web console by performing the folloing tasks:\nAdding source and destination providers Creating network and storage mapping Creating and running a migration plan Prerequisites\nEnsure that all Prerequisites are set. VMware only: Create a VMware Virtual Disk Development Kit (VDDK) image. Adding source providers Follow the procedures below to add VMware or oVirt source providers using the Forklift web console.\nAdding a VMware source provider Add a VMware source provider by using the Forklift web console.\nPrerequisites\nVMware Virtual Disk Development Kit (VDDK) image in a secure registry that is accessible to all clusters. Procedure\nOpen the Forklift web console. Click Providers, then Add provider. Select VMware from the Type list. Fill in the following fields: Name: Name to display in the list of providers Hostname or IP address: vCenter host name or IP address Username: vCenter user, for example, user@vsphere.local Password: vCenter user password VDDK init image: VDDKInitImage path Click Verify certificate. Select the I trust the authenticity of this certificate checkbox. Click Add to add and save the provider. The source provider appears in the list of providers.\nAdding an oVirt source provider Add an oVirt source provider by using the Forklift web console.\nPrerequisites\nCA certificate of the Engine. Procedure\nOpen the Forklift web console. Click Providers, then Add provider. Select Red Hat Virtualization from the Type list. Fill in the following fields: Name: Name toU display in the list of providers Hostname or IP address: Engine host name or IP address Username: Engine user Password: Engine password CA certificate: CA certificate of the Engine Click Add to add and save the provider. The source provider appears in the list of providers.\nSelecting a migration network for a source provider Select a migration network in the Forklift web console for a source provider to reduce risk to the source environment and to improve performance.\nImportant: Using the default network for migration can result in poor performance because the network might not have sufficient bandwidth which can have a negative effect on the source platform because the disk transfer operation might saturate the network.\nPrerequisites\nThe migration network must a minimum speed of 10 Gbps for disk transfer. The migration network must be accessible to the KubeVirt nodes through the default gateway. The source virtual disks are copied by a pod that is connected to the pod network of the target namespace. The migration network must have jumbo frames enabled. Procedure\nOpen the Forklift web console. Click Providers, the the oVirt8 or VMware tab. Click the host number in the Hosts column beside a provider to view a list of hosts. Select one or more hosts and click Select migration network. Select a Network. Note: Clear the selection by selecting the default network.\nComplete the following fields: For VMware:\nESXi host admin username: Specify the ESXi host admin user, for example, root. ESXi host admin password: Specify the ESXi host admin password. For oVirt:\nUsername: Specify the Engine user. Password: Specify the Engine password. Click Save. Verify that the status of each host is Ready. Note: If a host status is not Ready, the host might be unreachable on the migration network or the credentials might be incorrect. Modify the host configuration and save the changes to correct.\nAdding a KubeVirt destination provider Add a KubeVirt destination provider to the Forklift web console in addition to the default KubeVirt destination provider, which is the provider where Forklift.\nPrerequisites\nKubeVirt service account token with cluster-admin privileges. Procedure\nOpen the Forklift web console. Click Providers, the Add provider. Select KubeVirt from the Type list. Complete the following fields: Cluster name: Specify the cluster name to display in the list of target providers. URL: Specify the API endpoint of the cluster. Service account token: Specify the cluster-admin service account token. Click Check connection to verify the credentials. Click Add. The provider appears in the list of providers.\nSelecting a migration network for a KubeVirt provider Select a default migration network for a KubeVirt provider in the Forklift web console to improve performance. The default migration network is used to transfer disks to the namespaces in which it is configured.\nNote: If a migration network is not selected, the default migration network is the pod network which might not be optimal for disk transfer.\nOverride the default migration network of the provider by selecting a different network when creating a migration plan.\nProcedure\nOpen the Forklift web console. Click Providers then the KubeVirt tab. Select a provider and click Select migration network. Select a network from the list of available networks and click Select. Click the network number in the Networks column beside the provider to verify that the selected network is the default migration network. Creating a network mapping Create one or more network mappings using the Forklift web console to map source networks to KubeVirt networks.\nPrerequisites\nSource and target providers added to the web console. Note: If more than one source and target network are mapped, each additional KubeVirt network requires its own network attachment definition.\nProcedure\nClick Mappings. Click the Network tab then click Create mapping. Complete the following fields: Name: Enter a name to display in the network mappings list. Source provider: Select a source provider. Target provider: Select a target provider. Source networks: Select a source network. Target namespaces/networks: Select a target network. Optional: Click Add to create additional network mappings or to map multiple source networks to a single target network. Note: If an additional network mapping is created, select the network attachment definition as the target network.\nClick Create. The network mapping is displayed on the Network mappings screen.\nCreating a storage mapping Create a storage mapping using the Forklift web console to map source data stores to KubeVirt storage classes.\nPrerequisites\nSource and target providers added to the web console. Local and shared persistent storage that support VM migration. Procedure\nClick Mappings. Click the Storage tab and then click Create mapping. Set the following parameters: Name of the storage mapping. Source provider Target provider VMware: Source datastore Target storage class oVirt: Source storage domain Target storage class Optional: Click Add to create additional storage mappings or to map multiple source data stores or storage domains to a single storage class. Click Create. The mapping is displayed on the Storage mappings page.\nCreating a migration plan Create a migration plan by using the Forklift web console.\nA migration plan allows virtual machines to be grouped and migrated together or with the same migration parameters. For example: a percentage of the members of a cluster or a complete application.\nConfigure a hook to run an Ansible playbook or custom container image during a specified stage of the migration plan.\nPrerequisites\nIf Forklift is not installed on the target cluster, add a target provider on the Providers page of the web console. Procedure\nOpen the web console. Click Migration plans then click Create migration plan. Complete the following fields: Plan name: Enter a migration plan name to display in the migration plan list. Plan description: Optional: Brief description of the migration plan. Source provider: Select a source provider. Target provider: Select a target provider. Target namespace: Type to search for an existing target namespace or create a new namespace. Change the migration transfer network for this plan by clicking Select a different network, selecting a network from the list, and clicking Select. If a migration transfer network is defined for the KubeVirt provider and if the network is in the target namespace, that network is the default network for all migration plans. Otherwise, the pod network is used.\nClick Next. Select options to filter the list of source VMs and click Next. Select the VMs to migrate and then click Next. Select an existing network mapping or create a new network mapping. Creating a new network mapping Follow the steps below to create a network mapping.\nProcedure\nSelect a target network for each source network. Optional: Select Save mapping to use again and enter a network mapping name. Click Next. Select an existing storage mapping or create a new storage mapping. Creating a new storage mapping Follow the steps below to create a storage mapping.\nProcedure\nSelect a target storage class for each VMware data store or oVirt storage domain. Optional: Select Save mapping to use again and enter a storage mapping name. Click Next. Select a migration type and click Next. Cold migration: The source VMs are stopped while the data is copied. Warm migration: The source VMs run while the data is copied incrementally and the cutover runs later which stops the VMs and copies the remaining VM data and metadata. Optional: Create a migration hook to run an Ansible playbook before or after migration: Click Add hook. Select the step when the hook will run. Select a hook definition: Ansible playbook: Browse to the Ansible playbook or paste it into the field. Custom container image: Enter the image path: \u0026lt;registry_path\u0026gt;/\u0026lt;image_name\u0026gt;: to not use the default hook-runner image. Important: The registry must be accessible to the OKD cluster.\nClick Next. Review the migration plan and click Finish. The migration plan is saved in the migration plan list.\nClick the Options menu kebab of the migration plan and select View details to verify the migration plan details. Running a migration plan Run a migration plan and view its progress in the Forklift web console.\nPrerequisites\nValid migration plan. Procedure\nClick Migration plans. The Migration plans list displays the source and target providers, the number of virtual machines (VMs) being migrated, and the status of the plan.\nClick Start beside a migration plan to start the migration. Warm migration only: When the precopy stage starts.\nClick Cutover to complete the migration. Expand a migration plan to view the migration details. The migration details screen displays the migration start and end time, the amount of data copied, and a progress pipeline for each VM being migrated.\nExpand a VM to view the migration steps, elapsed time of each step, and its state. Source\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/cancelmigrate/","title":"Canceling a migration","tags":[],"description":"","content":"Follow the steps below to cancel the migration of some or all virtual machines (VMs) while a migration plan is in progress by using the Forklift web console.\nProcedure\nClick Migration Plans. Click the name of a running migration plan to view the migration details. Select one or more VMs and click Cancel. Click Yes, cancel to confirm the cancellation. The status of the VM shows canceled in the Migration details by VM list. The unmigrated and the migrated virtual machines are not affected.\nRestarting Restart a canceled migration by clicking Restart beside the migration plan on the Migration plans page.\nSource\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/advancedmigrate/","title":"Advanced migration options","tags":[],"description":"","content":"Changing precopy intervals for warm migration Follow the steps below to change the snapshot interval by patching the ForkliftController custom resource (CR).\nProcedure\nPatch the ForkliftController CR: $ kubectl patch forkliftcontroller/\u0026lt;forklift-controller\u0026gt; -n konveyor-forklift -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;controller_precopy_interval\u0026#34;: \u0026lt;60\u0026gt;}}\u0026#39; --type=merge (1) The explanation below refers to the callout in the sample code above.\n(1) Specify the precopy interval in minutes. The default value is 60. Note: The forklift-controller pod does not need to be restarted.\nCreating custom rules for the Validation service The Validation service uses Open Policy Agent (OPA) policy rules to check the suitability of each virtual machine (VM) for migration. The Validation service generates a list of concerns for each VM, which are stored in the Provider Inventory service as VM attributes. The web console displays the concerns for each VM in the provider inventory.\nCustom rules to extend the default ruleset of the Validation service can be created. For example, create a rule that checks whether a VM has multiple disks.\nAbout Rego files Validation rules are written in Rego, the Open Policy Agent (OPA) native query language. The rules are stored as .rego files in the /usr/share/opa/policies/io/konveyor/forklift/ directory of the Validation pod.\nEach validation rule is defined in a separate .rego file and tests for a specific condition. If the condition evaluates as true, the rule adds a {category, label, assessment} hash to the concerns. The concerns content is added to the concerns key in the inventory record of the VM. The web console displays the content of the concerns key for each VM in the provider inventory.\nThe following .rego file example checks for distributed resource scheduling enabled in the cluster of a VMware VM:\ndrs_enabled.rego example:\npackage io.konveyor.forklift.vmware (1) has_drs_enabled { input.host.cluster.drsEnabled (2) } concerns[flag] { has_drs_enabled flag := { \u0026#34;category\u0026#34;: \u0026#34;Information\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;VM running in a DRS-enabled cluster\u0026#34;, \u0026#34;assessment\u0026#34;: \u0026#34;Distributed resource scheduling is not currently supported by OpenShift Virtualization. The VM can be migrated but it will not have this feature in the target environment.\u0026#34; } } The explanations below refer to the callouts in the sample code above.\n(1) Each validation rule is defined within a package. The package namespaces are io.konveyor.forklift.vmware for VMware and io.konveyor.forklift.ovirt for oVirt. (2) Query parameters are based on the input key of the Validation service JSON. Checking the default validation rules Before creating a custom rule, follow the steps below to check the default rules of the Validation service to prevent creating a rule that redefines an existing default value.\nExample: If a default rule contains the line default valid_input = false and a custom rule that contains the line default valid_input = true is created, the Validation service will not start.\nProcedure\nConnect to the terminal of the Validation pod: $ kubectl rsh \u0026lt;validation_pod\u0026gt; Go to the OPA policies directory for the provider: $ cd /usr/share/opa/policies/io/konveyor/forklift/\u0026lt;provider\u0026gt; (1) The explanations below refer to the callouts in the sample code above.\n(1) Specify vmware or ovirt. Search for the default policies: $ grep -R \u0026#34;default\u0026#34; * Retrieving the Inventory service JSON Follow the steps below to retrieve the Inventory service JSON by sending an Inventory service query to a virtual machine (VM). The output contains an \u0026ldquo;input\u0026rdquo; key, which contains the inventory attributes that are queried by the Validation service rules.\nCreate a validation rule based on any attribute in the \u0026ldquo;input\u0026rdquo; key, for example, input.snapshot.kind.\nProcedure\nRetrieve the Inventory service route: $ kubectl get route \u0026lt;inventory_service\u0026gt; -n konveyor-forklift Retrieve the UUID of a provider: $ GET https://\u0026lt;inventory_service_route\u0026gt;/providers/\u0026lt;provider\u0026gt; (1) The explanations below refer to the callouts in the sample code above.\n(1) Allowed values for the provider are vsphere and ovirt. Retrieve the VMs of a provider: $ GET https://\u0026lt;inventory_service_route\u0026gt;/providers/\u0026lt;provider\u0026gt;/\u0026lt;UUID\u0026gt;/vms Retrieve the details of a VM: $ GET https://\u0026lt;inventory_service_route\u0026gt;/providers/\u0026lt;provider\u0026gt;/\u0026lt;UUID\u0026gt;/workloads/\u0026lt;vm\u0026gt; Example output:\n{ \u0026#34;input\u0026#34;: { \u0026#34;selfLink\u0026#34;: \u0026#34;providers/vsphere/c872d364-d62b-46f0-bd42-16799f40324e/workloads/vm-431\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;vm-431\u0026#34;, \u0026#34;parent\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Folder\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;group-v22\u0026#34; }, \u0026#34;revision\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;iscsi-target\u0026#34;, \u0026#34;revisionValidated\u0026#34;: 1, \u0026#34;isTemplate\u0026#34;: false, \u0026#34;networks\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-31\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-33\u0026#34; } ], \u0026#34;disks\u0026#34;: [ { \u0026#34;key\u0026#34;: 2000, \u0026#34;file\u0026#34;: \u0026#34;[iSCSI_Datastore] iscsi-target/iscsi-target-000001.vmdk\u0026#34;, \u0026#34;datastore\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-63\u0026#34; }, \u0026#34;capacity\u0026#34;: 17179869184, \u0026#34;shared\u0026#34;: false, \u0026#34;rdm\u0026#34;: false }, { \u0026#34;key\u0026#34;: 2001, \u0026#34;file\u0026#34;: \u0026#34;[iSCSI_Datastore] iscsi-target/iscsi-target_1-000001.vmdk\u0026#34;, \u0026#34;datastore\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-63\u0026#34; }, \u0026#34;capacity\u0026#34;: 10737418240, \u0026#34;shared\u0026#34;: false, \u0026#34;rdm\u0026#34;: false } ], \u0026#34;concerns\u0026#34;: [], \u0026#34;policyVersion\u0026#34;: 5, \u0026#34;uuid\u0026#34;: \u0026#34;42256329-8c3a-2a82-54fd-01d845a8bf49\u0026#34;, \u0026#34;firmware\u0026#34;: \u0026#34;bios\u0026#34;, \u0026#34;powerState\u0026#34;: \u0026#34;poweredOn\u0026#34;, \u0026#34;connectionState\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;snapshot\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;VirtualMachineSnapshot\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;snapshot-3034\u0026#34; }, \u0026#34;changeTrackingEnabled\u0026#34;: false, \u0026#34;cpuAffinity\u0026#34;: [ 0, 2 ], \u0026#34;cpuHotAddEnabled\u0026#34;: true, \u0026#34;cpuHotRemoveEnabled\u0026#34;: false, \u0026#34;memoryHotAddEnabled\u0026#34;: false, \u0026#34;faultToleranceEnabled\u0026#34;: false, \u0026#34;cpuCount\u0026#34;: 2, \u0026#34;coresPerSocket\u0026#34;: 1, \u0026#34;memoryMB\u0026#34;: 2048, \u0026#34;guestName\u0026#34;: \u0026#34;Red Hat Enterprise Linux 7 (64-bit)\u0026#34;, \u0026#34;balloonedMemory\u0026#34;: 0, \u0026#34;ipAddress\u0026#34;: \u0026#34;10.19.2.96\u0026#34;, \u0026#34;storageUsed\u0026#34;: 30436770129, \u0026#34;numaNodeAffinity\u0026#34;: [ \u0026#34;0\u0026#34;, \u0026#34;1\u0026#34; ], \u0026#34;devices\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;RealUSBController\u0026#34; } ], \u0026#34;host\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;host-29\u0026#34;, \u0026#34;parent\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Cluster\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;domain-c26\u0026#34; }, \u0026#34;revision\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;IP address or host name of the vCenter host or {rhv-short} Engine host\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;providers/vsphere/c872d364-d62b-46f0-bd42-16799f40324e/hosts/host-29\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;inMaintenance\u0026#34;: false, \u0026#34;managementServerIp\u0026#34;: \u0026#34;10.19.2.96\u0026#34;, \u0026#34;thumbprint\u0026#34;: \u0026lt;thumbprint\u0026gt;, \u0026#34;timezone\u0026#34;: \u0026#34;UTC\u0026#34;, \u0026#34;cpuSockets\u0026#34;: 2, \u0026#34;cpuCores\u0026#34;: 16, \u0026#34;productName\u0026#34;: \u0026#34;VMware ESXi\u0026#34;, \u0026#34;productVersion\u0026#34;: \u0026#34;6.5.0\u0026#34;, \u0026#34;networking\u0026#34;: { \u0026#34;pNICs\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PhysicalNic-vmnic0\u0026#34;, \u0026#34;linkSpeed\u0026#34;: 10000 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PhysicalNic-vmnic1\u0026#34;, \u0026#34;linkSpeed\u0026#34;: 10000 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PhysicalNic-vmnic2\u0026#34;, \u0026#34;linkSpeed\u0026#34;: 10000 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PhysicalNic-vmnic3\u0026#34;, \u0026#34;linkSpeed\u0026#34;: 10000 } ], \u0026#34;vNICs\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualNic-vmk2\u0026#34;, \u0026#34;portGroup\u0026#34;: \u0026#34;VM_Migration\u0026#34;, \u0026#34;dPortGroup\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ipAddress\u0026#34;: \u0026#34;192.168.79.13\u0026#34;, \u0026#34;subnetMask\u0026#34;: \u0026#34;255.255.255.0\u0026#34;, \u0026#34;mtu\u0026#34;: 9000 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualNic-vmk0\u0026#34;, \u0026#34;portGroup\u0026#34;: \u0026#34;Management Network\u0026#34;, \u0026#34;dPortGroup\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ipAddress\u0026#34;: \u0026#34;10.19.2.13\u0026#34;, \u0026#34;subnetMask\u0026#34;: \u0026#34;255.255.255.128\u0026#34;, \u0026#34;mtu\u0026#34;: 1500 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualNic-vmk1\u0026#34;, \u0026#34;portGroup\u0026#34;: \u0026#34;Storage Network\u0026#34;, \u0026#34;dPortGroup\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ipAddress\u0026#34;: \u0026#34;172.31.2.13\u0026#34;, \u0026#34;subnetMask\u0026#34;: \u0026#34;255.255.0.0\u0026#34;, \u0026#34;mtu\u0026#34;: 1500 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualNic-vmk3\u0026#34;, \u0026#34;portGroup\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dPortGroup\u0026#34;: \u0026#34;dvportgroup-48\u0026#34;, \u0026#34;ipAddress\u0026#34;: \u0026#34;192.168.61.13\u0026#34;, \u0026#34;subnetMask\u0026#34;: \u0026#34;255.255.255.0\u0026#34;, \u0026#34;mtu\u0026#34;: 1500 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualNic-vmk4\u0026#34;, \u0026#34;portGroup\u0026#34;: \u0026#34;VM_DHCP_Network\u0026#34;, \u0026#34;dPortGroup\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ipAddress\u0026#34;: \u0026#34;10.19.2.231\u0026#34;, \u0026#34;subnetMask\u0026#34;: \u0026#34;255.255.255.128\u0026#34;, \u0026#34;mtu\u0026#34;: 1500 } ], \u0026#34;portGroups\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM Network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM Network\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch0\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-Management Network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Management Network\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch0\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM_10G_Network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM_10G_Network\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM_Storage\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM_Storage\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM_DHCP_Network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM_DHCP_Network\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-Storage Network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Storage Network\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM_Isolated_67\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM_Isolated_67\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch2\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM_Migration\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM_Migration\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch2\u0026#34; } ], \u0026#34;switches\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vSwitch0\u0026#34;, \u0026#34;portGroups\u0026#34;: [ \u0026#34;key-vim.host.PortGroup-VM Network\u0026#34;, \u0026#34;key-vim.host.PortGroup-Management Network\u0026#34; ], \u0026#34;pNICs\u0026#34;: [ \u0026#34;key-vim.host.PhysicalNic-vmnic4\u0026#34; ] }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vSwitch1\u0026#34;, \u0026#34;portGroups\u0026#34;: [ \u0026#34;key-vim.host.PortGroup-VM_10G_Network\u0026#34;, \u0026#34;key-vim.host.PortGroup-VM_Storage\u0026#34;, \u0026#34;key-vim.host.PortGroup-VM_DHCP_Network\u0026#34;, \u0026#34;key-vim.host.PortGroup-Storage Network\u0026#34; ], \u0026#34;pNICs\u0026#34;: [ \u0026#34;key-vim.host.PhysicalNic-vmnic2\u0026#34;, \u0026#34;key-vim.host.PhysicalNic-vmnic0\u0026#34; ] }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch2\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vSwitch2\u0026#34;, \u0026#34;portGroups\u0026#34;: [ \u0026#34;key-vim.host.PortGroup-VM_Isolated_67\u0026#34;, \u0026#34;key-vim.host.PortGroup-VM_Migration\u0026#34; ], \u0026#34;pNICs\u0026#34;: [ \u0026#34;key-vim.host.PhysicalNic-vmnic3\u0026#34;, \u0026#34;key-vim.host.PhysicalNic-vmnic1\u0026#34; ] } ] }, \u0026#34;networks\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-31\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-34\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-57\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-33\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;dvportgroup-47\u0026#34; } ], \u0026#34;datastores\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-35\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-63\u0026#34; } ], \u0026#34;vms\u0026#34;: null, \u0026#34;networkAdapters\u0026#34;: [], \u0026#34;cluster\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;domain-c26\u0026#34;, \u0026#34;parent\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Folder\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;group-h23\u0026#34; }, \u0026#34;revision\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;mycluster\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;providers/vsphere/c872d364-d62b-46f0-bd42-16799f40324e/clusters/domain-c26\u0026#34;, \u0026#34;folder\u0026#34;: \u0026#34;group-h23\u0026#34;, \u0026#34;networks\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-31\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-34\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-57\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-33\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;dvportgroup-47\u0026#34; } ], \u0026#34;datastores\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-35\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-63\u0026#34; } ], \u0026#34;hosts\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Host\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;host-44\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Host\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;host-29\u0026#34; } ], \u0026#34;dasEnabled\u0026#34;: false, \u0026#34;dasVms\u0026#34;: [], \u0026#34;drsEnabled\u0026#34;: true, \u0026#34;drsBehavior\u0026#34;: \u0026#34;fullyAutomated\u0026#34;, \u0026#34;drsVms\u0026#34;: [], \u0026#34;datacenter\u0026#34;: null } } } } Creating a validation rule Follow the steps below to create a validation rule by applying a config map custom resource (CR) containing the rule to the Validation service.\nIf the rule has the same name as an existing rule, the Validation service performs an OR operation with the rules.\nIf the rule contradicts a default rule, the Validation service will not start.\nValidation rule example Validation rules are based on virtual machine (VM) attributes collected by the Provider Inventory service.\nFor example, the VMware API uses this path to check whether a VMware VM has NUMA node affinity configured: MOR:VirtualMachine.config.extraConfig[\u0026ldquo;numa.nodeAffinity\u0026rdquo;].\nThe Provider Inventory service simplifies this configuration and returns a testable attribute with a list value:\n\u0026#34;numaNodeAffinity\u0026#34;: [ \u0026#34;0\u0026#34;, \u0026#34;1\u0026#34; ], Create a Rego query, based on this attribute, and add it to the forklift-validation-config config map:\n`count(input.numaNodeAffinity) != 0` Procedure\nCreate a config map CR according to the following example: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: \u0026lt;forklift-validation-config\u0026gt; namespace: konveyor-forklift data: vmware_multiple_disks.rego: |- package \u0026lt;provider_package\u0026gt; (1) has_multiple_disks { (2) count(input.disks) \u0026gt; 1 } concerns[flag] { has_multiple_disks (3) flag := { \u0026#34;category\u0026#34;: \u0026#34;\u0026lt;Information\u0026gt;\u0026#34;, (4) \u0026#34;label\u0026#34;: \u0026#34;Multiple disks detected\u0026#34;, \u0026#34;assessment\u0026#34;: \u0026#34;Multiple disks detected on this VM.\u0026#34; } } EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify the provider package name. Allowed values are io.konveyor.forklift.vmware for VMware and io.konveyor.forklift.ovirt for oVirt. (2) Specify the concerns name and Rego query. (3) Specify the concerns name and flag parameter values. (4) Allowed values are Critical, Warning, and Information. Stop the Validation pod by scaling the forklift-controller deployment to 0: $ kubectl scale -n konveyor-forklift --replicas=0 deployment/forklift-controller Start the Validation pod by scaling the forklift-controller deployment to 1: $ kubectl scale -n konveyor-forklift --replicas=1 deployment/forklift-controller Check the Validation pod log to verify that the pod started: $ kubectl logs -f \u0026lt;validation_pod\u0026gt; If the custom rule conflicts with a default rule, the Validation pod will not start.\nRemove the source provider: $ kubectl delete provider \u0026lt;provider\u0026gt; -n konveyor-forklift Add the source provider to apply the new rule: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Provider metadata: name: \u0026lt;provider\u0026gt; namespace: konveyor-forklift spec: type: \u0026lt;provider_type\u0026gt; (1) url: \u0026lt;api_end_point\u0026gt; (2) secret: name: \u0026lt;secret\u0026gt; (3) namespace: konveyor-forklift EOF The explanations below refer to the callouts in the sample code above.\n(1) Allowed values are ovirt and vsphere. (2) Specify the API end point URL, for example, https://\u0026lt;vCenter_host\u0026gt;/sdk for vSphere or https://\u0026lt;{rhv-short}_engine_host\u0026gt;/ovirt-engine/api/ for oVirt. (3) Specify the name of the provider Secret CR. Important: Update the rules version after creating a custom rule so that the Inventory service detects the changes and validates the VMs.\nUpdating the inventory rules version Follow the steps below to update the inventory rules version each time the rules are updated so that the Provider Inventory service detects the changes and triggers the Validation service.\nThe rules version is recorded in a rules_version.rego file for each provider.\nProcedure\nRetrieve the current rules version: $ GET https://forklift-validation/v1/data/io/konveyor/forklift/\u0026lt;provider\u0026gt;/rules_version (1) Example output:\n{ \u0026#34;result\u0026#34;: { \u0026#34;rules_version\u0026#34;: 5 } } Connect to the terminal of the Validation pod: $ kubectl rsh \u0026lt;validation_pod\u0026gt; Update the rules version in the /usr/share/opa/policies/io/konveyor/forklift//rules_version.rego file.\nLog out of the Validation pod terminal.\nVerify the updated rules version:\n$ GET https://forklift-validation/v1/data/io/konveyor/forklift/\u0026lt;provider\u0026gt;/rules_version (1) Example output { \u0026#34;result\u0026#34;: { \u0026#34;rules_version\u0026#34;: 6 } } Source\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/","title":"Migrating virtual machines","tags":[],"description":"","content":"Use this section to configure, perform, and upgrade VM migration and cancel it.\n"},{"uri":"http://konveyor.github.io/forklift/installingforklift/","title":"Installing Forklift","tags":[],"description":"","content":"Use this section to set up the environment, install Forklift, upgrade, and uninstall.\n"},{"uri":"http://konveyor.github.io/forklift/installingforklift/upgrade/","title":"Upgrading Forklift","tags":[],"description":"","content":"Follow the steps below to upgrade the Forklift Operator using the OKD web console.\nImportant: Do not skip a release when upgrading. For example, upgrade 2.0 to 2.1 and then 2.1 to 2.2.\nIf upgrading from 2.2 to 2.3, see the ### Forklift 2.2 to 2.3 Upgrade Notes section after the procedure\nProcedure\nOpen the OKD web console. Click Operators, Installed Operators, Migration Tookit for Virtualization Operator, then Subscription. Change the update channel to the correct release. See Changing update channel for an Operator in the OKD documentation.\nConfirm that the Upgrade status changes from Up to date to Upgrade available. If it does not, restart the CatalogSource pod:\na. Note the catalog source, for example, redhat-operators. b. Open the command line. c. Retrieve the catalog source pod:\n$ kubectl get pod -n openshift-marketplace | grep \u0026lt;catalog_source\u0026gt; d. Delete the pod: $ kubectl delete pod -n openshift-marketplace \u0026lt;catalog_source_pod\u0026gt; The Upgrade status changes from Up to date to Upgrade available.\nNote: Update approval settings on the Subscriptions tab:\nAutomatic: Starts upgrades automatically. Manual Forces approval to start the upgrade. See Manually approving a pending Operator upgrade in the OKD documentation. Verify that the forklift-ui pod is in a Ready state before logging into the web console: $ kubectl get pods -n konveyor-forklift Example output:\nNAME READY STATUS RESTARTS AGE forklift-controller-788bdb4c69-mw268 2/2 Running 0 2m forklift-operator-6bf45b8d8-qps9v 1/1 Running 0 5m forklift-ui-7cdf96d8f6-xnw5n 1/1 Running 0 2m Forklift 2.2 to 2.3 Upgrade Notes VMware source providers in Forklift If VMware source providers were added to 2.2, upgrading to 2.3 changes the state of any VMware providers to Critical.\nFix: Edit the VMware provider by adding a VDDK init image and verifying the certificate of the VMware provider. For more information see Addding a VMSphere source provider.\nNFS Mapping If the configuration is mapped to NFS on the OKD destination provider in Forklift 2.2, upgrading to Forklift 2.3 invalidates the NFS mapping.\nFix: Edit the AccessModes and VolumeMode parameters in the NFS storage profile. For more information, see Customizing the storage profile.\nSource\n"},{"uri":"http://konveyor.github.io/forklift/installingforklift/uninstall/","title":"Uninstalling Forklift","tags":[],"description":"","content":"Uninstall Forklift using the OKD web console or the command line interface (CLI).\nUninstalling Forklift with the OKD web console Follow the steps below to uninstall Forklift using the OKD web console to delete the konveyor-forklift project and custom resource definitions (CRDs).\nPrerequisites\nLog in as a user with cluster-admin privileges. Procedure\nClick Home and then Projects. Locate the konveyor-forklift project. Select Delete Project from the Options menu kebab on the right side of the project. Navigate to the Delete Project pane, enter the project name, and then click Delete. Click Administration and then CustomResourceDefinitions. Type forklift in the Search field to locate the CRDs in the forklift.konveyor.io group. Select Delete CustomResourceDefinition from the Options menu kebab in the right side of each CRD. Uninstalling Forklift with the command line interface Follow the steps below to uninstall Forklift using the command line interface (CLI) by deleting the konveyor-forklift project and the forklift.konveyor.io custom resource definitions (CRDs).\nPrerequisites\nLog in as a user with cluster-admin privileges. Procedure\nDelete the project: $ kubectl delete project konveyor-forklift Delete the CRDs: $ kubectl get crd -o name | grep \u0026#39;forklift\u0026#39; | xargs kubectl delete Delete the OAuthClient: $ kubectl delete oauthclient/forklift-ui Source\n"},{"uri":"http://konveyor.github.io/forklift/installingforklift/installation/","title":"Installing the Forklift Operator","tags":[],"description":"","content":"The Forklift Operator can be installed using the OKD web console or the command line interface (CLI).\nInstalling the Forklift Operator using the OKD web console Follow the steps below to install the Forklift Operator by using the OKD web console.\nPrerequisites\nOKD 4.10 installed. KubeVirt Operator installed. Procedure\nLog in with cluster-admin permissions. OPen the OKD web console, click Operators, then OperatorHub. Use the Filter by keyword field to search for forklift-operator. Note: The Forklift Operator is a Community Operator. Red Hat does not support Community Operators.\nClick Migration Tookit for Virtualization Operator and then click **Install. Click Install on the Install Operator page. Click Operators then Installed Operators to verify that Migration Tookit for Virtualization Operator appears in the konveyor-forklift project with the status Succeeded. Click Migration Tookit for Virtualization Operator. Locate the ForkliftController, and click Create Instance under Provided APIs. Click Create. Click Workloads, then Pods to verify that the Forklift pods are running. Log in to the OKD web console. Click Networking then Routes. Select the konveyor-forklift project in the Project: list. The URL for the forklift-ui service that opens the login page for the Forklift web console is displayed.\nClick the URL to navigate to the Forklift web console. Installing the Forklift Operator from the command line interface Follow the steps below to install the Forklift Operator from the command line interface (CLI).\nPrerequisites\nOKD 4.10 installed. KubeVirt Operator installed. Procedure\nLog in with cluster-admin permissions. Create the konveyor-forklift project: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: project.openshift.io/v1 kind: Project metadata: name: konveyor-forklift EOF Create an OperatorGroup CR called migration. $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: migration namespace: konveyor-forklift spec: targetNamespaces: - konveyor-forklift EOF Create a Subscription CR for the Operator: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: forklift-operator namespace: konveyor-forklift spec: channel: development installPlanApproval: Automatic name: forklift-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: \u0026#34;konveyor-forklift-operator.2.3.0\u0026#34; EOF Create a ForkliftController CR: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: ForkliftController metadata: name: forklift-controller namespace: konveyor-forklift spec: olm_managed: true EOF Verify that the Forklift pods are running: $ kubectl get pods -n konveyor-forklift Example output:\nNAME READY STATUS RESTARTS AGE forklift-controller-788bdb4c69-mw268 2/2 Running 0 2m forklift-operator-6bf45b8d8-qps9v 1/1 Running 0 5m forklift-ui-7cdf96d8f6-xnw5n 1/1 Running 0 2m Get the Forklift web console URL with the following command: $ kubectl get route virt -n konveyor-forklift \\ -o custom-columns=:.spec.host The URL for the forklift-ui service that opens the login page for the Forklift web console is displayed.\nExample output:\nhttps://virt-konveyor-forklift.apps.cluster.openshift.com. Getting the Forklift web console URL Using OKD Follow the steps below to retrieve the Forklift web console URL at any time using the OKD web console.\nPrerequisites\nKubeVirt Operator installed. Forklift Operator installed. Procedure\nLog in with cluster-admin privileges. Log in to the OKD web console. Click Networking then Routes. Select the konveyor-forklift project in the Project: list. The URL for the forklift-ui service that opens the login page for the Forklift web console is displayed.\nClick the URL to navigate to the Forklift web console. Getting the Forklift web console URL Using CLI Follow the steps below to retrieve the Forklift web console URL at any time using the command line.\nEnter the following command to get the Forklift web console URL: $ kubectl get route virt -n konveyor-forklift \\ -o custom-columns=:.spec.host The URL for the forklift-ui service that opens the login page for the Forklift web console is displayed.\nExample output:\nhttps://virt-konveyor-forklift.apps.cluster.openshift.com. Launch a browser and navigate to the Forklift web console. Source\n"},{"uri":"http://konveyor.github.io/forklift/installingforklift/prereqs/","title":"Prerequisites","tags":[],"description":"","content":"The following prerequisites ensure the environment is prepared for migration.\nSoftware compatibility guidelines Install compatible software versions with Forklift using the table below.\nForklift OKD KubeVirt VMware vSphere oVirt 2.3 4.10 4.10 6.5 or later 4.4.9 or later Storage support and default modes Forklift uses the following default volume and access modes for supported storage.\nNote: The following settings must be applied if the KubeVirt storage does not support dynamic provisioning:\nFilesystem volume mode: Slower than Block volume mode. ReadWriteOnce access mode: Does not support live virtual machine migration. See Enabling a statically-provisioned storage class for details on editing the storage profile.\nDefault volume and access modes.\nProvisioner Volume mode Access mode kubernetes.io/aws-ebs Block ReadWriteOnce kubernetes.io/azure-disk Block ReadWriteOnce kubernetes.io/azure-file Filesystem ReadWriteMany kubernetes.io/cinder Block ReadWriteOnce kubernetes.io/gce-pd Block ReadWriteOnce kubernetes.io/hostpath-provisioner Filesystem ReadWriteOnce manila.csi.openstack.org Filesystem ReadWriteMany openshift-storage.cephfs.csi.ceph.com Filesystem ReadWriteMany openshift-storage.rbd.csi.ceph.com Block ReadWriteOnce kubernetes.io/rbd Block ReadWriteOnce kubernetes.io/vsphere-volume Block ReadWriteOnce Network prerequisites The following prerequisites apply to all migrations:\nIP addresses, VLANs, and other network configuration settings must not be changed before or after migration. The MAC addresses of the virtual machines are preserved during migration. The network connections between the source environment, the KubeVirt cluster, and the replication repository must be reliable and uninterrupted. A network attachment definition for each additional destination network must be created if mapping more than one source and destination network. Ports Use the following port parameters for migration.\nVMware VSphere The firewalls must enable traffic over the following required network ports for migrating from VMware vSphere.\nPort Protocol Source Destination Purpose 443 TCP OpenShift nodes VMware vCenter VMware provider inventory \u0026amp; Disk transfer authentication 443 TCP OpenShift nodes VMware ESXi hosts Disk transfer authentication 902 TCP OpenShift nodes VMware ESXi hosts Disk transfer data copy oVirt The firewalls must enable traffic over the following required network ports for migrating from oVirt.\nPort Protocol Source Destination Purpose 443 TCP OpenShift nodes oVirt Engine oVirt provider inventory 443 TCP OpenShift nodes oVirt hosts Disk transfer authentication 54322 TCP OpenShift nodes oVirt hosts Disk transfer data copy Source virtual machine prerequisites The following prerequisites apply to all migrations:\nISO/CDROM disks must be unmounted. Each NIC must contain one IPv4 and/or one IPv6 address. The VM name must contain only lowercase letters (a-z), numbers (0-9), or hyphens (-), up to a maximum of 253 characters. The first and last characters must be alphanumeric. The name must not contain uppercase letters, spaces, periods (.), or special characters. The VM name must not duplicate the name of a VM in the KubeVirt environment. The VM operating system must be certified and supported for use as a guest operating system with KubeVirt and for conversion to KVM with virt-v2v. oVirt prerequisites The following prerequisites apply to oVirt migrations:\nMust have the CA certificate of the engine. Able to obtain the CA certificate by navigating to: https://\u0026lt;{rhv-short}_engine_host\u0026gt;/ovirt-engine/services/pki-resource?resource=ca-certificate\u0026amp;format=X509-PEM-CA VMware prerequisites The following prerequisites apply to VMware migrations:\nInstall VMware Tools must be installon all source virtual machines (VMs). If running a warm migration, changed block tracking (CBT) on the VMs and on the VM disks must be enabled. Create a VMware Virtual Disk Development Kit (VDDK) image. Obtain the SHA-1 fingerprint of the vCenter host. The NFC service memory of the host must be increased if migrating more than 10 VMs from an ESXi host in the same migration plan. Creating a VDDK image Forklift uses the VMware Virtual Disk Development Kit (VDDK) SDK to transfer virtual disks from VMware vSphere. Follow the steps below to create the VDDK image.\nNote: The VDDK init image path is required to add a VMware source provider:\nDownload the VMware Virtual Disk Development Kit (VDDK) uild a VDDK image. Push the VDDK image to the image registry. Important: Storing the VDDK image in a public registry might violate the VMware license terms.\nPrerequisites\nOKD image registry. podman installed. KubeVirt must be able to access an external registry if used. Procedure\nCreate and navigate to a temporary directory: Example output: $ mkdir /tmp/\u0026lt;dir_name\u0026gt; \u0026amp;\u0026amp; cd /tmp/\u0026lt;dir_name\u0026gt; Open a browser and navigate to the VMware VDDK download page. Select the latest VDDK version and click Download. Save the VDDK archive file in the temporary directory. Extract the VDDK archive: $ tar -xzf VMware-vix-disklib-\u0026lt;version\u0026gt;.x86_64.tar.gz Create a Dockerfile: $ cat \u0026gt; Dockerfile \u0026lt;\u0026lt;EOF FROM registry.access.redhat.com/ubi8/ubi-minimal COPY vmware-vix-disklib-distrib /vmware-vix-disklib-distrib RUN mkdir -p /opt ENTRYPOINT [\u0026#34;cp\u0026#34;, \u0026#34;-r\u0026#34;, \u0026#34;/vmware-vix-disklib-distrib\u0026#34;, \u0026#34;/opt\u0026#34;] EOF Build the VDDK image: $ podman build . -t \u0026lt;registry_route_or_server_path\u0026gt;/vddk:\u0026lt;tag\u0026gt; Push the VDDK image to the registry: $ podman push \u0026lt;registry_route_or_server_path\u0026gt;/vddk:\u0026lt;tag\u0026gt; Ensure that the image is accessible to the KubeVirt environment. Obtaining the SHA-1 fingerprint of a vCenter host Follow the steps below to obtain the SHA-1 fingerprint of a vCenter host in order to create a Secret CR.\nProcedure\nRun the following command: $ openssl s_client \\ -connect \u0026lt;vcenter_host\u0026gt;:443 \\ (1) \u0026lt; /dev/null 2\u0026gt;/dev/null \\ | openssl x509 -fingerprint -noout -in /dev/stdin \\ | cut -d \u0026#39;=\u0026#39; -f 2 1\tSpecify the IP address or FQDN of the vCenter host. Example output:\n01:23:45:67:89:AB:CD:EF:01:23:45:67:89:AB:CD:EF:01:23:45:67 Increasing the NFC service memory of an ESXi host Note: If more than 10 VMs are migrating from an ESXi host in the same migration plan, increase the NFC service memory of the host.\nThe migration will fail because the NFC service memory is limited to 10 parallel connections.\nProcedure\nLog in to the ESXi host as root. Change the value of maxMemory to 1000000000 in /etc/vmware/hostd/config.xml: ... \u0026lt;nfcsvc\u0026gt; \u0026lt;path\u0026gt;libnfcsvc.so\u0026lt;/path\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;maxMemory\u0026gt;1000000000\u0026lt;/maxMemory\u0026gt; \u0026lt;maxStreamMemory\u0026gt;10485760\u0026lt;/maxStreamMemory\u0026gt; \u0026lt;/nfcsvc\u0026gt; ... Restart hostd: # /etc/init.d/hostd restart The host does not need to be restarted.\nSource\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step1export/","title":"Step One: Export Resources","tags":[],"description":"","content":"The first step of the cluster migration process is exporting resources from a source cluster of any namespace to be input for the subsequent commands.\nAll of the following export commands will output the contents of the foo namespace into a local export directory with the context demo defined in KUBECONFIG.\ncrane export -n foo -e export --context demo cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; conf.yaml namespace: foo export-dir: export context: demo EOF crane export -c conf.yaml cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; conf.yaml namespace: foo export-dir: export context: testing EOF crane export -c conf.yaml --context demo # Note the difference is we are overriding \u0026#34;context\u0026#34; here with flag Note: There are multiple ways to input a command, precedence of which is input from flags \u0026gt; input from config file \u0026gt; env variables \u0026gt; default values (not all the flags can have a corresponding env variable). This behavior persists across all Crane CLI commands.\nSource\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step2transform/","title":"Step Two: Transform Exports","tags":[],"description":"","content":"The Transform command facilitates the changes to the exported resources that are frequently necessary when migrating workloads between one environment to another. For example:\nStripping the resource status information that is no longer relevant after the resource is serialized out of a cluster. Adjusting resource quotas to fit the destination environment. Altering node selectors to match the new environment if the node labels do not match the source environment. Applying custom labels or annotations to resources during the migration. Because most changes are specific to an environment, Crane is designed for total customization and the transform command accepts a plugins directory argument.\nEach plugin is an executable with a well defined stdin/out interface allowing for customization or installation and use of published generic plugins.\nAfter exporting the resources into a local directory and installing the desired transformation plugins, the crane transform command can run. The output is placed in a directory with a set of transform files that describe the changes that need to be applied to the original resources before their final import. The changes are written in the JSONPatch format, are human readable, and easily hackable.\nThis command generates a patch to add an annotation transform-test:test for objects in the export directory and the transform directory to be used as input for apply command.\ncrane transform -e export -p plugins -t transform --optional-flags=\u0026#34;add-annotations=transform-test:test\u0026#34; Run the following to see the list of available optional commands for configured plugins.\ncrane transform optionals See Managing PlugIns for more information on plugins that can be consumed by the transform command.\nSource\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step3apply/","title":"Step Three: Apply Patches","tags":[],"description":"","content":"The final step of the cluster migration process is to apply all the patches generated by the Transform command to exported resources.\ncrane apply -e export -t transform -o output Apply the patches in the transform directory to the resources in the export directory and save the modified resource files in the output directory.\nAfter applying the patches, the resources located in output directory can either be deployed to the destination cluster using kubectl apply, or they can be pushed to a repository and then applied with the help of the GitOps pipeline. An example of the later scenario can be found here.\nSource\n"},{"uri":"http://konveyor.github.io/crane/tutorials/","title":"Tutorials","tags":[],"description":"","content":"Use this section to follow two examples of how to migrate Kubernetes workloads and their state between clusters, remove environment-specific configuration, and automate application deployments along the way.\n"},{"uri":"http://konveyor.github.io/crane/tools/","title":"Tools","tags":[],"description":"","content":"Use this section to better understand the Crane tools.\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/","title":"Using Crane","tags":[],"description":"","content":"Use this section to complete the three steps of the Crane migration process.\nStep 1: Exporting resources Step 2: Transforming exports Step 3: Applying patches "},{"uri":"http://konveyor.github.io/crane/tools/gitopsintegration/","title":"Integrating GitOps","tags":[],"description":"","content":"All Crane commands are individual utilities, but when used together in sequence, they form a pipeline.\nCrane makes it easy to integrate a gitops that applies the patches/resources generated at the end of the apply command on the destination cluster. The resources generated at the end of the process (i.e export, transform, apply) can be pushed to a github repository, and a pipeline can be created to deploy the resources on a cluster on every push.\nSource\n"},{"uri":"http://konveyor.github.io/crane/tools/customplugins/","title":"Developing custom plugins","tags":[],"description":"","content":"This document covers how to write a plugin binary using crane-lib. It requires:\nProcedure\nGo to the development environment setup. (Optionally, an overview of the crane toolkit.)\nCreate binary plugin for crane-lib as a simple Go program in the following format that will:\nRead an input from stdin. Call the Run function with the input object passed as unstructured. Print the return value of Run function on stdout. package main import ( \u0026#34;fmt\u0026#34; jsonpatch \u0026#34;github.com/evanphx/json-patch\u0026#34; \u0026#34;github.com/konveyor/crane-lib/transform\u0026#34; \u0026#34;github.com/konveyor/crane-lib/transform/cli\u0026#34; ) func main() { fields := []transform.OptionalFields{ { FlagName: \u0026#34;my-flag\u0026#34;, Help: \u0026#34;What the flag does\u0026#34;, Example: \u0026#34;true\u0026#34;, }, } cli.RunAndExit(cli.NewCustomPlugin(\u0026#34;MyCustomPlugin\u0026#34;, \u0026#34;v1\u0026#34;, fields, Run)) } func Run(request transform.PluginRequest) (transform.PluginResponse, error) { // plugin writers need to write custom code here. resp := transform.PluginResponse{ Version: string(transform.V1), } // prepare the response return resp, nil } The json is passed in using stdin is a transform.PluginRequest which consists of an inline unstructured object and an optional Extras map containing additional flags. Without any Extras the format is identical to the json output from a kubectl get -o json call.\nWhen adding extra parameters, a map field extras is added at the top level (parallel to apiVersion, kind, etc.).\nVersion the plugin development output by passing in the JSOC object on stdin manually during development. For example, if the code above is compiled and run with the following json as input, the output will be {\u0026quot;version\u0026quot;: \u0026quot;v1\u0026quot;}.\n./my-plugin { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Route\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;openshift.io/host.generated\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34;, \u0026#34;managedFields\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:spec\u0026#34;: { \u0026#34;f:path\u0026#34;: {}, \u0026#34;f:to\u0026#34;: { \u0026#34;f:kind\u0026#34;: {}, \u0026#34;f:name\u0026#34;: {}, \u0026#34;f:weight\u0026#34;: {} }, \u0026#34;f:wildcardPolicy\u0026#34;: {} } }, \u0026#34;manager\u0026#34;: \u0026#34;oc\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34; }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:status\u0026#34;: { \u0026#34;f:ingress\u0026#34;: {} } }, \u0026#34;manager\u0026#34;: \u0026#34;openshift-router\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;mssql-app-route\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;mssql-persistent\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;155816271\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/route.openshift.io/v1/namespaces/mssql-persistent/routes/mssql-app-route\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;42dca205-31bf-463d-b516-f84064523c2c\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;mssql-app-route-mssql-persistent.apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;to\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mssql-app-service\u0026#34;, \u0026#34;weight\u0026#34;: 100 }, \u0026#34;wildcardPolicy\u0026#34;: \u0026#34;None\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;ingress\u0026#34;: [ { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Admitted\u0026#34; } ], \u0026#34;host\u0026#34;: \u0026#34;mssql-app-route-mssql-persistent.apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;routerCanonicalHostname\u0026#34;: \u0026#34;apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;routerName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;wildcardPolicy\u0026#34;: \u0026#34;None\u0026#34; } ] } } When the plugin is ready to be tested, put it in a directory and run with the crane cli command.\nMore accurate detail can be found [here] (https://github.com/konveyor/crane-lib/blob/main/transform/binary-plugin/README.md).\nSource\n"},{"uri":"http://konveyor.github.io/crane/tools/tunnelapi/","title":"Tunnel API","tags":[],"description":"","content":"The tunnel-api sub-command can be used to access an on-premise cluster from a cloud cluster to allow orchestrating migrations from on-premise clusters using MTC where access is not possible otherwise.\nAn openvpn client on the on-premise cluster will connect to a server running on the cloud cluster and the openvpn server is exposed to the client using a load balancer address on the cloud cluster.\nA service created on the cloud cluster is used to expose the on-premise clusters API to MTC running on the cloud cluster.\nRequirements The system used to create the VPN tunnel must have access and be logged in to both clusters. It must be possible to create a load balancer on the cloud cluster. An available namespace on each cluster to run the tunnel in not created in advance. Note: To connect multiple on-premise source clusters to the cloud cluster use a separate namespace for each.\napi-tunnel options namespace: The namespace used to launch the VPN tunnel in, defaults to openvpn destination-context: The cloud destination cluster context where the openvpn server will be launched. destination-image: The container image to use on the destination cluster. (Default: quay.io/konveyor/openvpn:latest) source-context: The on-premise source cluster context where the openvpn client will be launched. source-image: The container image to use on the source cluster. (Default: quay.io/konveyor/openvpn:latest) proxy-host: The hostname of an http-proxy to use on the source cluster for connecting to the destination cluster. proxy-pass: The password for the http-proxy. If specified, also specify a username or it will be ignored. proxy-port: The port the http-proxy is listening on. If none is specified it will default to 3128 proxy-user: The username for the http-proxy. If specified, also specify a password or it will be ignored. Example\ncrane tunnel-api --namespace openvpn-311 \\ --destination-context openshift-migration/c131-e-us-east-containers-cloud-ibm-com/admin \\ --source-context default/192-168-122-171-nip-io:8443/admin \\ --source-image: my.registry.server:5000/konveyor/openvpn:latest \\ --proxy-host my.proxy.server \\ --proxy-port 3128 \\ --proxy-user foo \\ --proxy-pass bar MTC Configuration When configuring the source cluster in MTC the API URL takes the form of https://proxied-cluster.${namespace}.svc.cluster.local:8443.\nOptional: Set the image registry for direct image migrations to proxied-cluster.${namespace}.svc.cluster.local:5000.\nReplace ${namespace} with either openvpn or the specified namespace when running the command to set up the tunnel.\nDemo https://youtu.be/wrPVcZ4bP1M\nTroubleshooting It may take 3 to 5 minutes after the setup to complete for the load balancer address to become resolvable. During this time the client will be unable to connect and establish a connection and the tunnel will not function.\nDuring this time, run oc get pods in the specified namespace for setup, and monitor the logs of the openvpn container to see the connection establish.\nExample\noc logs -f -n openvpn-311 openvpn-7b66f65d48-79dbs -c openvpn Source\n"},{"uri":"http://konveyor.github.io/crane/tools/pluginmanager/","title":"Plugin Manager","tags":[],"description":"","content":"The Plugin Manager is an optional utility that assists in adding plugins to the appropriate location to be consumed by the transform command.\nList Plugin utility The List Plugin utility discovers available plugins that that are compatible with the current OS and architecture.\ncrane plugin-manager list Listing from the repo default +-----------------+------------------+-------------------+ | NAME | SHORTDESCRIPTION | AVAILABLEVERSIONS | +-----------------+------------------+-------------------+ | OpenshiftPlugin | OpenshiftPlugin | v0.0.1 | +-----------------+------------------+-------------------+ Other valid execution examples This command lists all installed plugins managed by plugin-manager.\ncrane plugin-manager --installed -p plugin-dir This command lists all version of the foo plugin with detailed information.\ncrane plugin-manager --params -n foo Add Plugin utility The Add Plugin utility places the plugin into a directory to be consumed by Transform command.\nThis command downloads the binary of the foo version 0.0.1 plugin from the appropriate source and places it in the plugin-dir/managed directory (the default is plugins).\ncrane plugin-manager add foo --version 0.0.1 -p plugin-dir Remove Plugin utility The Remove Plugin utility removes unwanted plugins from being consumed by the Transform command. This command removes the foo plugin from the plugin-dir/managed dir.\ncrane plugin-manager remove foo -p plugin-dir Note: The plugin-manager command operates in the \u0026lt;plugin-dir\u0026gt;/managed directory. Whenever the flag -p, plugin-dir is used with plugin-manager, the utility operates in the managed places folder in \u0026lt;plugin-dir\u0026gt;.\nFor example: plugin-manager add places the plugin binary within \u0026lt;plugin-dir\u0026gt;/managed, plugin-manager removes the binary from \u0026lt;plugin-dir\u0026gt;/managed, and plugin-manager list --installed uses the path \u0026lt;plugin-dir\u0026gt;/managed to list installed plugins.\nManual plugin management Currently only two plugins are available with more plugins available soon.\nAvailable plugins: -Kubernetes (build into crane-lib) -OpenShift.\nThese plugins can be added to the desired plugin directory. (The default directory is plugin where crane is installed.)\nImportant: The Kubernetes plugin is built into the crane-lib and is not to be added manually or otherwise.\nTo install the plugins:\nDownload the binary of the plugin from the release and place it in the plugin directory.\ncurl -sL https://api.github.com/repos/konveyor/crane-plugin-\u0026lt;plugin-name\u0026gt;/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;\u0026lt;arch\u0026gt;-\u0026lt;os\u0026gt;\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- chmod +x \u0026lt;binary\u0026gt; cp \u0026lt;binary\u0026gt; /bin/usr/crane/plugins/\u0026lt;plugin-name\u0026gt; Build the binary locally and place it in theplugin directory.\ncd $GOPATH git clone https://github.com/konveyor/crane-plugin-\u0026lt;plugin-name\u0026gt;.git cd crane-plugin-\u0026lt;plugin-name\u0026gt; go build -f \u0026lt;plugin-name\u0026gt; . cp \u0026lt;plugin\u0026gt; /bin/usr/crane/plugins/\u0026lt;plugin-name\u0026gt; Note: Adding plugins available in the plugin repo manually is not advisable as long as it can be added usingplugin-manager. For custom plugins or testing plugins under development, manual management is necessary.\nSource\n"},{"uri":"http://konveyor.github.io/crane/tutorials/statelessappmirror/","title":"Stateless application mirror","tags":[],"description":"","content":"This tutorial is an example of how to mirror a simple, stateless PHP Guestbook application using Crane.\nRoadmap\n1. Deploy the Guestbook application in the source cluster. 2. Extract resources from the source cluster using Crane Export. 3. Transform resources to prepare manifests for the destination cluster using Crane Transform. 4. Apply the transformations using Crane Apply. Apply application manifests to the destination cluster. Prerequisites\nCreate a source and destination Kubernetes cluster environment in minikube or Kind: minikube\nwget \u0026#34;https://raw.githubusercontent.com/konveyor/crane/main/hack/minikube-clusters-start.sh\u0026#34; chmod +x minikube-clusters-start.sh./minikube-clusters-start.sh Kind\nwget \u0026#34;https://raw.githubusercontent.com/konveyor/crane-runner/main/hack/kind-up.sh\u0026#34; chmod +x kind-up.sh./kind-up.sh Install Crane using the Installation Guide. Important: Read through Kubernetes\u0026rsquo; documentation on accessing multiple clusters. This document references src and dest contexts that refer to the clusters created using the minikube startup scripts above.\nWhen working in the home environment, or use kind (kind-src and kind-dest), modify the commands below to reference the correct cluster context.\n1. Deploy the Guestbook application in the source cluster Deploy the Kubernetes\u0026rsquo; stateless guestbook application and modify it to be consumable with Kustomize (Kubernetes native and template-free tool to manage application configuration). The guestbook application consists of:\nredis leader deployment and service redis follower deployment and service guestbook front-end deployment and service kubectl --context src create namespace guestbook kubectl --context src --namespace guestbook apply -k github.com/konveyor/crane-runner/examples/resources/guestbook kubectl --context src --namespace guestbook wait --for=condition=ready pod --selector=app=guestbook --timeout=180s Optional\nForward localhost traffic to the frontend of the Guestbook application to access Guestbook from a browser using localhost:8080:\nkubectl --context src --namespace guestbook port-forward svc/frontend 8080:80 2. Extract from the source cluster Cranes export command extracts all of the specified resources from the source cluster.\ncrane export --context src --namespace guestbook Check the export directory to verify it is working correctly. The directory should look similar to the example below:\n$ tree -a export export  failures   guestbook  resources  guestbook  ConfigMap_guestbook_kube-root-ca.crt.yaml  Deployment_guestbook_frontend.yaml  Deployment_guestbook_redis-master.yaml  Deployment_guestbook_redis-slave.yaml  Endpoints_guestbook_frontend.yaml  Endpoints_guestbook_redis-master.yaml  Endpoints_guestbook_redis-slave.yaml  EndpointSlice_guestbook_frontend-bkqbs.yaml  EndpointSlice_guestbook_redis-master-hxr5k.yaml  EndpointSlice_guestbook_redis-slave-8wt7z.yaml  Pod_guestbook_frontend-5fd859dcf6-5nvbm.yaml  Pod_guestbook_frontend-5fd859dcf6-j8w94.yaml  Pod_guestbook_frontend-5fd859dcf6-s9x8p.yaml  Pod_guestbook_redis-master-55d9747c6c-6f9bz.yaml  Pod_guestbook_redis-slave-5c6b4c5b47-jnrsr.yaml  Pod_guestbook_redis-slave-5c6b4c5b47-xz776.yaml  ReplicaSet_guestbook_frontend-5fd859dcf6.yaml  ReplicaSet_guestbook_redis-master-55d9747c6c.yaml  ReplicaSet_guestbook_redis-slave-5c6b4c5b47.yaml  Secret_guestbook_default-token-5vsrb.yaml  ServiceAccount_guestbook_default.yaml  Service_guestbook_frontend.yaml  Service_guestbook_redis-master.yaml  Service_guestbook_redis-slave.yaml 4 directories, 24 files Crane Export is using a discovery client to see all of the API resources in the specified namespace of the designated cluster and outputing them to the disk in YAML form. This allows workloads to migrate in a non-destructive way.\nGoing forward these manifests will be working on the disk without impacting the active resources in the source cluster.\n3. Generate Transformations Cranes transform command generates tranformations in the form of JSON patches and stores them on the disk in the transform directory (unless overridden using --transform-dir).\ncrane transform Check the transform directory to verify the command worked correctly:\n$ tree -a transform The directory should look similar to the example below:\ntransform  resources  guestbook  transform-ConfigMap_guestbook_kube-root-ca.crt.yaml  transform-Deployment_guestbook_frontend.yaml  transform-Deployment_guestbook_redis-master.yaml  transform-Deployment_guestbook_redis-slave.yaml  transform-Secret_guestbook_default-token-5vsrb.yaml  transform-ServiceAccount_guestbook_default.yaml  transform-Service_guestbook_frontend.yaml  transform-Service_guestbook_redis-master.yaml  transform-Service_guestbook_redis-slave.yaml  .wh.Endpoints_guestbook_frontend.yaml  .wh.Endpoints_guestbook_redis-master.yaml  .wh.Endpoints_guestbook_redis-slave.yaml  .wh.EndpointSlice_guestbook_frontend-bkqbs.yaml  .wh.EndpointSlice_guestbook_redis-master-hxr5k.yaml  .wh.EndpointSlice_guestbook_redis-slave-8wt7z.yaml  .wh.Pod_guestbook_frontend-5fd859dcf6-5nvbm.yaml  .wh.Pod_guestbook_frontend-5fd859dcf6-j8w94.yaml  .wh.Pod_guestbook_frontend-5fd859dcf6-s9x8p.yaml  .wh.Pod_guestbook_redis-master-55d9747c6c-6f9bz.yaml  .wh.Pod_guestbook_redis-slave-5c6b4c5b47-jnrsr.yaml  .wh.Pod_guestbook_redis-slave-5c6b4c5b47-xz776.yaml  .wh.ReplicaSet_guestbook_frontend-5fd859dcf6.yaml  .wh.ReplicaSet_guestbook_redis-master-55d9747c6c.yaml  .wh.ReplicaSet_guestbook_redis-slave-5c6b4c5b47.yaml 2 directories, 24 files Crane Transform is iterating through the configured plugins and running them against the exported resources from the previous step. View which plugins are configured with Crane Transform list-plugins and optional arguments to those plugins with crane transform optionals.\nExplore what plugins can be configured with the Crane plugin-manager list, install one, and customize the exported resources:\ncrane transform --optional-flags=\u0026#34;add-annotations=custom-crane-annotation=foo\u0026#34; If the flags get difficult to manage via the command-line, specify a --flags-file similar to the example below::\ndebug: false export-dir: myExport transform-dir: myTransform output-dir: myOutput optional-flags: add-annotations: custom-crane-annotation: \u0026#34;foo\u0026#34; 4. Apply Transformations The Crane Apply command takes the exported resources and transformations and renders the results as YAML files that can be applied to another cluster.\ncrane apply Look at one of the transformations created in the last step to better understand the Apply command.\n$ cat transform/resources/guestbook/transform-Deployment_guestbook_frontend.yaml [{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/uid\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/resourceVersion\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/creationTimestamp\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/generation\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/status\u0026#34;}]% When crane applies the transformations for the Guestbook frontend it executes a handful of JSON patches that:\nRemove the UID Remove the resourceVersion Remove the creationTimestamp Remove the generation field Remove the status The leftover data from the source cluster is removed from the final manifests to make them applicable to the destination cluster.\nThe resources are effectively cluster agnostic and ready to be kubectl applied to the chosen cluster or placed under version control to be later managed by GitOps and CI/CD pipelines.\nNote: Additional patches to add/remove/replace additional fields on the resources previously exported are available if optional flags are specified..\nApply the manifests to the destination cluster Apply the manifests prepared for the destination cluster using kubectl directly:\nkubectl --context dest create namespace guestbook kubectl --context dest --namespace guestbook --recursive=true apply -f ./output Note: To change the namespace, use Kustomize.\ncd output/resources/guestbook kustomize init --namespace custom-guestbook --autodetect The result is a kustomization.yaml like the example below:\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ConfigMap_guestbook_kube-root-ca.crt.yaml - Deployment_guestbook_frontend.yaml - Deployment_guestbook_redis-master.yaml - Deployment_guestbook_redis-slave.yaml - Secret_guestbook_default-token-5vsrb.yaml - ServiceAccount_guestbook_default.yaml - Service_guestbook_frontend.yaml - Service_guestbook_redis-master.yaml - Service_guestbook_redis-slave.yaml namespace: custom-guestbook After creating the custom-guestbook namespace, apply the kustomization.yaml with kubectl apply -k.\nNext Steps Read more about Crane. Check out Crane Runner to perform application migrations inside Kubernetes. Cleanup kubectl --context dest delete namespace guestbook Source\n"},{"uri":"http://konveyor.github.io/crane/installation/","title":"Installing Crane","tags":[],"description":"","content":"Follow the procedure below to install the Crane tool.\nStep 1. Install the Crane binary. Enter the following command to install the latest version of Crane binary. curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;\u0026lt;arch\u0026gt;-\u0026lt;os\u0026gt;\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- chmod +x \u0026lt;binary\u0026gt; cp \u0026lt;binary\u0026gt; /usr/bin/crane Crane currently supports three architectures: amd64-linux amd64-darwin arm64-darwin Run the following command to download the latest version of Crane for amd64-linux. curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;amd64-linux\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- Run the following command to download the latest version of Crane for amd64-darwin. curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;amd64-darwin\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- Run the following command to download the latest version of Crane for arm64-darwin. curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;arm64-darwin\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- Step 2. Install the most recent version of Crane from the upstream main branch. GOPATHshould be configured to build the project. cd $GOPATH git clone https://github.com/konveyor/crane.git cd crane go build -o crane main.go cp crane /usr/bin/crane Note: Install the released version instead of building from upstream main.\nSource\n"},{"uri":"http://konveyor.github.io/crane/tutorials/migratek8cluster/","title":"Migrating a Kubernetes cluster","tags":[],"description":"","content":"This tutorial is an example of how to use the Konveyor tool Crane to migrate an application (inventory) from the source Kubernetes cluster (src) to the destination cluster (dest).\nRefer to the Crane Documentation for more detailed information.\nNote: In addition to migrating with Crane, it is helpful to push the application to git so it can be automatically deployed to any cluster in the future. This demo includes those steps.\nView details of the destination and source clusters. \u0026gt; $ minikube profile list View the applications on the source cluster. \u0026gt; $ kubectl --context src get namespace View the inventory and postgres services. \u0026gt; $ kubectl --context src --namespace inventory get all View the inventory storage capacity. \u0026gt; $ kubectl --context src --namespace inventory get pvc View the front-end database content. \u0026gt; $ curl http:// Set the context to the source. \u0026gt; $ kubectx src List the application namespaces on the source. \u0026gt; $ kubectl get ns Tip: Use the kubectx and kubectl get ns commands to change the context to the destination and verify the migrating application does not exist.\nCreate an export folder. \u0026gt; $crane export --namespace=inventory View what was extracted from the source cluster into yaml files. \u0026gt; $ tree export Note: These files cannot be imported into another cluster because of the existing IP addresses, timestamps, etc. which should not be pushed to git. View this data using the cat command.\nWhen the application is migrated, this information must be updated to the new cluster.\nClean the manifest files before pushing to git. \u0026gt; $ crane transform list-plugins Note: This example uses the Kubernetes plugin. Use the crane plugin-manager list command to view available plugins to install and translate the manifest into OpenShift for example.\nView available options that can be used to change the manifests before pushing to git. \u0026gt; $ crane transform optionals Create a transform folder using the default Kubernetes transform options. \u0026gt; $ crane transform View the patches in the transform folder that will be applied to the original manifests to create new ones. \u0026gt; $ tree transform Open the patches and verify the data that will be changed before the transform. \u0026gt; $ cat [directory path] Clean up the manifest files using the patches and create an output folder. \u0026gt; $ crane apply View the cleaned manifest files. \u0026gt; $ tree output Copy the cleaned manifests to the git folder for future migrations. \u0026gt; $ crane apply -o Change the context to the destination. \u0026gt; $ kubectx dest Create a new namespace. \u0026gt; $ kubectl --context dest create ns inventory \\ namespace/inventory created Verify the name of the pvc needed for migration. \u0026gt; $ kubectl --context src --namespace inventory get pvc Begin the migration. \u0026gt; $ crane transfer-pvc --source-context=src \\ --pvc-name=postgres-pv-claim --destination-context=dest Tip: Perform the migration while the application is running and then again during a maintenance window when the application has been shut down to only migrate over the delta between the migrations. 22. List the contents of the demo directory.\n\u0026gt; $ ls View the migrated files. \u0026gt; $ tree output View the argo inventory file and verify the git repoURL, namespace, and server. \u0026gt; $ cat inventory.argo.yaml Copy the file into the argo cd. \u0026gt; $ kubectl --context dest --namespace argocd apply -f inventory.argo.yaml Open the Argo CD user interface. Open the migrated application. Verify Argo picked up and provisioned all the manifests from the git repo. Click the Sync button at the top of the screen where the application can be resynced from the git repo. Source\n"},{"uri":"http://konveyor.github.io/","title":"Konveyor Documentation","tags":[],"description":"","content":"Why Konveyor? Sysadmins and Developers are tired of the words Digital Transformation. They dont want to hear about the five keys to digital transformation or the seven must-haves to transform. They are also tired of every vendor presenting their framework and methodology for digital transformation bundled with a bunch of proprietary tools.\nDevelopers and sysadmins want to learn how to actually transform their applications and infrastructure so they can take advantage of new technologies to deliver new capabilities faster, at greater scale, and with higher quality while reducing technical debt. It is clear that one of the technologies that underpins the future of applications and infrastructure is Kubernetes - an open-source system for automating deployment, scaling, and management of containerized applications.\nThe Konveyor community exists to accelerate sysadmins and developers\u0026rsquo; journey to Kubernetes. Konveyor is a community of people passionate about helping others modernize and migrate their applications for Kubernetes by building tools, identifying patterns, and providing advice.\nKonveyor projects Konveyor currently consists of five tools or projects:\nForklift Crane Move2Kube Tackle Pelorus Forklift is focused on migrating virtual machines to Kubernetes and provides the ability to migrate multiple virtual machines to KubeVirt with minimal downtime. It allows organizations to rehost, or lift and shift\u0026rsquo;\u0026rsquo; applications residing on these VMs. While rehosting doesnt provide the same depth of benefits as replatforming or refactoring, its the first step in the right direction. Its often useful to have all the unmodified development workloads in Kubernetes as a basecamp for further transformations, or in cases where development teams may not have the ability to change or modify code, such as with vendor provided software. Rehosting also helps teams enjoy the benefits of the new platform with less friction in improving process and culture.\nThe downstream version of this tool, Migration Toolkit for Virtualization (MTV) is available to Red Hat customers interested in moving vSphere virtual machines to OpenShift Virtualization. As of February 2021, it is available as tech preview.\nCrane is another rehosting tool that meets a different use case. It allows organizations to migrate applications between Kubernetes clusters. There are many times when developers and operations teams want to migrate between older and newer versions of Kubernetes, evacuate a cluster, or migrate to different underlying infrastructure. In an ideal scenario, this would be a redeployment of the application, but in reality we have found that many users need a solution for migrating persistent data and the objects within Kubernetes namespaces continuously.\nThe downstream version of Crane, Migration Toolkit for Containers (MTC), is available for Red Hat OpenShift customers interested in moving from version 3.x to 4, as well as from different clusters of version 4. It is fully supported and available on OpenShift Operator Hub.\nMove2Kube is a project that allows customers to replatform their applications to Kubernetes orchestrated platforms. Replatforming, or lift, tinker, and shift, involves changing an underlying technology used by an application while minimizing the need for code change. One area where replatforming is taking place is in the consolidation of container orchestration platforms to Kubernetes. Due to this consolidation, the Move2Kube project was started to focus on accelerating the process of replatform to Kubernetes from platforms such as Swarm and Cloud Foundry. The project translates existing artifacts to Kubernetes artifacts to speed up the process of being able to run applications on Kubernetes.\nTackle provides a series of interrelated tools that allows users to assess, analyze, and ultimately move their applications onto a Kubernetes orchestrated platform. Often considered the most challenging application modernization strategy, adapting applications to a containerized runtime, also offers the largest potential long term impact. This strategy involves making changes to the application and development to take advantage of cloud native capabilities. The tool helps assess the depth of the changes ranging from minimal fixes to adapt the application to containers to a full rewrite of the application in more modern container-friendly runtimes.The Tackle project provides tools that inventory an application environment and identify which workloads are most suitable for refactoring into containers. A common application inventory can also be generated which is then made available as a basis for code execution. The team that is catalyzing this project has experience in these areas from working on tools such as Pathfinder and Windup and will be bringing these experiences to their work on the Tackle project.\nThe downstream version of Tackle, Migration Toolkit for Applications (MTA), is an assembly of tools that support large-scale Java application modernization and migration projects across a broad range of transformations and use cases. MTA accelerates application code analysis, supports effort estimation, accelerates code migration, and helps users move applications to a variety of platforms including OpenShift.\nPelorus is focused around measuring the improvement that moving and modernizing actually delivers as described in the Accelerate book, the reference in DevOps. The community feels strongly that being able to measure the impact of rehosting, replatform, refactoring, and changing processes and culture is vital to proving value. Pelorus is a project focused on measuring the key metrics to software delivery performance (lead time for change, deployment frequency, mean time to restore, and change failure rate) and enabling metrics driven transformation.\nTogether, these five projects comprise today the Konveyor community. Moving forward, it is inevitable that demand for each will align with evolving migration strategies and technology trends. For example, whereas rehosting with Forklift defined use cases may reach heightened demand in the short term, this project could also very well lose traction as Virtual Machines are fully containerized across the landscape in the longer term. New tools could then hypothetically emerge in the Konveyor community empowering users to further extend these containerized applications to edge computing environments, perhaps relying on AI/ML and event-driven architectures. That being said, it is widely expected that there will be ebb and flow with existing project usage, as well as the introduction of entirely new projects altogether.\n-Content taken from the Konveyor Messaging Guide\nSource\n"},{"uri":"http://konveyor.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"http://konveyor.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]