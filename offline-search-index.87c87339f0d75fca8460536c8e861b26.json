[{"body":"Konveyor helps organizations safely and predictably modernize their tradional/legacy applications to Kubernetes.\nKonveyor’s goal is to deliver a Unified Experience to the organizations embarking on their modernization journey. It follows a simple yet effective approach of surfacing the information about the application to aid a ‘Decision Maker’ to make decisions about their modernization and migration needs, plan the work in the form of ‘Migration waves’ and provide guidance to the developers to complete the needed migration/modernization by providing assets as well as a catalog of integrated tools to aid specific workflows.\nComponents of Konveyor Konveyor has three main components as follows,\nApplication Inventory Assessment module Analysis module Application Inventory Application Inventory houses the organization’s portfolio of legacy applications. It provides mechanisms to link them to their respective supported business services, define their interdependencies, and use an extensible tagging model to add metadata to describe and categorize them in multiple dimensions.\nAssessment module Assessment is a questionnaire-based tool that assesses the suitability of applications for modernization. The self-guided questionnaire is designed to bring out the risks associated with the application and the application’s suitability for Kubernetes. The result is available in the form of reports that can be used to generate an adoption plan influenced by various criteria.\nAnalysis module The analysis module examines the application’s source code/binary and its dependencies with a set of predefined/custom rules. It generates a report with a list of issues that need to be addressed to move the application to Kubernetes and provides an effort estimation for the same.\nFeatures Konveyor supports the following exciting features:\nAdministrator perspective: Dedicated perspective to manage tool-wide configuration. Enhanced RBAC: Three new differentiated personas with different permissions - Administrator, Architect and Migrator Integration with repositories: Full integration with source code (Git, Subversion) and binaries (Maven) repositories to automate the retrieval of applications for analysis. Credentials management: Secure store for multiple credential types (source control, Maven settings files, proxy). Credentials are managed by Administrators and assigned by Architects to applications. Proxy integration: HTTP and HTTPS proxy configuration can be managed in the Tackle UI. Analysis module: Full integration with the Windup project to allow the execution of application analysis from the application inventory. Enhanced analysis modes: Aside from source and binary analysis modes, now Tackle includes the Source + Dependencies mode that parses the POM file available in the source repository to gather dependencies from corporate or public artifact repositories, adding them to the scope of the analysis. Analysis scope selection: Simplified user experience to configure the analysis scope, with the possibility to force the analysis of known Open Source libraries. Authless deployment: Tackle can now be optionally deployed without Keycloak, allowing full unauthenticated admin access to the tool. This is especially useful when deploying the tool in resource constrained environments like local instances of Minikube, where only a single user would have access to it. Seamless upgrades: Tackle lifecycle is now managed by a new operator with Capability Level II, allowing seamless upgrades between GA versions. Konveyor Projects UI Application Inventory Assessment Analyzer Source\n","categories":"","description":"","excerpt":"Konveyor helps organizations safely and predictably modernize their …","ref":"/docs/konveyor/","tags":"","title":"Konveyor"},{"body":" Note: This is an optional step. If you are not familiar with Cloud Foundry or you do not want to collect information from the running app, skip to Customizing the output.\nTo analyze the running application in Cloud Foundry (CF), the Move2Kube CLI tool provides a command called collect. As the name suggests, it collects information about applications running in the cloud.\nFor collecting information from a CF running instance, consider requiring cf CLI for logging into Cloud Foundry. To target a specific Kubernetes cluster for the YAMLs, either oc or kubectl to collect information about the target cluster is necessary.\nIf logged into the Cloud Foundry instance, information about the apps such as environment variables, services, and more are collected. If logged into Kubernetes clusters, it collects information about the types of resources that are installed on the cluster, such as whether it has Tekton, BuildConfigs, etc.\nAll the information that was collected gets written into a directory called m2k_collect as YAML files. In this case, the info about Cloud Foundry apps is written to a sub-directory called cf. These YAMLs can then be used during the plan phase to get a holistic plan combining the source and metadata.\nFor example: Some of the information that is collected is port and environment variable information. This allows Move2Kube to select the right ports and set the right environment variables for each service when generating Dockerfiles for containerizing them.\nCollecting info from e2e-demo app Prerequisites\nThe cf tool installed. Logged into the Cloud Foundry instance. Run cf target to verify. The output should be similar to this: $ cf target API endpoint: https://api.cf.my.cloud.provider.com API version: 3.107.0 user: user@gmail.com org: my-org space: dev The enterprise-app app in the Cloud Foundry instance is deployed. $ cf apps Getting apps in org my-org / space dev as user@gmail.com... name requested state processes routes frontend started web:1/1 frontend-1234.my.cloud.provider.com gateway started web:1/1, task:0/0 gateway-5678.my.cloud.provider.com orders started web:1/1, task:0/0 orders-1234.my.cloud.provider.com ... Procedure\nRun move2kube collect to collect information about the app from Cloud Foundry. $ move2kube collect INFO[0000] Begin collection INFO[0000] [*collector.ClusterCollector] Begin collection INFO[0000] [*collector.ClusterCollector] Done INFO[0000] [*collector.ImagesCollector] Begin collection INFO[0000] [*collector.ImagesCollector] Done INFO[0000] [*collector.CfAppsCollector] Begin collection INFO[0011] [*collector.CfAppsCollector] Done INFO[0011] [*collector.CfServicesCollector] Begin collection INFO[0026] [*collector.CfServicesCollector] Done INFO[0026] Collection done INFO[0026] Collect Output in [/Users/user/Desktop/tutorial/m2k_collect]. Copy this directory into the source directory to be used for planning. The output will be in a directory called m2k_collect with a sub-directory called cf containing two YAML files: CfApps and CfServices.\n$ ls m2k_collect/ cf\tclusters\timages $ ls m2k_collect/cf/ cfapps-e3a2f9d68a7a5ecc.yaml\tcfservices-32194c9906854947.yaml The CfApps file contains all the information that was collected about the app such as service names, environment variables, ports, etc. An example is provided here\n\u003cdetails markdown=\"block\"\u003e \u003csummary markdown=\"block\"\u003e # click to see the full yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: CfApps ...... The returned YAML.\napiVersion: move2kube.konveyor.io/v1alpha1 kind: CfApps spec: applications: - application: guid: id1 createdat: \"2021-12-14T10:01:40Z\" updatedat: \"2021-12-14T10:03:08Z\" name: orders memory: 1024 instances: 1 diskquota: 1024 spaceguid: space-id1 stackguid: stack-id1 state: STARTED packagestate: STAGED command: \"\" buildpack: https://github.com/cloudfoundry/java-buildpack detectedbuildpack: java detectedbuildpackguid: \"\" healthcheckhttpendpoint: \"\" healthchecktype: port healthchecktimeout: 0 diego: true enablessh: true detectedstartcommand: 'JAVA_OPTS=\"-agentpath:$PWD/.java-buildpack/open_jdk_jre/bin/jvmkill-1.16.0_RELEASE=printHeapHistogram=1 -Djava.io.tmpdir=$TMPDIR -XX:ActiveProcessorCount=$(nproc) -Djava.ext.dirs=$PWD/.java-buildpack/container_security_provider:$PWD/.java-buildpack/open_jdk_jre/lib/ext -Djava.security.properties=$PWD/.java-buildpack/java_security/java.security $JAVA_OPTS\" \u0026\u0026 CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-3.13.0_RELEASE -totMemory=$MEMORY_LIMIT -loadedClasses=23193 -poolType=metaspace -stackThreads=250 -vmOptions=\"$JAVA_OPTS\") \u0026\u0026 echo JVM Memory Configuration: $CALCULATED_MEMORY \u0026\u0026 JAVA_OPTS=\"$JAVA_OPTS $CALCULATED_MEMORY\" \u0026\u0026 MALLOC_ARENA_MAX=2 SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher' dockerimage: \"\" dockercredentialsjson: {} dockercredentials: username: \"\" password: \"\" environment: {} stagingfailedreason: \"\" stagingfaileddescription: \"\" ports: - 8080 spaceurl: /v2/spaces/space-id1 spacedata: meta: guid: space-id1 url: /v2/spaces/space-id1 createdat: \"2020-10-05T05:29:46Z\" updatedat: \"2020-10-05T05:29:46Z\" entity: guid: space-id1 createdat: \"\" updatedat: \"\" name: dev organizationguid: org-id1 orgurl: /v2/organizations/org-id1 orgdata: meta: guid: org-id1 url: /v2/organizations/org-id1 createdat: \"2020-10-05T05:29:31Z\" updatedat: \"2020-10-05T05:29:31Z\" entity: guid: org-id1 createdat: \"\" updatedat: \"\" name: org-name status: active quotadefinitionguid: quota-id defaultisolationsegmentguid: \"\" quotadefinitionguid: \"\" isolationsegmentguid: \"\" allowssh: true packageupdatedat: \"2021-12-14T10:01:49Z\" environment: environment: {} stagingenv: BLUEMIX_REGION: region runningenv: BLUEMIX_REGION: region systemenv: VCAP_SERVICES: '{}' applicationenv: VCAP_APPLICATION: '{\"application_id\":\"id1\",\"application_name\":\"orders\",\"application_uris\":[\"orders-proud-bilby-rf.net\"],\"application_version\":\"app-ver1\",\"cf_api\":\"app-url\",\"limits\":{\"disk\":1024,\"fds\":16384,\"mem\":1024},\"name\":\"orders\",\"organization_id\":\"org-id1\",\"organization_name\":\"org-name\",\"process_id\":\"id1\",\"process_type\":\"web\",\"space_id\":\"space-id1\",\"space_name\":\"dev\",\"uris\":[\"orders-proud-bilby-rf.net\"],\"users\":null,\"version\":\"app-ver1\"}' - application: guid: id2 createdat: \"2021-12-14T10:04:00Z\" updatedat: \"2021-12-14T10:05:43Z\" name: gateway memory: 1024 instances: 1 diskquota: 1024 spaceguid: space-id1 stackguid: stack-id1 state: STARTED packagestate: STAGED command: \"\" buildpack: https://github.com/cloudfoundry/java-buildpack detectedbuildpack: java detectedbuildpackguid: \"\" healthcheckhttpendpoint: \"\" healthchecktype: port healthchecktimeout: 0 diego: true enablessh: true detectedstartcommand: 'JAVA_OPTS=\"-agentpath:$PWD/.java-buildpack/open_jdk_jre/bin/jvmkill-1.16.0_RELEASE=printHeapHistogram=1 -Djava.io.tmpdir=$TMPDIR -XX:ActiveProcessorCount=$(nproc) -Djava.ext.dirs=$PWD/.java-buildpack/container_security_provider:$PWD/.java-buildpack/open_jdk_jre/lib/ext -Djava.security.properties=$PWD/.java-buildpack/java_security/java.security $JAVA_OPTS\" \u0026\u0026 CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-3.13.0_RELEASE -totMemory=$MEMORY_LIMIT -loadedClasses=24458 -poolType=metaspace -stackThreads=250 -vmOptions=\"$JAVA_OPTS\") \u0026\u0026 echo JVM Memory Configuration: $CALCULATED_MEMORY \u0026\u0026 JAVA_OPTS=\"$JAVA_OPTS $CALCULATED_MEMORY\" \u0026\u0026 MALLOC_ARENA_MAX=2 SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher' dockerimage: \"\" dockercredentialsjson: {} dockercredentials: username: \"\" password: \"\" environment: {} stagingfailedreason: \"\" stagingfaileddescription: \"\" ports: - 8080 spaceurl: /v2/spaces/space-id1 spacedata: meta: guid: space-id1 url: /v2/spaces/space-id1 createdat: \"2020-10-05T05:29:46Z\" updatedat: \"2020-10-05T05:29:46Z\" entity: guid: space-id1 createdat: \"\" updatedat: \"\" name: dev organizationguid: org-id1 orgurl: /v2/organizations/org-id1 orgdata: meta: guid: org-id1 url: /v2/organizations/org-id1 createdat: \"2020-10-05T05:29:31Z\" updatedat: \"2020-10-05T05:29:31Z\" entity: guid: org-id1 createdat: \"\" updatedat: \"\" name: org-name status: active quotadefinitionguid: quota-id defaultisolationsegmentguid: \"\" quotadefinitionguid: \"\" isolationsegmentguid: \"\" allowssh: true packageupdatedat: \"2021-12-14T10:04:09Z\" environment: environment: {} stagingenv: BLUEMIX_REGION: region runningenv: BLUEMIX_REGION: region systemenv: VCAP_SERVICES: '{}' applicationenv: VCAP_APPLICATION: '{\"application_id\":\"id2\",\"application_name\":\"gateway\",\"application_uris\":[\"gateway-restless-fossa-ws.net\"],\"application_version\":\"app-ver2\",\"cf_api\":\"app-url\",\"limits\":{\"disk\":1024,\"fds\":16384,\"mem\":1024},\"name\":\"gateway\",\"organization_id\":\"org-id1\",\"organization_name\":\"org-name\",\"process_id\":\"id2\",\"process_type\":\"web\",\"space_id\":\"space-id1\",\"space_name\":\"dev\",\"uris\":[\"gateway-restless-fossa-ws.net\"],\"users\":null,\"version\":\"app-ver2\"}' - application: guid: id3 createdat: \"2021-12-14T14:54:25Z\" updatedat: \"2021-12-14T15:15:38Z\" name: frontend memory: 1024 instances: 1 diskquota: 1024 spaceguid: space-id1 stackguid: stack-id1 state: STARTED packagestate: STAGED command: npm run start buildpack: https://github.com/cloudfoundry/nodejs-buildpack detectedbuildpack: nodejs detectedbuildpackguid: \"\" healthcheckhttpendpoint: \"\" healthchecktype: port healthchecktimeout: 0 diego: true enablessh: true detectedstartcommand: npm start dockerimage: \"\" dockercredentialsjson: {} dockercredentials: username: \"\" password: \"\" environment: {} stagingfailedreason: \"\" stagingfaileddescription: \"\" ports: - 8080 spaceurl: /v2/spaces/space-id1 spacedata: meta: guid: space-id1 url: /v2/spaces/space-id1 createdat: \"2020-10-05T05:29:46Z\" updatedat: \"2020-10-05T05:29:46Z\" entity: guid: space-id1 createdat: \"\" updatedat: \"\" name: dev organizationguid: org-id1 orgurl: /v2/organizations/org-id1 orgdata: meta: guid: org-id1 url: /v2/organizations/org-id1 createdat: \"2020-10-05T05:29:31Z\" updatedat: \"2020-10-05T05:29:31Z\" entity: guid: org-id1 createdat: \"\" updatedat: \"\" name: org-name status: active quotadefinitionguid: quota-id defaultisolationsegmentguid: \"\" quotadefinitionguid: \"\" isolationsegmentguid: \"\" allowssh: true packageupdatedat: \"2021-12-14T14:59:40Z\" environment: environment: {} stagingenv: BLUEMIX_REGION: region runningenv: BLUEMIX_REGION: region systemenv: VCAP_SERVICES: '{}' applicationenv: VCAP_APPLICATION: '{\"application_id\":\"id3\",\"application_name\":\"frontend\",\"application_uris\":[\"frontend-patient-oryx-mc.net\"],\"application_version\":\"app-ver3\",\"cf_api\":\"app-url\",\"limits\":{\"disk\":1024,\"fds\":16384,\"mem\":1024},\"name\":\"frontend\",\"organization_id\":\"org-id1\",\"organization_name\":\"org-name\",\"process_id\":\"id3\",\"process_type\":\"web\",\"space_id\":\"space-id1\",\"space_name\":\"dev\",\"uris\":[\"frontend-patient-oryx-mc.net\"],\"users\":null,\"version\":\"app-ver3\"}' Now that the runtime information has been collected from the app running in the Cloud Foundry instance, it can be used during the planning phase by simply copying it into the source directory before starting the planning. All the steps are the same as the Plan step.\nNext steps Next is customizing the output that Move2Kube produces using customizations.\nSource\n","categories":"","description":"","excerpt":" Note: This is an optional step. If you are not familiar with Cloud …","ref":"/docs/move2kube/tutorials/cfappstok8/1collect/","tags":"","title":"1. Collect"},{"body":"Move2Kube has four concepts that are useful to understand when customizing output.\nImportant: It may be helpful to go through the tutorials first, then reading this section to learn more about each concept in more detail.\nArtifacts Source code\nArtifacts represent the application objects that can be passed between transformers.\ntype Artifact struct { Name string `yaml:\"name,omitempty\" json:\"name,omitempty\"` Type ArtifactType `yaml:\"type,omitempty\" json:\"type,omitempty\"` ProcessWith metav1.LabelSelector `yaml:\"processWith,omitempty\" json:\"processWith,omitempty\"` // Selector for choosing transformers that should process this artifact, empty is everything Paths map[PathType][]string `yaml:\"paths,omitempty\" json:\"paths,omitempty\" m2kpath:\"normal\"` Configs map[ConfigType]interface{} `yaml:\"configs,omitempty\" json:\"config,omitempty\"` // Could be IR or template config or any custom configuration } Artifact fields Each artifact is an object with fields that need to be understood in order to write transformers effectively.\nname : string - Name of the artifact. type : string - Type of the artifact (any artifact type can be used). Important: Transformers consume artifacts based on their type, so custom artifact types can only be consumed by custom transformers that understand them. Example built-in artifact types include: IR, KubernetesYamls, Dockerfile, etc.\nprocessWith : object - This is the same as the Kubernetes label selector field. See Resources that support set-based requirements paths : object ([string]: []string) - Mapping from a file type to a list of directories containing files of that type. The key is a string containing the file type. The value is a list of strings/paths to directories containing files of that type. configs : object ([string]: any) - Mapping between different types of configurations and the configuration data. The key is a string containing the type of configuration. The value can be anything, but usually an object. Example built-in configs: types/transformer/artifacts/cloudfoundry.go#L35-L39 types/transformer/artifacts/java.go#L49-L75 types/transformer/artifacts/gradle.go#L24-L27 Transformers Move2Kube uses transformers to modify input into the desired output form. Each transformer consumes artifacts as input and returns output artifacts and PathMappings.\nThe artifacts allow multiple transformers to be chained together to achieve an end to end transformation and the PathMappings are used for persisting the changes in the file system. Some transformers have detection capability to go through the source directories to identify once it understands and creates new artifacts to start the process.\nTransformer directories Each transformer generally has its own directory with all the configuration parameters required for that transformer whether it is a built-in transformer or external transformer. The transformer YAML is the most important part of the definition because it specifies its behavior. It also can have a templates directory for template files to be used by the transformer, and other files/configurations that are specific to each transformer.\nTransformer YAML Source code\nTransformers define the definition of the Cloud Foundry (CF) runtime instance app file.\ntype Transformer struct { types.TypeMeta `yaml:\",inline\" json:\",inline\"` types.ObjectMeta `yaml:\"metadata,omitempty\" json:\"metadata,omitempty\"` Spec TransformerSpec `yaml:\"spec,omitempty\" json:\"spec,omitempty\"` } TransformerSpec stores the data.\ntype TransformerSpec struct { FilePath string `yaml:\"-\" json:\"-\"` Class string `yaml:\"class\" json:\"class\"` Isolated bool `yaml:\"isolated\" json:\"isolated\"` DirectoryDetect DirectoryDetect `yaml:\"directoryDetect\" json:\"directoryDetect\"` ExternalFiles map[string]string `yaml:\"externalFiles\" json:\"externalFiles\"` // [source]destination ConsumedArtifacts map[ArtifactType]ArtifactProcessConfig `yaml:\"consumes\" json:\"consumes\"` ProducedArtifacts map[ArtifactType]ProducedArtifact `yaml:\"produces\" json:\"produces\"` Dependency interface{} `yaml:\"dependency\" json:\"dependency\"` // metav1.LabelSelector Override interface{} `yaml:\"override\" json:\"override\"` // metav1.LabelSelector DependencySelector labels.Selector `yaml:\"-\" json:\"-\"` OverrideSelector labels.Selector `yaml:\"-\" json:\"-\"` TemplatesDir string `yaml:\"templates\" json:\"templates\"` // Relative to yaml directory or working directory in image Config interface{} `yaml:\"config\" json:\"config\"` } The example above shows the format of the transformer YAML file. For more information on the YAML format, see this quick tutorial.\nYAML files have four main fields with sub-fields that define them.\nNote: The apiVersion and kind are necessary to tell Move2Kube that this is a transformer.\napiVersion : string - Similar to Kubernetes apiVersion strings. This should be move2kube.konveyor.io/v1alpha1 for now.\nkind : string - Resource type contained in the YAML file which should be labeled Transformer for transformers.\nmetadata : object - Defines the transformer name and can set optional labels used to enable or disable the transformer.\nname : string - Transformer name. labels : object ([string]: string) - Set of labels similar to Kubernetes used to enable/disable a set of transformers during both planning and transformation phases. For more details run the move2kube help transform command. spec : object - Main transformer data.\nclass : string - Mandatory field specifying which Move2Kube internal implementation to use for this transformer. Examples are: Kubernetes, Parameterizer, GolangDockerfileGenerator, Executable, Starlark, etc. isolated : boolean - If true, the transformer will receive a full unmodified copy of the source directory. By default, transformers do not run in isolation but instead receive a temporary directory containing a copy of the source directory that has already been used by other transformers. If other transformers have created temporary files, all of those files will be visible to the transformer. Note: Running in isolation increases the run time of your transformer but makes writing transformers easier because no clean up is necessary after the transformer has finished.\ndirectoryDetect : object - Used to control the directories the transformer runs on during the planning phase. levels : int - Supported values: -1: Runs on the source directory and all sub-directories. 0: Skips directory detect entirely, does not run on any directories. 1: Runs on only the source directory, not on any of the sub-directories. externalFiles : object ([string]: string) - Used to specify files that need to be copied from outside the context of the transformer into the transformer. This is helpful to specify files used by multiple transformers in a single location. consumes : object ([string]: object) - Used to narrow down the artifacts that the transformer runs on during the transformation phase. The key is a string containing the type of the artifact and the value is an object with the following fields: merge : boolean - If true, all artifacts of this type will be merged into a single artifact before being passed to the transformer. produces : object ([string]: object) - Used to tell Move2Kube the type of output artifacts the transformer will return. The key is a string containing the type of the artifact and the value is an object with the following fields: changeTypeTo : string - Used to change the artifact type to something else. Useful for overriding the behavior of existing transformers. dependency : any - If the transformer wants the artifacts that are about to be processed by this transformer to be preprocessed by another transformer, this field specifies the transformer to use for preprocessing. override : any - If this transformer overrides the behavior of other transformers, a selector can be specified to disable those transformers. templates : string - Specifies the template directory. The default value is templates config : any - Each transformer has a type/class specified by the class field which provides certain configuration options that can be configured here. For more details, refer to documentation for the transformer class being used. Example: Parameterizer config Other files/directories templates - If the Template type path mapping created by this transformer uses a relative path, it is considered to be relative to this directory. There can be other files/configs in the directory that are interpreted differently by each transformer class which then determines how the values are interpreted and executed.\nTransformer Class The Transformer Class determines the code used for the internal execution of the transformer using the configuration in the Transformer Yaml and other config files to model its behavior. There are many transformer classes supported by Move2Kube: Kubernetes, Parameterizer, GolangDockerfileGenerator, Executable, Starlark, Router, etc. Most of them have a specific task, but some transformer classes like Executable and Starlark are customizable allowing users to write the entire logic of the transformer in the customization.\nTransformer Class Internal Implementation Source code\nThe transformer interface defines the transformer that modifies and converts files to IR representation.\ntype Transformer interface { Init(tc transformertypes.Transformer, env *environment.Environment) (err error) // GetConfig returns the transformer config GetConfig() (transformertypes.Transformer, *environment.Environment) DirectoryDetect(dir string) (services map[string][]transformertypes.Artifact, err error) Transform(newArtifacts []transformertypes.Artifact, alreadySeenArtifacts []transformertypes.Artifact) ([]transformertypes.PathMapping, []transformertypes.Artifact, error) } This example is the interface all transformers are expected to implement.\nTransform: The main function that needs to be implemented. Use DirectoryDetect for custom behavior during the planning phase. Important: Set directoryDetect to a value other than 0 in the transformer YAML.\nThe Init and GetConfig functions are fixed and implemented by transformers built into Move2Kube. They cannot be implemented by custom transformers. Methods Init : (Transformer, Environment) -\u003e (error) - TODO\nGetConfig : (Transformer, Environment) -\u003e () - TODO\nDirectoryDetect : (string) -\u003e (object ([string]: []Artifact), error): This function is called during the planning phase and given the path of a directory containing the source files and then returns a list of artifacts listed in the plan file generated by Move2Kube. It will also return an error if planning does not run correctly.\nInput is a string containing the path to a directory with source files which could be the source directory itself or a sub-directory based on the value of directoryDetect in the transformer YAML. -1: The function runs on the source directory and all of its sub-directories. 0: The function is disabled entirely (it will not be run on any directories). 1: The function runs only on the source directory but not on its sub-directories. The output is a list of artifacts which will be included in the plan-file. Transform : ([]Artifact, []Artifact) -\u003e ([]PathMapping, []Artifact, error): This function is called during the transformation phase and contains the code to perform the actual transformation and produce part of the Move2Kube output. The path mappings returned by this function cause changes to the Move2Kube output and the artifacts returned are passed to other transformers during the next iteration. It will also return an error if planning does not run correctly.\nThe first input is a list of new artifacts produced during the previous iteration. The second input is a list of artifacts that the transformer has already seen. The third output is a list of path mappings. Path Mapping Path mappings are a way for transformers to add files to the Move2Kube output directory. They can be used to generate new files, delete existing files, modify the output directory structure, etc. Usually transformers deal with artifacts as they take them as input and outputs new artifacts, but does nothing to change the Move2Kube output since all transformers are run inside temporary directories.\nIn order to affect the output directory, transformers need to return path mappings indicating the type of change to be made.\nFor example: Consider a transformer that adds an annotation to Kubernetes Ingress YAML files. The transformer reads the file, adds the annotation, and then writes it back out. However this modified file is only present inside the temporary directory and does not appear in the output directory of Move2Kube. To copy this file over to the output directory, create a path mapping to return this from the transformer.:\n{ \"type\": \"Source\", \"sourcePath\": \"annotated-ingress.yaml\", \"destinationPath\": \"deploy/yamls/ingress.yaml\" } Once the transformer is finished, Move2Kube will look at the path mapping the transformer returned and copy over the file to the output directory.\nThe example above shows the simplest use case for path mappings, but they are capable of much more advanced uses. For example: the source file is a template and needs to be filled in before being copied to the output.\nAnother example is when the source and destination paths are template strings that need to be filled in order to get the actual paths.\nDifferent types of path mappings Source code\nPathMappingType refers to the Path Mapping type.\ntype PathMappingType = string const ( // DefaultPathMappingType allows normal copy with overwrite // TemplatePathMappingType allows copy of source to destination and applying of template TemplatePathMappingType PathMappingType = \"Template\" // Source path when relative, is relative to yaml file location // SourcePathMappingType allows for copying of source directory to another directory SourcePathMappingType PathMappingType = \"Source\" // Source path becomes relative to source directory // DeletePathMappingType allows for deleting of files or directories DeletePathMappingType PathMappingType = \"Delete\" // Delete path becomes relative to source directory // ModifiedSourcePathMappingType allows for copying of deltas wrt source ModifiedSourcePathMappingType PathMappingType = \"SourceDiff\" // Source path becomes relative to source directory // PathTemplatePathMappingType allows for path template registration PathTemplatePathMappingType PathMappingType = \"PathTemplate\" // Path Template type // SpecialTemplatePathMappingType allows copy of source to destination and applying of template with custom delimiter SpecialTemplatePathMappingType PathMappingType = \"SpecialTemplate\" // Source path when relative, is relative to yaml file location ) PathMapping is the mapping between source and intermediate files and output files.\ntype PathMapping struct { Type PathMappingType `yaml:\"type,omitempty\" json:\"type,omitempty\"` // Default - Normal copy SrcPath string `yaml:\"sourcePath\" json:\"sourcePath\" m2kpath:\"normal\"` DestPath string `yaml:\"destinationPath\" json:\"destinationPath\" m2kpath:\"normal\"` // Relative to output directory TemplateConfig interface{} `yaml:\"templateConfig\" json:\"templateConfig\"` } There are seven different types of path mappings:\nDefault - sourcePath must be an absolute path. destinationPath must be a relative path, relative to Move2Kube’s output directory. This will copy the directory/file specified in sourcePath to destinationPath. Template - sourcePath must be a relative path, relative to the templates directory of the transformer. destinationPath must be a relative path, relative to Move2Kube’s output directory. This fills the template in the file given by sourcePath and copies the filled template to destinationPath. The values for filling the template are given in templateConfig. Source - Same as Default path mapping except now the sourcePath can now be a relative path, relative to the temporary directory where the transformer is running. Delete - sourcePath must be a relative path, relative to Move2Kube’s output directory. The directory/file specified by sourcePath will be deleted. SourceDiff - TODO PathTemplate - The path itself becomes a template. sourcePath contains the template path. templateConfig can be used to set a name for this template path. SpecialTemplate - Same as Template path mapping except now the template has a different syntax. The delimiters used in normal templates are {{ and }}. In special templates, the delimiters are \u003c~ and ~\u003e. Same as before, the values for filling the template are provided in templateConfig. Phases Move2Kube uses two key phases:\nPlanning Transformation Planning phase This phase starts by running the move2kube plan -s path/to/source/directory command. Move2Kube runs all the transformers that support the detect capability on the source directory to create a plan. The plan is written to a file called m2k.plan in YAML format which the transformation phase will use this plan to modify the source files into the desired output. The plan file is human readable and can be edited manually to change the modifications performed during the transformation phase.\nThe plan contains the list of detected services that Move2Kube found inside the source directory, including the path to the sub-directories/files where it detected information about those services. It also contains a list of all the built-in and external transformers that were detected which will be run during the transformation phase. Custom transformers can be written and provided during the plan phase to affect the contents of the plan.\nTransformation phase This phase starts by running the move2kube transform command. Move2Kube evaluates which transformers to run in an iterative manner. Each iteration will evaluate the list of artifacts produced during the previous iteration and runs all transformers that consume those artifact types. This continues until it hits an iteration where there are no more artifacts or transformers that consume those artifact types at which point the transformation phase is complete.\nThe evaluated result of all PathMappings is the output.\nSource\n","categories":"","description":"","excerpt":"Move2Kube has four concepts that are useful to understand when …","ref":"/docs/move2kube/concepts/","tags":"","title":"Concepts"},{"body":" We start by planning the migration. During the plan phase, Move2Kube will analyze the files in the source directory, detect what services exist, create a plan on how to containerize them using Dockerfiles, and transform them into Kubernetes deployments, services, ingress, etc.\nIn order to do the planning, Move2Kube has a large number of built-in transformers for different languages and platforms. Each transformer walks through the source directory from top to bottom and tries to find files that it recognizes. For example, a Golang transformer will try to find a go.mod file to detect a Golang project. Once it detects a directory containing a service, it will try to extract as much information from it as possible. Some of the information it tries to find are the service name, ports, environment variables, etc.\nThe plan file can be created using the CLI or through the UI. When complete, the plan file contains all the transformers that Move2Kube detected and ran. These transformers will be run again during the transformation phase.\nThe plan file also contains all the services that Move2Kube was able to detect. The service name comes from the transformer that detected that service. We can edit this plan before moving to the transformation phase. For now, leave it as is.\nThis information is stored in YAML format in a plan file called m2k.plan which is used later during the transformation phase. We can edit this file to enable/disable transformers, add/remove detected services, etc.\nPrerequisites Download the enterprise-app using the command below.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/src -r move2kube-demos $ ls src README.md\tconfig-utils\tcustomers\tdocs\tfrontend\tgateway\torders Planning using the CLI Run move2kube plan -s src to generate a plan for migrating the multiple components of the app to Kubernetes. $ move2kube plan -s src INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [CloudFoundry] Planning transformation INFO[0000] Identified 3 named services and 0 to-be-named services INFO[0000] [CloudFoundry] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [Base Directory] Identified 3 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in config-utils INFO[0000] Identified 1 named services and 0 to-be-named services in customers INFO[0000] Identified 1 named services and 0 to-be-named services in frontend INFO[0000] Identified 1 named services and 0 to-be-named services in gateway INFO[0000] Identified 1 named services and 0 to-be-named services in inventory INFO[0000] Identified 1 named services and 0 to-be-named services in orders INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 6 named services and 0 to-be-named services INFO[0000] [Named Services] Identified 6 named services INFO[0000] No of services identified : 6 INFO[0000] Plan can be found at [/Users/user/Desktop/tutorial/m2k.plan]. Look at the plan file that was generated. $ cat m2k.plan apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan ...... apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: src services: config-utils: - transformerName: Maven paths: MavenPom: - config-utils/pom.xml ServiceDirPath: - config-utils configs: Maven: mavenAppName: config-utils artifactType: jar mvnwPresent: false customers: - transformerName: Maven paths: MavenPom: - customers/pom.xml ServiceDirPath: - customers configs: Maven: mavenAppName: customers artifactType: war mavenProfiles: - dev-inmemorydb - prod-externaldb mvnwPresent: true SpringBoot: springBootVersion: 2.5.0 springBootProfiles: - dev-inmemorydb - prod-externaldb frontend: - transformerName: CloudFoundry paths: CfManifest: - frontend/manifest.yml ServiceDirPath: - frontend configs: CloudFoundryService: serviceName: frontend ContainerizationOptions: - Nodejs-Dockerfile - transformerName: Nodejs-Dockerfile paths: ServiceDirPath: - frontend gateway: - transformerName: CloudFoundry paths: BuildArtifact: - gateway/target/ROOT.jar CfManifest: - gateway/manifest.yml ServiceDirPath: - gateway configs: CloudFoundryService: serviceName: gateway ContainerizationOptions: - Maven - transformerName: Maven paths: MavenPom: - gateway/pom.xml ServiceDirPath: - gateway configs: Maven: mavenAppName: gateway artifactType: jar mavenProfiles: - dev - prod mvnwPresent: true SpringBoot: springBootAppName: gateway springBootProfiles: - dev - prod inventory: - transformerName: Maven paths: MavenPom: - inventory/pom.xml ServiceDirPath: - inventory configs: Maven: mavenAppName: inventory artifactType: jar mavenProfiles: - dev-inmemorydb - prod-externaldb mvnwPresent: true SpringBoot: springBootProfiles: - dev-inmemorydb - prod-externaldb orders: - transformerName: CloudFoundry paths: BuildArtifact: - orders/target/ROOT.jar CfManifest: - orders/manifest.yml ServiceDirPath: - orders configs: CloudFoundryService: serviceName: orders ContainerizationOptions: - Maven - transformerName: Maven paths: MavenPom: - orders/pom.xml ServiceDirPath: - orders configs: Maven: mavenAppName: orders artifactType: jar mavenProfiles: - dev-inmemorydb - prod-externaldb mvnwPresent: true SpringBoot: springBootAppName: orders springBootProfiles: - dev-inmemorydb - prod-externaldb transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/transformer.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/transformer.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/transformer.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/transformer.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/transformer.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimagespushscript/transformer.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/transformer.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/transformer.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/transformer.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/transformer.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/transformer.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/transformer.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/transformer.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/transformer.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/transformer.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/transformer.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/transformer.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/transformer.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/transformer.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/transformer.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/transformer.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/transformer.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/transformer.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/transformer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/transformer.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/transformer.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/transformer.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/transformer.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/transformer.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/transformer.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/transformer.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/transformer.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/transformer.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/transformer.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/transformer.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/transformer.yaml Next step: Transform to generate the output needed to deploy the app to Kubernetes.\nPlanning using the UI Open the UI. $ docker run --rm -it -p 8080:8080 quay.io/konveyor/move2kube-ui:v0.3.1 INFO[0000] Starting Move2Kube API server at port: 8080 Create a new workspace. Create a new project. Scroll down to the project inputs section and then upload the source directory and the collected information zip files. Optional: If you have collected Cloud Foundry runtime metadata using the move2kube collect command you can create a zip file and upload that as well. Make sure to upload it as sources.\nScroll down to the planning section and click Start Planning. Note Planning takes a few minutes.\nNext step: Transform to generate the output needed to deploy app to Kubernetes.\nSource\n","categories":"","description":"","excerpt":" We start by planning the migration. During the plan phase, Move2Kube …","ref":"/docs/move2kube/tutorials/cfappstok8/2plan/","tags":"","title":"2. Plan"},{"body":"Now run the transformation according to the plan file generated in the previous step. The transformation phase runs all of the transformers again, but this time the transformers will use the plan to generate the output files.\nDuring this process, the transformers might run into situations where it requires some more information to generate the output. In order to get this information, it will ask the user some questions. The questions range from yes/no, to multiple choice, to string input and most will have a default answer.\nExample: Some of the questions Move2Kube will ask is about the type of container registry where you want to push the images to. It also needs to know the registry namespace and any authentication necessary for pulling images from that registry.\nIf you want to skip the QA, use the --qa-skip flag to accept the default answers. However, a config file that contains all of the answers using the --config flag can be used instead of skipping.\nAfter the transformation is finished, all the answers are written to a config file called m2kconfig.yaml which can be used for later transformations.\nThe transformation phase produces all the necessary output files including the Dockerfiles, build scripts for containerizing various services and Kubernetes deployment, and the service and ingress YAMLs necessary for deploying the application to a Kubernetes cluster.\nMove2Kube also generates the CI/CD pipeline and parameterized versions of all the Kubernetes YAMLs (Helm chart, Kustomize YAMLs, Openshift templates, etc.) for various environments (dev, staging, prod, etc.).\nPrerequisites Perform the Plan step before this procedure.\nTransforming using the CLI Run the transformation in the same directory as the plan file. This will detect the plan file and use it to find the source directory. $ move2kube transform Optional: Provide answers to questions using a config file... If you want to avoid the question answers during transformation, you can use this config file\n$ move2kube transform --config m2kconfig.yaml Answer all the questions as appropriate. For most questions accept the default answers. Some questions to watch out for are: A spurious service called config-utils was detected by one of the transformers can be deselected when asked to select the services or by editing the plan file. Move2Kube has detected the Maven profiles for each of the Java services. Select the dev-inmemorydb profile to deploy to MiniKube. There will be similar questions for the SpringBoot profiles. The container registry and namespace to use. A container registry is where all the images are stored (Example: Quay, Docker Hub, etc.). The ingress hostname and ingress TLS secret. If deploying to MiniKube, give localhost as the ingress host and leave the TLS secret blank. Select ClusterIP to only expose the order customers inventory and gateway services inside the cluster. Choose Ingress and / as the path to expose the frontend service. This way only the frontend will be exposed outside the cluster through the ingress. $ move2kube transform INFO[0000] Detected a plan file at path /Users/user/Desktop/tutorial/m2k.plan. Will transform using this plan. INFO[0000] Starting Plan Transformation ? Select all transformer types that you are interested in: ID: move2kube.transformers.types Hints: [Services that don't support any of the transformer types you are interested in will be ignored.] Buildconfig, CloudFoundry, ClusterSelector, ComposeAnalyser, ComposeGenerator, ContainerImagesPushScriptGenerator, DockerfileDetector, DockerfileImageBuildScript, DockerfileParser, DotNetCore-Dockerfile,EarAnalyser, EarRouter, Golang-Dockerfile, Gradle, Jar, Jboss, Knative, Kubernetes, KubernetesVersionChanger, Liberty, Maven, Nodejs-Dockerfile, PHP-Dockerfile, Parameterizer, Python-Dockerfile, ReadMeGenerator,Ruby-Dockerfile, Rust-Dockerfile, Tekton, Tomcat, WarAnalyser, WarRouter, WinConsoleApp-Dockerfile, WinSLWebApp-Dockerfile, WinWebApp-Dockerfile, ZuulAnalyser ? Select all services that are needed: ID: move2kube.services.[].enable Hints: [The services unselected here will be ignored.] customers, frontend, gateway, inventory, orders INFO[0005] Iteration 1 INFO[0005] Iteration 2 - 5 artifacts to process INFO[0005] Transformer CloudFoundry processing 3 artifacts INFO[0005] Transformer CloudFoundry Done INFO[0005] Transformer Maven processing 2 artifacts ? Choose the Maven profile to be used for the service customers ID: move2kube.services.customers.activemavenprofiles Hints: [Selected Maven profiles will be used for setting configuration for the service customers] prod-externaldb ? Choose Springboot profiles to be used for the service customers ID: move2kube.services.customers.activespringbootprofiles Hints: [Selected Springboot profiles will be used for setting configuration for the service customers] prod-externaldb ? Choose the Maven profile to be used for the service inventory ID: move2kube.services.inventory.activemavenprofiles Hints: [Selected Maven profiles will be used for setting configuration for the service inventory] prod-externaldb ? Choose Springboot profiles to be used for the service inventory ID: move2kube.services.inventory.activespringbootprofiles Hints: [Selected Springboot profiles will be used for setting configuration for the service inventory] prod-externaldb ? Select port to be exposed for the service inventory : ID: move2kube.services.inventory.port Hints: [Select Other if you want to expose the service inventory to some other port] 8080 INFO[0010] Transformer WarRouter processing 2 artifacts ? Select the transformer to use for service customers ID: move2kube.services.customers.wartransformer Tomcat INFO[0012] Transformer WarRouter Done INFO[0012] Transformer Maven Done INFO[0012] Created 2 pathMappings and 6 artifacts. Total Path Mappings : 2. Total Artifacts : 5. INFO[0012] Iteration 3 - 6 artifacts to process INFO[0012] Transformer Jar processing 1 artifacts INFO[0012] Transformer Jar Done INFO[0012] Transformer Maven processing 2 artifacts ? Choose the Maven profile to be used for the service gateway ID: move2kube.services.gateway.activemavenprofiles Hints: [Selected Maven profiles will be used for setting configuration for the service gateway] prod ? Choose Springboot profiles to be used for the service gateway ID: move2kube.services.gateway.activespringbootprofiles Hints: [Selected Springboot profiles will be used for setting configuration for the service gateway] prod ? Select port to be exposed for the service gateway : ID: move2kube.services.gateway.port Hints: [Select Other if you want to expose the service gateway to some other port] 8080 ? Choose the Maven profile to be used for the service orders ID: move2kube.services.orders.activemavenprofiles Hints: [Selected Maven profiles will be used for setting configuration for the service orders] prod-externaldb ? Choose Springboot profiles to be used for the service orders ID: move2kube.services.orders.activespringbootprofiles Hints: [Selected Springboot profiles will be used for setting configuration for the service orders] prod-externaldb ? Select port to be exposed for the service orders : ID: move2kube.services.orders.port Hints: [Select Other if you want to expose the service orders to some other port] 8080 INFO[0018] Transformer Maven Done INFO[0018] Transformer Nodejs-Dockerfile processing 1 artifacts ? Enter the port to be exposed for the service frontend: ID: move2kube.services.frontend.port Hints: [The service frontend will be exposed to the specified port] 8080 INFO[0021] Transformer Nodejs-Dockerfile Done INFO[0021] Transformer Tomcat processing 2 artifacts INFO[0021] Transformer Tomcat Done INFO[0021] Created 10 pathMappings and 10 artifacts. Total Path Mappings : 12. Total Artifacts : 11. INFO[0021] Iteration 4 - 10 artifacts to process INFO[0021] Transformer DockerfileImageBuildScript processing 4 artifacts ? Select the container runtime to use : ID: move2kube.containerruntime Hints: [The container runtime selected will be used in the scripts] docker INFO[0022] Transformer DockerfileImageBuildScript Done INFO[0022] Transformer DockerfileParser processing 4 artifacts INFO[0022] Transformer ZuulAnalyser processing 2 artifacts INFO[0022] Transformer ZuulAnalyser Done INFO[0022] Transformer DockerfileParser Done INFO[0022] Transformer Jar processing 2 artifacts INFO[0022] Transformer Jar Done INFO[0022] Created 5 pathMappings and 10 artifacts. Total Path Mappings : 17. Total Artifacts : 21. INFO[0022] Iteration 5 - 10 artifacts to process INFO[0022] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: ID: move2kube.target.clustertype Hints: [Choose the cluster type you would like to target] Kubernetes INFO[0024] Transformer ClusterSelector Done INFO[0024] Transformer Buildconfig processing 2 artifacts ? What kind of service/ingress to create for inventory's 8080 port? ID: move2kube.services.\"inventory\".\"8080\".servicetype Hints: [Choose Ingress if you want a ingress/route resource to be created] ClusterIP ? What kind of service/ingress to create for frontend's 8080 port? ID: move2kube.services.\"frontend\".\"8080\".servicetype Hints: [Choose Ingress if you want a ingress/route resource to be created] Ingress ? Specify the ingress path to expose frontend's 8080 port? ID: move2kube.services.\"frontend\".\"8080\".urlpath Hints: [Leave out leading / to use first part as subdomain] / ? What kind of service/ingress to create for customers's 8080 port? ID: move2kube.services.\"customers\".\"8080\".servicetype Hints: [Choose Ingress if you want a ingress/route resource to be created] ClusterIP ? Provide the minimum number of replicas each service should have ID: move2kube.minreplicas Hints: [If the value is 0 pods won't be started by default] 2 ? Enter the URL of the image registry : ID: move2kube.target.imageregistry.url Hints: [You can always change it later by changing the yamls.] quay.io ? Enter the namespace where the new images should be pushed : ID: move2kube.target.imageregistry.namespace Hints: [Ex : myproject] move2kube ? [quay.io] What type of container registry login do you want to use? ID: move2kube.target.imageregistry.logintype Hints: [Docker login from config mode, will use the default config from the local machine.] No authentication INFO[0051] Transformer Buildconfig Done INFO[0051] Transformer ComposeGenerator processing 2 artifacts INFO[0051] Transformer ComposeGenerator Done INFO[0051] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[0051] Transformer ContainerImagesPushScriptGenerator Done INFO[0051] Transformer DockerfileImageBuildScript processing 3 artifacts INFO[0051] Transformer DockerfileImageBuildScript Done INFO[0051] Transformer DockerfileParser processing 2 artifacts INFO[0051] Transformer ZuulAnalyser processing 2 artifacts INFO[0051] Transformer ZuulAnalyser Done INFO[0051] Transformer DockerfileParser Done INFO[0051] Transformer ClusterSelector processing 2 artifacts INFO[0051] Transformer ClusterSelector Done INFO[0051] Transformer Knative processing 2 artifacts INFO[0051] Transformer Knative Done INFO[0051] Transformer ClusterSelector processing 2 artifacts INFO[0051] Transformer ClusterSelector Done INFO[0051] Transformer Kubernetes processing 2 artifacts ? Provide the ingress host domain ID: move2kube.target.ingress.host Hints: [Ingress host domain is part of service URL] localhost ? Provide the TLS secret for ingress ID: move2kube.target.ingress.tls Hints: [Leave empty to use http] INFO[0058] Transformer Kubernetes Done INFO[0058] Transformer ClusterSelector processing 2 artifacts INFO[0058] Transformer ClusterSelector Done INFO[0058] Transformer Tekton processing 2 artifacts INFO[0059] Transformer Tekton Done INFO[0059] Created 32 pathMappings and 15 artifacts. Total Path Mappings : 49. Total Artifacts : 31. INFO[0059] Iteration 6 - 15 artifacts to process INFO[0059] Transformer ClusterSelector processing 2 artifacts INFO[0059] Transformer ClusterSelector Done INFO[0059] Transformer Buildconfig processing 2 artifacts ? What kind of service/ingress to create for orders's 8080 port? ID: move2kube.services.\"orders\".\"8080\".servicetype Hints: [Choose Ingress if you want a ingress/route resource to be created] ClusterIP ? What kind of service/ingress to create for gateway's 8080 port? ID: move2kube.services.\"gateway\".\"8080\".servicetype Hints: [Choose Ingress if you want a ingress/route resource to be created] ClusterIP INFO[0066] Transformer Buildconfig Done INFO[0066] Transformer ComposeGenerator processing 2 artifacts INFO[0066] Transformer ComposeGenerator Done INFO[0066] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[0066] Transformer ContainerImagesPushScriptGenerator Done INFO[0066] Transformer ClusterSelector processing 2 artifacts INFO[0067] Transformer ClusterSelector Done INFO[0067] Transformer Knative processing 2 artifacts INFO[0067] Transformer Knative Done INFO[0067] Transformer ClusterSelector processing 2 artifacts INFO[0067] Transformer ClusterSelector Done INFO[0067] Transformer Kubernetes processing 2 artifacts INFO[0067] Transformer Kubernetes Done INFO[0067] Transformer Parameterizer processing 4 artifacts INFO[0067] Transformer Parameterizer Done INFO[0067] Transformer ReadMeGenerator processing 5 artifacts INFO[0067] Transformer ReadMeGenerator Done INFO[0067] Transformer ClusterSelector processing 2 artifacts INFO[0067] Transformer ClusterSelector Done INFO[0067] Transformer Tekton processing 2 artifacts INFO[0068] Transformer Tekton Done INFO[0068] Created 52 pathMappings and 7 artifacts. Total Path Mappings : 101. Total Artifacts : 46. INFO[0068] Iteration 7 - 7 artifacts to process INFO[0068] Transformer Parameterizer processing 4 artifacts INFO[0068] Transformer Parameterizer Done INFO[0068] Transformer ReadMeGenerator processing 5 artifacts INFO[0068] Transformer ReadMeGenerator Done INFO[0069] Plan Transformation done INFO[0069] Transformed target artifacts can be found at [/Users/user/Desktop/tutorial/myproject]. Transforming using the UI Continue from the previous step in the UI.\nScroll down from the Plan section to the Outputs section. spec: sourceDir:sources services: config-utils: -transformerName: Maven paths: MavenPom: -src/src/config-utils/pom.xml ServiceDirPath: -src/src/config-utils configs: Maven: mavenAppName: config-utils artifactType: jar customers-tomcat: - transformerName: Maven Click the Start transformation button. A form to ask the user questions to guide the transformation opens.\nAnswer all the questions as appropriate. For most questions accept the default answers. Some questions to watch out for are: A spurious service called config-utils was detected by one of the transformers can be deselected when asked to select the services or by editing the plan file. Move2Kube has detected the Maven profiles for each of the Java services. Select the dev-inmemorydb profile to deploy to MiniKube. There will be similar questions for the SpringBoot profiles. The container registry and namespace to use. A container registry is where all the images are stored (Example: Quay, Docker Hub, etc.). The ingress hostname and ingress TLS secret. If deploying to MiniKube, give localhost as the ingress host and leave the TLS secret blank. Select ClusterIP to only expose the order customers inventory and gateway services inside the cluster. Choose Ingress and / as the path to expose the frontend service. This way only the frontend will be exposed outside the cluster through the ingress. Click the Next button to continue going through the questions and then run the tranformation. Move2Kube processes the transformation and the output appears.\nClick the output ID link to download. Using the output generated by Move2Kube transform For a sample output of what Move2Kube generates for this enterprise app, click here.\nAfter the output has generated, run the scripts inside the scripts directory.\nRun the builddockerimages.sh script to build all the container images for each service using the Dockerfiles that were generated. $ cd myproject/scripts/ $ ./builddockerimages.sh Run the pushimages.sh script to push them to the specified container registry. $ ./pushimages.sh Because the prod-externaldb profile was selected, deploy the database using the YAMLs located here. $ cd .. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/database -r move2kube-demos $ minikube start --memory 8192 --cpus 2 # do this only if you are deploying to Minikube $ kubectl apply -f database/ Deploy the Kubernetes YAMLs that Move2Kube generated to the cluster $ kubectl apply -f deploy/yamls The application is now running on the cluster.\nGet the URL where the app has been deployed to, using kubectl get ingress myproject -o yaml Note: If deployed to Minikube, enable the ingress addon and start minikube tunnel to access the ingress on localhost.\n$ minikube addons enable ingress 💡 After the addon is enabled, please run \"minikube tunnel\" and the ingress resources would be available at \"127.0.0.1\" ▪ Using image k8s.gcr.io/ingress-nginx/controller:v1.0.4 ▪ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 ▪ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 🔎 Verifying ingress addon... 🌟 The 'ingress' addon is enabled $ minikube addons enable ingress-dns 💡 After the addon is enabled, please run \"minikube tunnel\" and the ingress resources would be available at \"127.0.0.1\" ▪ Using image gcr.io/k8s-minikube/minikube-ingress-dns:0.0.2 🌟 The 'ingress-dns' addon is enabled $ minikube tunnel ❗ The service/ingress myproject requires privileged ports to be exposed: [80 443] 🔑 sudo permission will be asked for it. 🏃 Starting tunnel for service myproject. Password: The app is now available on http://localhost.\nOptional: As part of the transformation, if Cloud Foundry runtime information is required, use the collect output in planning and transformation: Collect information from running apps.\nCustomizing the output After inspecting the output that Move2Kube produced some changes might be necessary. For example:\nChanging the base image used in the Dockerfiles. Adding some annotations to the Ingress YAML. Changing the output directory structure. Changing which values are parameterized in the Helm chart. Generating some new files, etc. For all these user specific requirements and more, use customizations.\nSource\n","categories":"","description":"","excerpt":"Now run the transformation according to the plan file generated in the …","ref":"/docs/move2kube/tutorials/cfappstok8/3transform/","tags":"","title":"3. Transform"},{"body":"This tutorial shows how to transform a set of sample applications to run on Kubernetes using the Move2Kube CLI tool to generate the Kubernetes YAMLs, Dockerfiles, build scripts for each application, and then build the container images to deploy them to a cluster.\nPrerequisites Install the Move2Kube CLI tool.\nUse thelanguage-platforms sample directory which has a combination of multiple applications in different languages (Java, Go, Python, Ruby, etc.) that need to be containerized and deployed to Kubernetes.\nUsing the CLI to perform a transformation Download the language platforms sample. Each directory contains a simple web application written in different languages. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/language-platforms -r move2kube-demos $ ls language-platforms django\tgolang\tjava-gradle\tjava-gradle-war\tjava-maven\tjava-maven-war\tnodejs\tphp\tpython\truby\trust Run move2kube plan -s language-platforms to generate a plan file. The -s flag specifies language-platforms as the source directory for Move2Kube to analyze the source code and come up with a plan for transforming them to Kubernetes YAMLs. $ move2kube plan -s language-platforms INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [CloudFoundry] Planning transformation INFO[0000] [CloudFoundry] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [Base Directory] Identified 0 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in django INFO[0000] Identified 1 named services and 0 to-be-named services in golang INFO[0000] Identified 1 named services and 0 to-be-named services in java-gradle INFO[0000] Identified 1 named services and 0 to-be-named services in java-gradle-war INFO[0000] Identified 1 named services and 0 to-be-named services in java-maven INFO[0000] Identified 1 named services and 0 to-be-named services in java-maven-war INFO[0000] Identified 1 named services and 0 to-be-named services in nodejs INFO[0000] Identified 1 named services and 0 to-be-named services in php INFO[0000] Identified 1 named services and 0 to-be-named services in python INFO[0000] Identified 1 named services and 0 to-be-named services in ruby INFO[0000] Identified 1 named services and 0 to-be-named services in rust INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 6 named services and 5 to-be-named services INFO[0000] [Named Services] Identified 11 named services INFO[0000] No of services identified : 11 INFO[0000] Plan can be found at [/Users/user/Desktop/tutorial/m2k.plan]. Look at the plan file generated in YAML format. Notice Move2Kube has detected all the different services, one for each web app. $ ls language-platforms\tlanguage-platforms.zip\tm2k.plan $ cat m2k.plan apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: language-platforms services: golang: - transformerName: Golang-Dockerfile paths: GoModFilePath: - golang/go.mod ServiceDirPath: - golang myproject-django: - transformerName: Python-Dockerfile paths: MainPythonFilesPathType: [] PythonFilesPathType: - django/manage.py RequirementsTxtPathType: - django/requirements.txt ServiceDirPath: - django configs: PythonConfig: IsDjango: true myproject-java-gradle: - transformerName: Gradle paths: GradleBuildFile: - java-gradle/build.gradle ServiceDirPath: - java-gradle configs: Gradle: artifactType: jar myproject-java-gradle-war: - transformerName: WarAnalyser paths: ServiceDirPath: - java-gradle-war War: - java-gradle-war/java-gradle-war.war configs: War: deploymentFile: java-gradle-war.war javaVersion: \"\" buildContainerName: \"\" deploymentFileDirInBuildContainer: \"\" envVariables: {} myproject-java-maven-war: - transformerName: WarAnalyser paths: ServiceDirPath: - java-maven-war War: - java-maven-war/java-maven-war.war configs: War: deploymentFile: java-maven-war.war javaVersion: \"\" buildContainerName: \"\" deploymentFileDirInBuildContainer: \"\" envVariables: {} myproject-php: - transformerName: PHP-Dockerfile paths: ServiceDirPath: - php myproject-python: - transformerName: Python-Dockerfile paths: MainPythonFilesPathType: [] PythonFilesPathType: - python/main.py RequirementsTxtPathType: - python/requirements.txt ServiceDirPath: - python configs: PythonConfig: IsDjango: false nodejs: - transformerName: Nodejs-Dockerfile paths: ServiceDirPath: - nodejs ruby: - transformerName: Ruby-Dockerfile paths: ServiceDirPath: - ruby rust: - transformerName: Rust-Dockerfile paths: ServiceDirPath: - rust simplewebapp: - transformerName: Maven paths: MavenPom: - java-maven/pom.xml ServiceDirPath: - java-maven configs: Maven: mavenAppName: simplewebapp artifactType: war transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/buildconfig.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/cloudfoundry.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/clusterselector.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/composeanalyser.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/composegenerator.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimage/containerimagespushscript/containerimagespushscript.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/dockerfiledetector.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/dockerfilebuildscriptgenerator.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/dockerfileparser.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/dotnetcore.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/ear.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/earrouter.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/golang.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/gradle.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/jar.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/jboss.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/knative.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/kubernetes.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/kubernetesversionchanger.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/liberty.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/maven.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/nodejs.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/php.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/parameterizer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/python.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/readmegenerator.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/ruby.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/rust.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/tekton.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/tomcat.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/war.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/warrouter.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/winconsole.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/winsilverlightweb.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/winweb.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/zuulanalyser.yaml Run the transformation using move2kube transform to perform the transformation according to the generated plan. By default Move2Kube looks for a plan file in the current directory. Specify the path to a different plan file using the -p flag. During transformation Move2Kube will ask several questions to help guide the transformation process. For most questions accept the default answers. Some questions to watch out for are:\nThe container registry and namespace to use. A container registry is where all the images are stored (Example: Quay, Docker Hub, etc.). The ingress hostname and ingress TLS secret. If deploying to MiniKube, give localhost as the ingress host and leave the TLS secret blank. For all other questions accept the default answers by pressing Enter for each.\n$ move2kube transform INFO[0000] Detected a plan file at path /Users/user/Desktop/tutorial/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: Hints: [Services that don't support any of the transformer types you are interested in will be ignored.] ComposeGenerator, DockerfileDetector, Jboss, WinSLWebApp-Dockerfile, ZuulAnalyser, Buildconfig, Maven, Tekton, Tomcat, WarRouter, WinConsoleApp-Dockerfile,DotNetCore-Dockerfile, EarAnalyser, KubernetesVersionChanger, Nodejs-Dockerfile, Ruby-Dockerfile, WinWebApp-Dockerfile, CloudFoundry, ComposeAnalyser, DockerfileParser, EarRouter, Gradle, ClusterSelector, Rust-Dockerfile, ContainerImagesPushScriptGenerator, ReadMeGenerator, WarAnalyser, Jar, Golang-Dockerfile, Knative, Kubernetes, Liberty, PHP-Dockerfile, Parameterizer, Python-Dockerfile, DockerfileImageBuildScript ? Select all services that are needed: Hints: [The services unselected here will be ignored.] golang, myproject-python, nodejs, rust, simplewebapp, myproject-django, myproject-java-gradle, myproject-java-gradle-war, myproject-java-maven-war, myproject-php, ruby INFO[0009] Starting Plan Transformation INFO[0009] Iteration 1 INFO[0009] Iteration 2 - 11 artifacts to process INFO[0009] Transformer Maven processing 1 artifacts INFO[0009] Transformer WarRouter processing 2 artifacts ? Select the transformer to use for service simplewebapp Tomcat INFO[0014] Transformer WarRouter Done INFO[0014] Transformer Maven Done INFO[0014] Transformer PHP-Dockerfile processing 1 artifacts INFO[0014] Transformer PHP-Dockerfile Done INFO[0014] Transformer Nodejs-Dockerfile processing 1 artifacts ? Enter the port to be exposed for the service nodejs: Hints: [The service nodejs will be exposed to the specified port] 8080 INFO[0016] Transformer Nodejs-Dockerfile Done INFO[0016] Transformer Ruby-Dockerfile processing 1 artifacts ? Select port to be exposed for the service ruby : Hints: [Select Other if you want to expose the service ruby to some other port] 8080 INFO[0017] Transformer Ruby-Dockerfile Done INFO[0017] Transformer WarAnalyser processing 2 artifacts INFO[0017] Transformer WarRouter processing 3 artifacts ? Select the transformer to use for service myproject-java-gradle-war Tomcat ? Select the transformer to use for service myproject-java-maven-war Tomcat INFO[0020] Transformer WarRouter Done INFO[0020] Transformer WarAnalyser Done INFO[0020] Transformer Golang-Dockerfile processing 1 artifacts ? Select ports to be exposed for the service golang : Hints: [Select Other if you want to add more ports] 8080 INFO[0021] Transformer Golang-Dockerfile Done INFO[0021] Transformer Gradle processing 1 artifacts ? Select port to be exposed for the service myproject-java-gradle : Hints: [Select Other if you want to expose the service myproject-java-gradle to some other port] 8080 INFO[0022] Transformer Gradle Done INFO[0022] Transformer Rust-Dockerfile processing 1 artifacts ? Select port to be exposed for the service rust : Hints: [Select Other if you want to expose the service rust to some other port] 8085 INFO[0023] Transformer Rust-Dockerfile Done INFO[0023] Transformer Python-Dockerfile processing 2 artifacts ? Select port to be exposed for the service myproject-django : Hints: [Select Other if you want to expose the service myproject-django to some other port] 8080 ? Select port to be exposed for the service myproject-python : Hints: [Select Other if you want to expose the service myproject-python to some other port] 8080 INFO[0024] Transformer Python-Dockerfile Done INFO[0024] Created 16 pathMappings and 20 artifacts. Total Path Mappings : 16. Total Artifacts : 11. INFO[0024] Iteration 3 - 20 artifacts to process INFO[0024] Transformer DockerfileImageBuildScript processing 8 artifacts ? Select the container runtime to use : Hints: [The container runtime selected will be used in the scripts] docker INFO[0028] Transformer DockerfileImageBuildScript Done INFO[0028] Transformer Jar processing 1 artifacts INFO[0028] Transformer Jar Done INFO[0028] Transformer DockerfileParser processing 7 artifacts INFO[0028] Transformer ZuulAnalyser processing 2 artifacts INFO[0028] Transformer ZuulAnalyser Done INFO[0028] Transformer DockerfileParser Done INFO[0028] Transformer Tomcat processing 4 artifacts INFO[0028] Transformer Tomcat Done INFO[0028] Created 11 pathMappings and 20 artifacts. Total Path Mappings : 27. Total Artifacts : 31. INFO[0028] Iteration 4 - 20 artifacts to process INFO[0028] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: Hints: [Choose the cluster type you would like to target] Kubernetes INFO[0030] Transformer ClusterSelector Done INFO[0030] Transformer Tekton processing 2 artifacts ? What URL/path should we expose the service rust's 8085 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /rust ? What URL/path should we expose the service golang's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /golang ? What URL/path should we expose the service ruby's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /ruby ? What URL/path should we expose the service myproject-python's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-python ? What URL/path should we expose the service nodejs's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /nodejs ? What URL/path should we expose the service myproject-php's 8082 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-php ? What URL/path should we expose the service myproject-django's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-django ? Provide the minimum number of replicas each service should have Hints: [If the value is 0 pods won't be started by default] 2 ? Enter the URL of the image registry : Hints: [You can always change it later by changing the yamls.] quay.io ? Enter the namespace where the new images should be pushed : Hints: [Ex : myproject] move2kube ? [quay.io] What type of container registry login do you want to use? Hints: [Docker login from config mode, will use the default config from your local machine.] No authentication ? Provide the ingress host domain Hints: [Ingress host domain is part of service URL] localhost ? Provide the TLS secret for ingress Hints: [Leave empty to use http] INFO[0049] Transformer Tekton Done INFO[0049] Transformer ClusterSelector processing 2 artifacts INFO[0049] Transformer ClusterSelector Done INFO[0049] Transformer Knative processing 2 artifacts INFO[0050] Transformer Knative Done INFO[0050] Transformer ComposeGenerator processing 2 artifacts INFO[0050] Transformer ComposeGenerator Done INFO[0050] Transformer ClusterSelector processing 2 artifacts INFO[0050] Transformer ClusterSelector Done INFO[0050] Transformer Kubernetes processing 2 artifacts INFO[0050] Transformer Kubernetes Done INFO[0050] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[0050] Transformer ContainerImagesPushScriptGenerator Done INFO[0050] Transformer DockerfileImageBuildScript processing 5 artifacts INFO[0050] Transformer DockerfileImageBuildScript Done INFO[0050] Transformer DockerfileParser processing 5 artifacts INFO[0050] Transformer ZuulAnalyser processing 2 artifacts INFO[0050] Transformer ZuulAnalyser Done INFO[0050] Transformer DockerfileParser Done INFO[0050] Transformer ClusterSelector processing 2 artifacts INFO[0050] Transformer ClusterSelector Done INFO[0050] Transformer Buildconfig processing 2 artifacts INFO[0050] Transformer Buildconfig Done INFO[0050] Created 40 pathMappings and 21 artifacts. Total Path Mappings : 67. Total Artifacts : 51. INFO[0050] Iteration 5 - 21 artifacts to process INFO[0050] Transformer ClusterSelector processing 2 artifacts INFO[0050] Transformer ClusterSelector Done INFO[0050] Transformer Buildconfig processing 2 artifacts ? What URL/path should we expose the service simplewebapp's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /simplewebapp ? What URL/path should we expose the service myproject-java-gradle's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-java-gradle ? What URL/path should we expose the service myproject-java-gradle-war's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-java-gradle-war ? What URL/path should we expose the service myproject-java-maven-war's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] /myproject-java-maven-war INFO[0054] Transformer Buildconfig Done INFO[0054] Transformer ReadMeGenerator processing 5 artifacts INFO[0054] Transformer ReadMeGenerator Done INFO[0054] Transformer ClusterSelector processing 2 artifacts INFO[0054] Transformer ClusterSelector Done INFO[0054] Transformer Knative processing 2 artifacts INFO[0055] Transformer Knative Done INFO[0055] Transformer ClusterSelector processing 2 artifacts INFO[0055] Transformer ClusterSelector Done INFO[0055] Transformer Tekton processing 2 artifacts INFO[0055] Transformer Tekton Done INFO[0055] Transformer ClusterSelector processing 2 artifacts INFO[0055] Transformer ClusterSelector Done INFO[0055] Transformer Kubernetes processing 2 artifacts INFO[0055] Transformer Kubernetes Done INFO[0055] Transformer ComposeGenerator processing 2 artifacts INFO[0055] Transformer ComposeGenerator Done INFO[0055] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[0055] Transformer ContainerImagesPushScriptGenerator Done INFO[0055] Transformer Parameterizer processing 4 artifacts INFO[0055] Transformer Parameterizer Done INFO[0056] Created 60 pathMappings and 7 artifacts. Total Path Mappings : 127. Total Artifacts : 72. INFO[0056] Iteration 6 - 7 artifacts to process INFO[0056] Transformer Parameterizer processing 4 artifacts INFO[0056] Transformer Parameterizer Done INFO[0056] Transformer ReadMeGenerator processing 5 artifacts INFO[0056] Transformer ReadMeGenerator Done INFO[0056] Plan Transformation done INFO[0056] Transformed target artifacts can be found at [/Users/user/Desktop/tutorial/myproject]. After the questions are finished wait a few minutes for it to finish processing and generated a directory called `myproject`. The name of the output directory is the same as the project name (by default `myproject`). The project name can be changed using the `-n` flag. ```console $ ls language-platforms\tlanguage-platforms.zip\tm2k.plan\tm2kconfig.yaml\tm2kqacache.yaml\tmyproject $ ls myproject/ Readme.md\tdeploy\tscripts\tsource The applications can now be deployed to Kubernetes using these generated artifacts.\nDeploying the application to Kubernetes with the generated artifacts View the full structure of the output directory by executing the tree command. $ cd myproject/ $ tree $ cd myproject/ $ tree . ├── Readme.md ├── deploy │ ├── cicd │ │ ├── tekton │ │ │ ├── myproject-clone-build-push-pipeline.yaml │ │ │ ├── myproject-clone-push-serviceaccount.yaml │ │ │ ├── myproject-git-event-triggerbinding.yaml │ │ │ ├── myproject-git-repo-eventlistener.yaml │ │ │ ├── myproject-image-registry-secret.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-run-clone-build-push-triggertemplate.yaml │ │ │ ├── myproject-tekton-triggers-admin-role.yaml │ │ │ ├── myproject-tekton-triggers-admin-rolebinding.yaml │ │ │ └── myproject-tekton-triggers-admin-serviceaccount.yaml │ │ └── tekton-parameterized │ │ ├── helm-chart │ │ │ └── myproject │ │ │ ├── Chart.yaml │ │ │ └── templates │ │ │ ├── myproject-clone-build-push-pipeline.yaml │ │ │ ├── myproject-clone-push-serviceaccount.yaml │ │ │ ├── myproject-git-event-triggerbinding.yaml │ │ │ ├── myproject-git-repo-eventlistener.yaml │ │ │ ├── myproject-image-registry-secret.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-run-clone-build-push-triggertemplate.yaml │ │ │ ├── myproject-tekton-triggers-admin-role.yaml │ │ │ ├── myproject-tekton-triggers-admin-rolebinding.yaml │ │ │ └── myproject-tekton-triggers-admin-serviceaccount.yaml │ │ ├── kustomize │ │ │ └── base │ │ │ ├── kustomization.yaml │ │ │ ├── myproject-clone-build-push-pipeline.yaml │ │ │ ├── myproject-clone-push-serviceaccount.yaml │ │ │ ├── myproject-git-event-triggerbinding.yaml │ │ │ ├── myproject-git-repo-eventlistener.yaml │ │ │ ├── myproject-image-registry-secret.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-run-clone-build-push-triggertemplate.yaml │ │ │ ├── myproject-tekton-triggers-admin-role.yaml │ │ │ ├── myproject-tekton-triggers-admin-rolebinding.yaml │ │ │ └── myproject-tekton-triggers-admin-serviceaccount.yaml │ │ └── openshift-template │ │ └── template.yaml │ ├── compose │ │ └── docker-compose.yaml │ ├── knative │ │ ├── golang-service.yaml │ │ ├── myproject-django-service.yaml │ │ ├── myproject-java-gradle-service.yaml │ │ ├── myproject-java-gradle-war-service.yaml │ │ ├── myproject-java-maven-war-service.yaml │ │ ├── myproject-php-service.yaml │ │ ├── myproject-python-service.yaml │ │ ├── nodejs-service.yaml │ │ ├── ruby-service.yaml │ │ ├── rust-service.yaml │ │ └── simplewebapp-service.yaml │ ├── knative-parameterized │ │ ├── helm-chart │ │ │ └── myproject │ │ │ ├── Chart.yaml │ │ │ └── templates │ │ │ ├── golang-service.yaml │ │ │ ├── myproject-django-service.yaml │ │ │ ├── myproject-java-gradle-service.yaml │ │ │ ├── myproject-java-gradle-war-service.yaml │ │ │ ├── myproject-java-maven-war-service.yaml │ │ │ ├── myproject-php-service.yaml │ │ │ ├── myproject-python-service.yaml │ │ │ ├── nodejs-service.yaml │ │ │ ├── ruby-service.yaml │ │ │ ├── rust-service.yaml │ │ │ └── simplewebapp-service.yaml │ │ ├── kustomize │ │ │ └── base │ │ │ ├── golang-service.yaml │ │ │ ├── kustomization.yaml │ │ │ ├── myproject-django-service.yaml │ │ │ ├── myproject-java-gradle-service.yaml │ │ │ ├── myproject-java-gradle-war-service.yaml │ │ │ ├── myproject-java-maven-war-service.yaml │ │ │ ├── myproject-php-service.yaml │ │ │ ├── myproject-python-service.yaml │ │ │ ├── nodejs-service.yaml │ │ │ ├── ruby-service.yaml │ │ │ ├── rust-service.yaml │ │ │ └── simplewebapp-service.yaml │ │ └── openshift-template │ │ └── template.yaml │ ├── yamls │ │ ├── golang-deployment.yaml │ │ ├── golang-service.yaml │ │ ├── myproject-django-deployment.yaml │ │ ├── myproject-django-service.yaml │ │ ├── myproject-ingress.yaml │ │ ├── myproject-java-gradle-deployment.yaml │ │ ├── myproject-java-gradle-service.yaml │ │ ├── myproject-java-gradle-war-deployment.yaml │ │ ├── myproject-java-gradle-war-service.yaml │ │ ├── myproject-java-maven-war-deployment.yaml │ │ ├── myproject-java-maven-war-service.yaml │ │ ├── myproject-php-deployment.yaml │ │ ├── myproject-php-service.yaml │ │ ├── myproject-python-deployment.yaml │ │ ├── myproject-python-service.yaml │ │ ├── nodejs-deployment.yaml │ │ ├── nodejs-service.yaml │ │ ├── ruby-deployment.yaml │ │ ├── ruby-service.yaml │ │ ├── rust-deployment.yaml │ │ ├── rust-service.yaml │ │ ├── simplewebapp-deployment.yaml │ │ └── simplewebapp-service.yaml │ └── yamls-parameterized │ ├── helm-chart │ │ └── myproject │ │ ├── Chart.yaml │ │ ├── templates │ │ │ ├── golang-deployment.yaml │ │ │ ├── golang-service.yaml │ │ │ ├── myproject-django-deployment.yaml │ │ │ ├── myproject-django-service.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-java-gradle-deployment.yaml │ │ │ ├── myproject-java-gradle-service.yaml │ │ │ ├── myproject-java-gradle-war-deployment.yaml │ │ │ ├── myproject-java-gradle-war-service.yaml │ │ │ ├── myproject-java-maven-war-deployment.yaml │ │ │ ├── myproject-java-maven-war-service.yaml │ │ │ ├── myproject-php-deployment.yaml │ │ │ ├── myproject-php-service.yaml │ │ │ ├── myproject-python-deployment.yaml │ │ │ ├── myproject-python-service.yaml │ │ │ ├── nodejs-deployment.yaml │ │ │ ├── nodejs-service.yaml │ │ │ ├── ruby-deployment.yaml │ │ │ ├── ruby-service.yaml │ │ │ ├── rust-deployment.yaml │ │ │ ├── rust-service.yaml │ │ │ ├── simplewebapp-deployment.yaml │ │ │ └── simplewebapp-service.yaml │ │ ├── values-dev.yaml │ │ ├── values-prod.yaml │ │ └── values-staging.yaml │ ├── kustomize │ │ ├── base │ │ │ ├── golang-deployment.yaml │ │ │ ├── golang-service.yaml │ │ │ ├── kustomization.yaml │ │ │ ├── myproject-django-deployment.yaml │ │ │ ├── myproject-django-service.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-java-gradle-deployment.yaml │ │ │ ├── myproject-java-gradle-service.yaml │ │ │ ├── myproject-java-gradle-war-deployment.yaml │ │ │ ├── myproject-java-gradle-war-service.yaml │ │ │ ├── myproject-java-maven-war-deployment.yaml │ │ │ ├── myproject-java-maven-war-service.yaml │ │ │ ├── myproject-php-deployment.yaml │ │ │ ├── myproject-php-service.yaml │ │ │ ├── myproject-python-deployment.yaml │ │ │ ├── myproject-python-service.yaml │ │ │ ├── nodejs-deployment.yaml │ │ │ ├── nodejs-service.yaml │ │ │ ├── ruby-deployment.yaml │ │ │ ├── ruby-service.yaml │ │ │ ├── rust-deployment.yaml │ │ │ ├── rust-service.yaml │ │ │ ├── simplewebapp-deployment.yaml │ │ │ └── simplewebapp-service.yaml │ │ └── overlays │ │ ├── dev │ │ │ ├── apps-v1-deployment-golang.yaml │ │ │ ├── apps-v1-deployment-myproject-django.yaml │ │ │ ├── apps-v1-deployment-myproject-java-gradle-war.yaml │ │ │ ├── apps-v1-deployment-myproject-java-gradle.yaml │ │ │ ├── apps-v1-deployment-myproject-java-maven-war.yaml │ │ │ ├── apps-v1-deployment-myproject-php.yaml │ │ │ ├── apps-v1-deployment-myproject-python.yaml │ │ │ ├── apps-v1-deployment-nodejs.yaml │ │ │ ├── apps-v1-deployment-ruby.yaml │ │ │ ├── apps-v1-deployment-rust.yaml │ │ │ ├── apps-v1-deployment-simplewebapp.yaml │ │ │ └── kustomization.yaml │ │ ├── prod │ │ │ ├── apps-v1-deployment-golang.yaml │ │ │ ├── apps-v1-deployment-myproject-django.yaml │ │ │ ├── apps-v1-deployment-myproject-java-gradle-war.yaml │ │ │ ├── apps-v1-deployment-myproject-java-gradle.yaml │ │ │ ├── apps-v1-deployment-myproject-java-maven-war.yaml │ │ │ ├── apps-v1-deployment-myproject-php.yaml │ │ │ ├── apps-v1-deployment-myproject-python.yaml │ │ │ ├── apps-v1-deployment-nodejs.yaml │ │ │ ├── apps-v1-deployment-ruby.yaml │ │ │ ├── apps-v1-deployment-rust.yaml │ │ │ ├── apps-v1-deployment-simplewebapp.yaml │ │ │ └── kustomization.yaml │ │ └── staging │ │ ├── apps-v1-deployment-golang.yaml │ │ ├── apps-v1-deployment-myproject-django.yaml │ │ ├── apps-v1-deployment-myproject-java-gradle-war.yaml │ │ ├── apps-v1-deployment-myproject-java-gradle.yaml │ │ ├── apps-v1-deployment-myproject-java-maven-war.yaml │ │ ├── apps-v1-deployment-myproject-php.yaml │ │ ├── apps-v1-deployment-myproject-python.yaml │ │ ├── apps-v1-deployment-nodejs.yaml │ │ ├── apps-v1-deployment-ruby.yaml │ │ ├── apps-v1-deployment-rust.yaml │ │ ├── apps-v1-deployment-simplewebapp.yaml │ │ └── kustomization.yaml │ └── openshift-template │ ├── parameters-dev.yaml │ ├── parameters-prod.yaml │ ├── parameters-staging.yaml │ └── template.yaml ├── scripts │ ├── builddockerimages.bat │ ├── builddockerimages.sh │ ├── pushimages.bat │ └── pushimages.sh └── source ├── django │ ├── Dockerfile │ ├── Pipfile │ ├── Pipfile.lock │ ├── db.sqlite3 │ ├── manage.py │ ├── requirements.txt │ ├── simplewebapp │ │ ├── __init__.py │ │ ├── asgi.py │ │ ├── settings.py │ │ ├── urls.py │ │ └── wsgi.py │ └── webroot │ ├── __init__.py │ ├── admin.py │ ├── apps.py │ ├── migrations │ │ └── __init__.py │ ├── models.py │ ├── tests.py │ ├── urls.py │ └── views.py ├── golang │ ├── Dockerfile │ ├── go.mod │ └── main.go ├── java-gradle │ ├── Dockerfile │ ├── build.gradle │ └── src │ └── main │ ├── java │ │ └── simplewebapp │ │ └── MainServlet.java │ └── webapp │ └── WEB-INF │ └── web.xml ├── java-gradle-war │ ├── Dockerfile │ └── java-gradle-war.war ├── java-maven │ ├── Dockerfile │ ├── pom.xml │ └── src │ └── main │ └── webapp │ ├── WEB-INF │ │ └── web.xml │ └── index.jsp ├── java-maven-war │ ├── Dockerfile │ └── java-maven-war.war ├── nodejs │ ├── Dockerfile │ ├── main.js │ └── package.json ├── php │ ├── Dockerfile │ ├── index.php │ └── site.conf ├── python │ ├── Dockerfile │ ├── main.py │ └── requirements.txt ├── ruby │ ├── Dockerfile │ ├── Gemfile │ ├── config.ru │ ├── ruby.rb │ └── views │ └── main.erb └── rust ├── Cargo.toml ├── Dockerfile ├── Rocket.toml └── src └── main.rs 59 directories, 241 files The CLI has created Kubernetes YAMLs which are stored inside the deploy/yamls directory. For each of the directories and the services identified, it has created the deployment artifacts, service artifacts, and the ingress as required. The scripts directory contains the scripts for building the images for the applications using Dockerfiles.\nMany scripts like builddockerimages.sh and pushimages.sh are also present inside the directory. It has also created a simple deploy/compose/docker-compose.yaml to test the images locally. It has also created Tekton artifacts inside the deploy/cicd/tekton directory that are required if you want to use Tekton as your CI/CD pipeline.\nThe Readme.md file guides the next steps to follow.\n$ cat Readme.md Move2Kube --------- Congratulations! Move2Kube has generated the necessary build artfiacts for moving all your application components to Kubernetes. Using the artifacts in this directory you can deploy your application in a kubernetes cluster. Next Steps ---------- To try locally, use the scripts in the \"./scripts\" directory, to build, push and deploy. For production usage, use the CI/CD pipelines for deployment. Run the builddockerimages.sh script. Note: This step may take some time to complete.\n$ cd scripts/ $ ./builddockerimages.sh [+] Building 2.2s (11/11) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 1.36kB 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for registry.access.redhat.com/ubi8/ubi-minimal:latest 2.0s =\u003e [1/6] FROM registry.access.redhat.com/ubi8/ubi-minimal:latest@sha256:cf1c63e3247e4074ee3549a064b8798a1a2513ad57dd79c9edb979836355b469 0.0s =\u003e [internal] load build context 0.0s =\u003e =\u003e transferring context: 19.68kB 0.0s =\u003e CACHED [2/6] RUN microdnf update \u0026\u0026 microdnf install -y java-11-openjdk-devel wget tar \u0026\u0026 microdnf clean all 0.0s =\u003e CACHED [3/6] WORKDIR /usr/local 0.0s =\u003e CACHED [4/6] RUN wget https://downloads.apache.org/tomcat/tomcat-9/v9.0.56/bin/apache-tomcat-9.0.56.tar.gz \u0026\u0026 tar -zxf apache-tomcat-9.0.56.tar.gz \u0026\u0026 rm -f apache-tomcat-9.0.56.tar.gz \u0026\u0026 mv apache-tomcat-9.0.56 tomcat9 0.0s =\u003e CACHED [5/6] RUN useradd -r tomcat \u0026\u0026 chown -R tomcat:tomcat tomcat9 0.0s =\u003e CACHED [6/6] COPY --chown=tomcat:tomcat java-gradle-war.war /usr/local/tomcat9/webapps/ 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:3b3a60601e19f502b4163984702bc4e35729a26470ec2e3b8e5e076ad0662db6 0.0s =\u003e =\u003e naming to docker.io/library/myproject-java-gradle-war 0.0s Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them /Users/user/Desktop/tutorial/myproject [+] Building 0.9s (11/11) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 1.36kB 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for registry.access.redhat.com/ubi8/ubi-minimal:latest 0.7s =\u003e [1/6] FROM registry.access.redhat.com/ubi8/ubi-minimal:latest@sha256:cf1c63e3247e4074ee3549a064b8798a1a2513ad57dd79c9edb979836355b469 0.0s =\u003e [internal] load build context 0.0s =\u003e =\u003e transferring context: 3.15kB 0.0s =\u003e CACHED [2/6] RUN microdnf update \u0026\u0026 microdnf install -y java-11-openjdk-devel wget tar \u0026\u0026 microdnf clean all 0.0s =\u003e CACHED [3/6] WORKDIR /usr/local 0.0s =\u003e CACHED [4/6] RUN wget https://downloads.apache.org/tomcat/tomcat-9/v9.0.56/bin/apache-tomcat-9.0.56.tar.gz \u0026\u0026 tar -zxf apache-tomcat-9.0.56.tar.gz \u0026\u0026 rm -f apache-tomcat-9.0.56.tar.gz \u0026\u0026 mv apache-tomcat-9.0.56 tomcat9 0.0s =\u003e CACHED [5/6] RUN useradd -r tomcat \u0026\u0026 chown -R tomcat:tomcat tomcat9 0.0s =\u003e CACHED [6/6] COPY --chown=tomcat:tomcat java-maven-war.war /usr/local/tomcat9/webapps/ 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:07fb8e8c412414af37d0cc43100325eae37f0ede3885f09f627836c540f8514e 0.0s =\u003e =\u003e naming to docker.io/library/myproject-java-maven-war 0.0s Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them /Users/user/Desktop/tutorial/myproject [+] Building 3.5s (8/8) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 798B 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for registry.access.redhat.com/ubi8/php-74:latest 3.3s =\u003e [1/3] FROM registry.access.redhat.com/ubi8/php-74:latest@sha256:6409cbedb0f80c0ffc823e8b9912245ea40f6bac7ec980c80f838554a8356d55 0.0s =\u003e [internal] load build context 0.0s =\u003e =\u003e transferring context: 1.79kB 0.0s =\u003e CACHED [2/3] COPY site.conf /etc/httpd/conf.d/ 0.0s =\u003e CACHED [3/3] COPY . /var/www/html/ 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:0c2f598d30a420c255d546fddbbf7ba5431bddd4befb6ce2c1e79caec95e6a89 0.0s =\u003e =\u003e naming to docker.io/library/myproject-php 0.0s Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them /Users/user/Desktop/tutorial/myproject [+] Building 1.6s (8/8) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 747B 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for registry.access.redhat.com/ubi8/nodejs-12:latest 1.4s =\u003e [internal] load build context 0.0s =\u003e =\u003e transferring context: 1.77kB 0.0s =\u003e [1/3] FROM registry.access.redhat.com/ubi8/nodejs-12@sha256:0d0632645a115013db659b59aaadc56473c628b0fe4f14585eee12d37d15b66e 0.0s =\u003e CACHED [2/3] COPY . . 0.0s =\u003e CACHED [3/3] RUN npm install 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:c1e60390daed5c3e6e2268b9fcbcd395da58c926d2b6ca542432aff3fbceada4 0.0s =\u003e =\u003e naming to docker.io/library/nodejs 0.0s ... Log in to a container registry from the terminal. Refer to the instructions for Quay and Docker Hub\nPush the container images to the registry specified during the transformation using the pushimages.sh script.\n$ ./pushimages.sh Using default tag: latest The push refers to repository [quay.io/move2kube/myproject-python] 8558b30e6fa3: Pushed c25982faebf0: Pushed e262751a7e43: Layer already exists 5e6a0ab87a4b: Layer already exists 7b17276847a2: Layer already exists 558b534f4e1b: Layer already exists 3ba8c926eef9: Layer already exists 352ba846236b: Layer already exists latest: digest: sha256:d76b1dc841442b0e31c533c1e2419b3ae670de7b4381d4ff1ca2eaf1fbf5dfe6 size: 1999 Using default tag: latest The push refers to repository [quay.io/move2kube/myproject-php] 44abc3aaaae4: Layer already exists c6a7a5dc5bb6: Layer already exists 6a1b2ca6ba18: Layer already exists 7b17276847a2: Layer already exists 558b534f4e1b: Mounted from quay.io/move2kube/myproject-django 3ba8c926eef9: Layer already exists 352ba846236b: Layer already exists latest: digest: sha256:a42087a9c938dc46a265c2aba787040ddd106248954c09a50a8e45e2d9e068f7 size: 1788 Using default tag: latest The push refers to repository [quay.io/move2kube/nodejs] c76f2a9d04f4: Layer already exists d9ba94873664: Layer already exists 24607ae115a3: Layer already exists 7b17276847a2: Layer already exists 558b534f4e1b: Layer already exists 3ba8c926eef9: Layer already exists 352ba846236b: Layer already exists latest: digest: sha256:169443441285694a29ec9235ac0b4d07c1f23a0e20b1f41be967e4416f8d1687 size: 1788 Using default tag: latest The push refers to repository [quay.io/move2kube/ruby] 2177ca5056ad: Layer already exists 856aec824b2a: Layer already exists 5f70bf18a086: Layer already exists ... Deploy the applications using kubectl apply -f ./deploy/yamls. $ cd .. $ kubectl apply -f deploy/yamls deployment.apps/golang created service/golang created deployment.apps/myproject-django created service/myproject-django created ingress.networking.k8s.io/myproject created deployment.apps/myproject-java-gradle created service/myproject-java-gradle created deployment.apps/myproject-java-gradle-war created service/myproject-java-gradle-war created deployment.apps/myproject-java-maven-war created service/myproject-java-maven-war created deployment.apps/myproject-php created service/myproject-php created deployment.apps/myproject-python created service/myproject-python created deployment.apps/nodejs created service/nodejs created deployment.apps/ruby created service/ruby created deployment.apps/rust created service/rust created deployment.apps/simplewebapp created service/simplewebapp created Now all applications are accessible on the cluster.\nGet the ingress to see the URLs where the apps have been deployed to kubectl get ingress myproject -o yaml. Important: If deployed to Minikube, enable the ingress addon.\n$ minikube addons enable ingress 💡 After the addon is enabled, please run \"minikube tunnel\" and your ingress resources would be available at \"127.0.0.1\" ▪ Using image k8s.gcr.io/ingress-nginx/controller:v1.0.4 ▪ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 ▪ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 🔎 Verifying ingress addon... 🌟 The 'ingress' addon is enabled $ minikube addons enable ingress-dns 💡 After the addon is enabled, please run \"minikube tunnel\" and your ingress resources would be available at \"127.0.0.1\" ▪ Using image gcr.io/k8s-minikube/minikube-ingress-dns:0.0.2 🌟 The 'ingress-dns' addon is enabled $ minikube tunnel ❗ The service/ingress myproject requires privileged ports to be exposed: [80 443] 🔑 sudo permission will be asked for it. 🏃 Starting tunnel for service myproject. Password: Conclusion A very diverse source environment like the language-platforms sample, which has multiple apps in different languages, but can simply containerize and deploy them to Kubernetes. A Move2Kube UI tool which has all the same features as the CLI.\nSource\n","categories":"","description":"","excerpt":"This tutorial shows how to transform a set of sample applications to …","ref":"/docs/move2kube/tutorials/usingcli/","tags":"","title":"Using Move2Kube CLI"},{"body":"The Move2Kube tool helps application owners migrate legacy workloads to run on Kubernetes clusters and eventually automate their deployments after multiple iterations. It analyzes Docker Compose files, Cloud Foundry manifest files, and even source code to generate Kubernetes deployment files including object YAML files, Helm charts, and operators.\nMove2Kube has a very modular architecture making it easy to custom functionality for a large variety of migration use-cases.\nThe project includes three tools:\nmove2kube: The primary tool is the command line interface (CLI) that takes in application source code and generates Kubernetes artifacts. move2kube-ui: A UI for interacting with the Move2Kube CLI tool for running fully-managed Move2Kube runtimes. move2kube-transformers: A collection of useful transformers for extending Move2Kube’s functionality that has been built by the Konveyor community based on experience from performing migrations for clients. Source\n","categories":"","description":"","excerpt":"The Move2Kube tool helps application owners migrate legacy workloads …","ref":"/docs/move2kube/","tags":"","title":"Move2Kube"},{"body":"Similar to the command line tool, the Move2Kube Web-UI can also perform the transformation with all the capabilities that are in the command line tool. This document explains the steps to bring up the UI and backend using Docker and use it for transformation.\nPrerequisites Install Docker.\nUse the language-platforms sample. The language-platforms file has a combination of multiple applications in different languages (Java, Go, Python, Ruby, etc.) which need to be containerized and then put into Kubernetes.\nUsing the UI to perform a transformation Download the language platforms sample as a zip.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/language-platforms -r move2kube-demos -z $ ls language-platforms.zip Run docker run --rm -it -p 8080:8080 quay.io/konveyor/move2kube-ui.\n$ docker run --rm -it -p 8080:8080 quay.io/konveyor/move2kube-ui INFO[0000] Starting Move2Kube API server at port: 8080 This starts a container using the Move2Kube UI image on port 8080.\n```console # Optionally if you need persistence then mount the current directory: $ docker run --rm -it -p 8080:8080 -v \"${PWD}/move2kube-api-data:/move2kube-api/data\" quay.io/konveyor/move2kube-ui:latest # And if you also need more advanced features of Move2Kube then mount the docker socket. This will allow Move2Kube to run container based transformers: $ docker run --rm -it -p 8080:8080 -v \"${PWD}/move2kube-api-data:/move2kube-api/data\" -v //var/run/docker.sock:/var/run/docker.sock quay.io/konveyormove2kube-ui:latest ``` Open http://localhost:8080 in a browser.\nClick the New Workspace button to create a new workspace named Workspace 1.\nScroll down and click the New Project button to create a new project named Project 1.\nScroll down to the Project inputs section and upload the language-platforms.zip file downloaded earlier in this tutorial and wait for it to finish uploading.\nScroll down to the Plan section and click on the Start Planning button.\nNote: Generating the plan takes about three to five minutes to generate the plan in YAML format.\nScroll to view the different services. Important: Click Save after all edits.\nScroll down to Outputs section and click on the Start Transformation button.\nMove2Kube will ask some questions to aid in the transfomation process.\nFor the container registry question, specify the container registry where you want to push the images.\nSame for the container registry namespace question.\nIf your container registry requires authentication for pulling images, specify that in the container registry login question.\nFor the ingress host question, specify the hostname of the Kubernetes cluster. If you are deploying to Minikube then specify localhost as the hostname and leave the TLS secret blank.\nFor all other questions, click the Next button to accept the default answers.\nClick on the output to download the generated artifacts as a zip file (here workspace-1-project-1-output-bcad1e64-23d0-4ea1-ad47-9d060e870b4f.zip), extract it, and browse them. The applications can now be deployed to Kubernetes using these generated artifacts.\nBuild and push the container images and deploy to Kubernetes using the downloaded output. The steps for doing that are same as the CLI tutorial.\nConclusion This tutorial shows how easy it is to performa a transformation using the UI. The UI can be hosted on a common server and used by different teams using different workspaces and also has authentication and authorization capabilities to restrict access to particular workspaces.\n[Source](https://github.com/konveyor/konveyor.github.io/blob/main/content/Move2Kube/Tutorials/usingUI.md\n","categories":"","description":"","excerpt":"Similar to the command line tool, the Move2Kube Web-UI can also …","ref":"/docs/move2kube/tutorials/usingui/","tags":"","title":"Using Move2Kube UI"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/appmodernization/","tags":"","title":"AppModernization"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Blog"},{"body":"Author: Savitha Raghunathan (GitHub)\nFor the past several weeks, I’ve been running through a series of migration scenarios,everything from JMS-based Message-Driven Beans to Quarkus Reactive Messaging, JDK 8 apps to Java 17, and Spring Boot 2 services to Spring Boot 3 with Jakarta EE. Each scenario is its own mini experiment. I curated a handful of flawless before/after code pairs, extracted their AST diffs, crafted a focused LLM prompt, then used that to get hints.\nThe above loop: “design hypothesis, feed the model examples, validate against real code, tweak prompts, repeat”,is the core of building Kai’s Generative AI hint component. It’s not “build feature X” so much as “run experiments until the model reliably reproduces our hard-won migrations.” In the sections below, you’ll shadow me through one of these R\u0026D cycles: spinning up a JMS→Quarkus scenario, framing the perfect prompt, and ultimately capturing the distilled insight into a reusable hint.\nNote: “Above image generated with AI”\nAs a part of Kai’s future roadmap, there is a huge emphasis on the Solution Server—a component designed to use the solved examples from previous successful migrations and apply that knowledge in similar new scenarios. In Konveyor’s migration workflows, migrators/architects write rules that identify a problem (for example, “this class uses JMS”), but these rules often lack the extra contextual information developers need to fix it efficiently. A migration rule must do two things:\nIdentify a problem Provide enough context so developers know exactly how to fix it This research addresses that second part by showing how to extract and generalize migration insights from a small set of solved examples. Migrating a Java EE codebase can derail timelines if every class needs manual annotation fixes, import corrections, or tweaks. I discovered that by researching how a small set of “good example” migrations worked, then asking an LLM model to distill that insight into a single, reusable hint, I was able to migrate Java classes with far less manual intervention.\nBelow, I’ll walk you through the research journey, give you a taste of the prompt outline, and show concise before/after snippets. Finally, we’ll see how this generated hint produces reliable results.\n1. From “Good Examples” to a Reusable Hint The starting point was:\n“I have a handful of Java EE JMS MDBs already migrated to Quarkus and they look perfect. How do I capture that pattern and apply it across various migrations?”\nI had several “before” Java EE classes (using @MessageDriven, implements MessageListener, legacy JMS imports) and their “after” Quarkus migrations (annotated with @Incoming(\"…\"), @ApplicationScoped, @Transactional, and valid MicroProfile/Jakarta imports). By studying these good examples, I aimed to use an LLM to generalize a hint that can be used in similar future migrations.\n1.1. High-Level Steps in Process With the goal of enriching rules with contextual guidance firmly in mind, here are the five stages of hint generation—a key step in our larger Solution Server architecture:\nCurate “Good Example” Pairs\nSelect a small set of Java EE → Quarkus migrations that compile perfectly (e.g., JMS MDBs already converted). Confirm each one uses correct Quarkus/MicroProfile imports, annotations, and naming conventions. Extract Structural Summaries \u0026 Diffs\nUse Kai’s extract_ast_info() to capture the “before” and “after” at a high level: which annotations disappeared, which interfaces dropped, which imports changed, and which annotations appeared. Compute a diff that distills those changes into operations like “remove @MessageDriven,” “remove implements MessageListener,” “add @ApplicationScoped,” and so on. Craft a Good Prompt\nGoal: Feed the LLM just enough context so it understands the pattern without overwhelming it with raw code. Prompt Blurb (example): You are an expert in migrating JMS MessageDriven beans (Java EE) to Quarkus SmallRye Reactive Messaging. Here is a reference migration summary: \u003cAST diff examples\u003e \u003cOriginal minimal hint\u003e **Task:** Take the above inputs and migrate the code to generate a hint that can assist in future migrations. \u003cformatting of the enhanced generated hint\u003e The prompt was iterated until it produced a hint that matched the expectations.\nValidate the Model’s Hint The original hint for migrating JMS elements to Quarkus was very minimal. Original minimal hint:\ndescription: References to JavaEE/JakartaEE JMS elements should be removed and replaced with their Quarkus SmallRye/MicroProfile equivalents. message: | References to JavaEE/JakartaEE JMS elements should be removed and replaced with their Quarkus SmallRye/MicroProfile equivalents. Using the examples and the prompt discussed earlier, the model ( in this case GPT-4o-mini) returned an enhanced hint.\nEnhanced Hint:\ndescription: Migrating Java EE MDBs and JMS to Quarkus-native messaging solutions message: | This migration focuses on replacing Java EE Message-Driven Beans (MDBs) and Java Message Service (JMS) usage with Quarkus-native equivalents, specifically using the Quarkus Messaging extension. The goal is to leverage Quarkus's reactive programming model and simplified configuration for improved performance and maintainability. Actionable Steps: * Step 1: Add the Quarkus Messaging extension to your project by including the following dependency in your `pom.xml` or `build.gradle`: \u003cdependency\u003e \u003cgroupId\u003eio.quarkus\u003c/groupId\u003e \u003cartifactId\u003equarkus-resteasy\u003c/artifactId\u003e \u003c/dependency\u003e * Step 2: Replace your MDBs with a Quarkus `@Incoming` method. Define a method in a CDI bean that listens to the desired queue or topic: @Incoming(\"your-queue-name\") public void processMessage(String message) { // Handle the incoming message } * Step 3: Configure your JMS connection settings in `application.properties`: quarkus.jms.connection-factory.my-connection-factory = your-connection-factory quarkus.jms.destination.your-queue-name = your-queue * Step 4: Remove any references to the old MDBs and JMS APIs from your codebase. Optional Changes: * Suggestion 1: Consider using Quarkus's reactive messaging capabilities for better scalability and performance. * Suggestion 2: Review your error handling strategy, as Quarkus provides different mechanisms for handling message processing failures. This enhanced hint was validated with other models and examples and consistently produced the expected, fully migrated code.\nGeneralize With this hint in hand, the same recipe could be reused for other JMS-based issues. This concept can be extended to any migration issue backed by a good set of before and after migration examples.\n2. Applying the Hint Once the hint was finalized, the next step was to test it with other models and examples. The below example is uses the above generated hint and the model, llama-3.1-8B-Instruct.\n2.1. Before / After Example (JMS → Quarkus)\nBefore:\nimport javax.jms.* public class InventoryNotificationMDB implements MessageListener { private static final int LOW_THRESHOLD = 50; @Inject private CatalogService catalogService; private final static String JNDI_FACTORY = \"weblogic.jndi.WLInitialContextFactory\"; private final static String JMS_FACTORY = \"TCF\"; private final static String TOPIC = \"topic/orders\"; private TopicConnection tcon; private TopicSession tsession; private TopicSubscriber tsubscriber; public void onMessage(Message rcvMessage) { TextMessage msg; { try { System.out.println(\"received message inventory\"); if (rcvMessage instanceof TextMessage) { msg = (TextMessage) rcvMessage; String orderStr = msg.getBody(String.class); Order order = Transformers.jsonToOrder(orderStr); order.getItemList().forEach(orderItem -\u003e { int old_quantity = catalogService.getCatalogItemById(orderItem.getProductId()).getInventory().getQuantity(); int new_quantity = old_quantity - orderItem.getQuantity(); if (new_quantity \u003c LOW_THRESHOLD) { System.out.println(\"Inventory for item \" + orderItem.getProductId() + \" is below threshold (\" + LOW_THRESHOLD + \"), contact supplier!\"); } else { orderItem.setQuantity(new_quantity); } }); } } catch (JMSException jmse) { System.err.println(\"An exception occurred: \" + jmse.getMessage()); } } } } After migration using the hint:\nimport jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Inject; import jakarta.transaction.Transactional; import org.eclipse.microprofile.reactive.messaging.Incoming; import java.math.BigDecimal; @ApplicationScoped public class InventoryNotification { private static final int LOW_THRESHOLD = 50; @Inject private CatalogService catalogService; @Incoming(\"orders-topic\") @Transactional public void processOrder(String orderStr) { Order order = Transformers.jsonToOrder(orderStr); order.getItemList().forEach(orderItem -\u003e { int oldQuantity = catalogService.getCatalogItemById(orderItem.getProductId()).getInventory().getQuantity(); int newQuantity = oldQuantity - orderItem.getQuantity(); if (newQuantity \u003c LOW_THRESHOLD) { System.out.println(\"Inventory for item \" + orderItem.getProductId() + \" is below threshold (\" + LOW_THRESHOLD + \"), contact supplier!\"); } else { orderItem.setQuantity(newQuantity); } }); } } Without the hint, migration might forget the @Transactional or miss dropping a javax.jms.* import, leading to compilation issues. With the enhanced hint, the model produced a code that matched the expectations.\nConclusion By starting with a curated set of solved migrations and extracting their structural differences, I used an LLM to generate a concise, reusable hint that addresses every necessary annotation change, import swap, and configuration tweak.Using the newly generated hint allowed me to migrate new Java EE MDBs (and similar classes) with minimal manual intervention.\nIf you’re interested in application modernization, AI-driven developer tools, or want to contribute to something exciting and community-led, come join us!\nLooking forward to your feedback, ideas, and collaboration as we grow Kai together!\n","categories":"","description":"","excerpt":"Author: Savitha Raghunathan (GitHub)\nFor the past several weeks, I’ve …","ref":"/blog/2025/hints_generation/","tags":["Konveyor","Kai","GenAI","GenAI R\u0026D","AppModernization"],"title":"From “Good Examples” to a Reusable Hint for App Modernization"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/genai/","tags":"","title":"GenAI"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/genai-rd/","tags":"","title":"GenAI R\u0026D"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kai/","tags":"","title":"Kai"},{"body":"","categories":"","description":"","excerpt":"","ref":"/","tags":"","title":"Konveyor"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/konveyor/","tags":"","title":"Konveyor"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"Author: Savitha Raghunathan, Jonathan Recinos\nAfter months of ideas, development, and feedback from early users and contributors, we’re excited to share the first official release of Konveyor AI 0.1.0.\nThis release is a significant step forward for the Konveyor project, and for anyone working through the complex journey of application modernization. It marks the beginning of developer-focused, AI-assisted modernization built with community input and open collaboration.\nWhat is Konveyor AI? Konveyor AI (Kai) provides real-time insights to help developers and migrators identify and refactor deprecated or outdated code. As a developer modernizes an application with Kai, they’ll see detailed issues and their location, solutions with the reasoning for it, and code suggestions that can be automatically implemented.\nBuilt on top of the Konveyor ecosystem, Kai brings:\nStatic code analysis: Identifies potential issues when migrating to new technologies (e.g., updating Java frameworks, containerizing an application).\nExtensible migration paths: Includes 2,400 predefined community contributed rules for different migration paths. Users can create their own custom rules for unique scenarios.\nVS Code extension: Suggested code changes show up within the IDE.\nModel-agnostic AI:\nUsers are not locked into a specific LLM, enabling flexibility in AI-driven modernization.\nAgentic AI for intelligent responses:\nDelivers more meaningful, sanitized suggestions, validating Maven compilations and dependency resolutions.\nVarying levels of AI assistance: Users can choose the level of issue complexity that Kai solves.\nKai can assist you with migrating from legacy Java, upgrading to Spring Boot 3, or prepping for a cloud native deployment. You can also build custom rulesets when needed.\nTry it out! The 0.1.0 release is available here!\nYou can take Kai for a spin using step-by-step guided scenarios.\nIf you are interested in learning how Kai works, here are some links to the technical blogs,\nKai - Generative AI Applied to Application Modernization Incident Storage in Kai - A Deep Dive Konveyor AI: Supporting Application Modernization The Konveyor project maintainers would like to thank the community for their continued support, feedback, and encouragement. Huge shoutout to the contributors (IDE extension, backend) who made this release possible. Thank you!\nIf you’re interested in application modernization, AI-driven developer tools, or want to contribute to something exciting and community-led, come join us!\nLooking forward to your feedback, ideas, and collaboration as we grow Kai together!\n","categories":"","description":"After months of ideas, development, and feedback from early users and contributors, we're excited to share the first official release of Konveyor AI 0.1.0. Please download it and try it out in your local environment. Report your issues and feature requests, and contribute fixes and documentation if you can!","excerpt":"After months of ideas, development, and feedback from early users and …","ref":"/blog/2025/kai-release-announcement/","tags":["Konveyor","Kai","GenAI","AppModernization"],"title":"Konveyor AI 0.1.0 is here!"},{"body":"","categories":"","description":"","excerpt":"","ref":"/announcements/","tags":"","title":"Announcements"},{"body":"Author: Jonah Sussman (GitHub, LinkedIn, E-mail)\nIntroduction The process of modernizing an application is complex and time-consuming. Despite our best efforts as software engineers with things like semantic versioning, detailed release notes, and thorough testing, modernizing a single application is an intimidating task.\nThis kind of work is often one of the most unglamorous, and error-prone tasks that developers have to do. It involves rewriting large portions of the codebase, which can be daunting for even the most battle-hardened of developers. There’s an interconnected web of functions, classes, and modules that all depend on each other. One wrong move and the whole thing can come crashing down.\nNow imagine you have to do this across tens or hundreds of applications.\nYeesh.\nEnter: Kai Kai is an AI-enabled tool that assists with modernizing applications. Kai is designed to help developers write code more efficiently by providing suggestions and solutions to common problems. It does this by using analysis reports from Konveyor about the codebase and generating solutions based on what it finds.\nNow, you may be thinking: How is Kai different than other generative AI tools? I’m so glad you asked.\nKonveyor’s analysis reports: Konveyor generates analysis reports throughout a migration. This history of reports tells you what’s wrong with your codebase, where the issues are, and when they happened. Best of all, this functionality exists today. Developers are already doing this during a migration! As a result… Kai can learn throughout a migration: As you migrate more pieces of your codebase with Kai, it can learn from the data available, and get better recommendations for the next application, and the next, and so on. This leads to the fact that… Kai is more focused: LLMs are very powerful tools, but without explicit guidance, they can generate a lot of garbage. Using Konveyor’s analysis reports allows us to focus Kai’s generative power on the specific problems that need to be solved. Pointed, specific data is the key to unlocking the full potential of large language models. Recently, we overhauled how incidents are handled inside Kai. I’m going to take you on a guided tour through these changes, and learn how Kai operates in a deep, technical level.\nAn Initial Migration Let’s imagine we’re a developer for Bloopify Solutions. Stick with me here, there’s going to be a fair amount of code snippets. You can follow along with the migration using this repository. Now, suppose we have the following Java application:\npackage net.jsussman.ioedict; import java.io.FileInputStream; import java.io.FileOutputStream; import java.io.IOException; public class App { public static void main(String[] args) { System.out.println(\"Starting application!\"); try (FileInputStream in = new FileInputStream(\"input.txt\"); FileOutputStream out = new FileOutputStream(\"output.txt\")) { int c; while ((c = in.read()) != -1) { out.write(c); } } catch (IOException e) { e.printStackTrace(); } } } Beautiful, isn’t it? Now, let’s say we have received an edict from on high that we can longer use old-school input/output from the Java IO package and standard logging via System.out. We decide to write a couple of rules to enforce this:\n- ruleID: blog-post-demo-0001 category: mandatory description: Avoid old-school input/output effort: 5 message: Legacy I/O should be avoided. Consider using NIO (java.nio) instead. when: java.referenced: location: CONSTRUCTOR_CALL pattern: java.io.* - ruleID: blog-post-demo-0002 category: mandatory description: Avoid standard logging effort: 5 message: Standard logging should be avoided. Consider using SLF4J instead. when: java.referenced: pattern: \"System*\" We feed this to Konveyor and presto! It generates an analysis report back for us:\n- name: kai/blog-post description: Rules focused on demonstrating the capabilities of Kai violations: blog-post-demo-0001: description: Avoid old-school input/output category: mandatory incidents: - uri: file:///opt/input/source/src/main/java/net/jsussman/ioedict/App.java message: Legacy I/O should be avoided. Consider using NIO (java.nio) instead. codeSnip: # ... code snippet here ... lineNumber: 11 variables: file: file:///opt/input/source/src/main/java/net/jsussman/ioedict/App.java kind: Constructor name: main package: net.jsussman.ioedict - uri: file:///opt/input/source/src/main/java/net/jsussman/ioedict/App.java message: Legacy I/O should be avoided. Consider using NIO (java.nio) instead. codeSnip: # ... code snippet here ... lineNumber: 12 variables: file: file:///opt/input/source/src/main/java/net/jsussman/ioedict/App.java kind: Constructor name: main package: net.jsussman.ioedict effort: 5 blog-post-demo-0002: description: Avoid standard logging category: mandatory incidents: - uri: file:///opt/input/source/src/main/java/net/jsussman/ioedict/App.java message: Standard logging should be avoided. Consider using SLF4J instead. codeSnip: # ... code snippet here ... lineNumber: 9 variables: file: file:///opt/input/source/src/main/java/net/jsussman/ioedict/App.java kind: Method name: main package: net.jsussman.ioedict effort: 5 Look at that! Each rule was violated, with two incidents for the io and one for the logging. Now, let’s fix the io issue:\npackage net.jsussman.ioedict; import java.io.IOException; import java.nio.file.Files; import java.nio.file.Path; import java.nio.file.Paths; public class App { public static void main(String[] args) { System.out.println(\"Starting application!\"); Path source = Paths.get(\"input.txt\"); Path destination = Paths.get(\"output.txt\"); try { Files.copy(source, destination); } catch (IOException e) { e.printStackTrace(); } } } I promise we’re going to get to the good stuff soon. Finally, Konveyor generates another report for us:\n- name: kai/blog-post description: Rules focused on demonstrating the capabilities of Kai violations: blog-post-demo-0002: description: Avoid standard logging category: mandatory incidents: - uri: file:///opt/input/source/src/main/java/net/jsussman/ioedict/App.java message: Standard logging should be avoided. Consider using SLF4J instead. codeSnip: # ... code snippet here ... lineNumber: 10 variables: file: file:///opt/input/source/src/main/java/net/jsussman/ioedict/App.java kind: Method name: main package: net.jsussman.ioedict effort: 5 unmatched: - blog-post-demo-0001 We solved the io issue, but left the logging issue (this will be important for later). Now, let’s see how Kai uses this data to generate solutions.\nThe Solution Pipeline At the core of Konveyor’s analysis reports (and by extension, Kai) is the humble incident. An incident is a code location in a file that signals something is wrong. It could be a bug, a security vulnerability, or a code smell.\nYou can view Kai’s input as a snapshot of code at a particular point in time, and the incidents associated with it. It’s Kai’s job to use these incidents and generate solutions for them. Solutions are generated as a part of a highly-configurable pipeline, consisting of:\nSolution Detectors: Classify incidents as unsolved, solved, or new. Solution Producers: Generate a solution for a solved incident. Solution Consumers: Turn the solution into something that an LLM can use. Solution Detection At a fundamental level, the job of a solution detector is to classify which incidents are unsolved, solved, or new. It’s given the old and new Report objects, as well as references to the git repo before and after the changes. They then perform their specific detection algorithm and spit out the classified incidents.\nCurrently, there are two types of detectors:\nnaive: Incidents are considered “the same” if everything matches. line_match: Incidents on different lines that are merely moved or can be matched to some other area of the AST are considered the same as well. Returning to our hypothetical job at Bloopify Solutions, let’s see what the database of incidents looks like after loading the project before fixing the io issue:\nThis is exactly what we expect. The report has been loaded successfully, and there are no solutions attached to the incidents. Now, let’s see what it looks like after loading the project after fixing the io issue with the line_match solution detector:\nKai has correctly identified that the io issue has been fixed and marked both incidents as solved. The logging issue remains unsolved. (Even though it was on a different line!)\nNow that we’ve seen Kai correctly mark which incidents are solved, let’s see how it produces solutions.\nSolution Producers Solution producers take an incident that’s been solved and generate a solution. Solutions are stored in Kai as flexible JSON, allowing for growth as the application evolves. A user can opt into and out of different solution detectors, producers, and consumers very fluidly, leaving maximum configurability for how we handle it on the table.\nCurrently, there are two types of producers:\ntext_only: The solution only contains a diff of the source code from the original to the updated version. llm_lazy: The same as text_only as well as the solution is earmarked for LLM summary generation. LLM summary is the generated only when the solution is needed, i.e. lazily. Let’s take a look at the solutions generated by Kai for one of the incidents of the io issue:\n{ \"uri\": \"file:///opt/input/source/src/main/java/net/jsussman/ioedict/App.java\", \"generated_at\": \"2024-08-22T17:35:31.979627\", \"file_diff\": \"diff --git ... rest of diff here ...\", \"repo_diff\": null, \"original_code\": \"... full original code here ...\", \"updated_code\": \"... full updated code here ...\", \"llm_summary_generated\": false, \"llm_summary\": null } We can see that the solution contains a variety of information, including the diff between the old and new code. Additionally, since we selected the llm_lazy producer, the solution is earmarked for LLM summary generation (note that llm_summary_generated is false and not null).\nThis is fantastic. We have a solution for the io issue that we can use to generate solutions to future incidents of this problem. Now, let’s see how we can do that.\nSolution Consumers The history of solutions is stored in the database. During a migration, we can search this database for potential solutions to our problem. Once the solution is found, we consume it by turning it into something that an LLM can use. Right now, all producers take in a solution and build a string that can be pasted into the prompt.\nAdditionally, consumers can perform additional post-processing work once a solution is ready to be used. This is so that we only have to do as much work as we need; we don’t want to hammer our LLM API endpoints.\nIt’s also through this mechanism that we can implement the next phase of Kai, so stay tuned!.\nGoing back to our migration at Bloopify Solutions, let’s say we have this separate application:\npackage io.konveyor.filer; import java.io.File; import java.io.FileInputStream; import java.io.FileOutputStream; import java.io.IOException; public class App { public static void main(String[] args) { File sourceDir = new File(\"source_directory\"); File destDir = new File(\"destination_directory\"); try { copyDirectoryLegacyIO(sourceDir, destDir); System.out.println(\"Directory copied successfully using legacy I/O.\"); } catch (IOException e) { e.printStackTrace(); } } public static void copyDirectoryLegacyIO(File source, File destination) throws IOException { if (source.isDirectory()) { if (!destination.exists()) { destination.mkdirs(); // Create destination directory if it doesn't exist } String[] children = source.list(); if (children != null) { for (String child : children) { copyDirectoryLegacyIO(new File(source, child), new File(destination, child)); } } } else { copyFileLegacyIO(source, destination); } } private static void copyFileLegacyIO(File source, File destination) throws IOException { try (FileInputStream in = new FileInputStream(source); FileOutputStream out = new FileOutputStream(destination)) { byte[] buffer = new byte[1024]; int bytesRead; while ((bytesRead = in.read(buffer)) != -1) { out.write(buffer, 0, bytesRead); } } } } Konveyor comes back to us with the following report (get ready for a doozy of a report):\n- name: kai/blog-post description: Rules focused on demonstrating the capabilities of Kai violations: blog-post-demo-0001: description: Avoid old-school input/output category: mandatory incidents: - uri: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java message: Legacy I/O should be avoided. Consider using NIO (java.nio) instead. codeSnip: # ... code snippet here ... lineNumber: 10 variables: file: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java kind: Constructor name: main package: io.konveyor.filer - uri: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java message: Legacy I/O should be avoided. Consider using NIO (java.nio) instead. codeSnip: # ... code snippet here ... lineNumber: 11 variables: file: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java kind: Constructor name: main package: io.konveyor.filer - uri: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java message: Legacy I/O should be avoided. Consider using NIO (java.nio) instead. codeSnip: # ... code snippet here ... lineNumber: 29 variables: file: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java kind: Constructor name: copyDirectoryLegacyIO package: io.konveyor.filer - uri: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java message: Legacy I/O should be avoided. Consider using NIO (java.nio) instead. codeSnip: # ... code snippet here ... lineNumber: 38 variables: file: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java kind: Constructor name: copyFileLegacyIO package: io.konveyor.filer - uri: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java message: Legacy I/O should be avoided. Consider using NIO (java.nio) instead. codeSnip: # ... code snippet here ... lineNumber: 39 variables: file: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java kind: Constructor name: copyFileLegacyIO package: io.konveyor.filer effort: 5 blog-post-demo-0002: description: Avoid standard logging category: mandatory incidents: - uri: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java message: Standard logging should be avoided. Consider using SLF4J instead. codeSnip: # ... code snippet here ... lineNumber: 15 variables: file: file:///opt/input/source/src/main/java/io/konveyor/filer/App.java kind: Method name: main package: io.konveyor.filer effort: 5 Ouch! Migrating this by hand would certainly be a pain. But now, Kai is loaded with examples of how to fix these issues. Let’s make a request to Kai to generate a solution for the io issue. First, it generates the LLM summary for the solutions it’s using:\n## Reasoning 1. **Identify the problem**: The incident report states that legacy I/O should be avoided and NIO should be used instead. The code in question is using Java's legacy I/O classes `FileInputStream` and `FileOutputStream` to read and write files. 2. **Understand the solution**: The solution provided uses the `java.nio` package, specifically the `Files` class, to copy the contents of one file to another. This is a more modern and efficient way to handle I/O operations. 3. **Implement the solution**: To implement this solution, we first import the necessary classes from the `java.nio.file` package. We then replace the `FileInputStream` and `FileOutputStream` with the `Paths` and `Files` classes. The `Paths.get()` method is used to get the file paths, and the `Files.copy()` method is used to copy the contents of the source file to the destination file. ## Additional Information The `Files.copy()` method throws a `IOException` if an I/O error occurs, so we wrap it in a try-catch block to handle this exception. This is similar to how the original code handles exceptions thrown by the `read()` method of the `FileInputStream`. The `Files` class also provides other useful methods for working with files, such as `Files.exists()`, `Files.delete()`, and `Files.move()`, which can be used for various file management tasks. It's also worth noting that the `Files.copy()` method can be used to copy a file to a new location, or to replace an existing file. If the destination file already exists, it will be deleted before the copy operation begins. If you want to keep the existing file and append to it instead, you can use the `Files.newOutputStream()` method with the `APPEND` option. And it comes back with the following code:\npackage io.konveyor.filer; import java.io.IOException; import java.nio.file.*; import java.nio.file.attribute.BasicFileAttributes; public class App { public static void main(String[] args) { Path sourceDir = Paths.get(\"source_directory\"); Path destDir = Paths.get(\"destination_directory\"); try { copyDirectoryNIO(sourceDir, destDir); System.out.println(\"Directory copied successfully using NIO.\"); } catch (IOException e) { e.printStackTrace(); } } public static void copyDirectoryNIO(Path source, Path destination) throws IOException { if (Files.notExists(destination)) { Files.createDirectories(destination); } Files.walkFileTree(source, new SimpleFileVisitor\u003cPath\u003e() { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { Path destFile = destination.resolve(source.relativize(file)); Files.copy(file, destFile, StandardCopyOption.REPLACE_EXISTING); return FileVisitResult.CONTINUE; } @Override public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException { Path destDir = destination.resolve(source.relativize(dir)); if (Files.notExists(destDir)) { Files.createDirectories(destDir); } return FileVisitResult.CONTINUE; } }); } } This is fantastic! We have a solution for the io issue for this entirely separate application, based on the history of solutions Kai has generated.\nConclusion In conclusion, Kai is a powerful tool that leverages Konveyor’s analysis reports to generate solutions for common problems in codebases. By using pointed, specific data, Kai can focus its generative power on the specific problems that need to be solved. This allows developers to write code more efficiently and modernize their applications with confidence.\nI hope this deep dive into Kai’s incident storage has given you a better understanding of how Kai operates and how it can be used to improve the generation of results. If you have any questions or would like to learn more about Kai, feel free to reach out to me on GitHub or even contribute to the project at Konveyor’s GitHub.\n","categories":"","description":"A walkthrough of how incident data is leveraged in Konveyor AI's (Kai)  workflow to improve the generation of results.","excerpt":"A walkthrough of how incident data is leveraged in Konveyor AI's (Kai) …","ref":"/blog/2024/kai-incident-storage-2024/","tags":["Konveyor","Kai","GenAI","AppModernization"],"title":"Incident Storage in Kai - A Deep Dive"},{"body":"Author: John Matthews\nThis post shares the roadmap for Konveyor AI (Kai) as of summer 2024. For a recap of what is Konveyor AI (Kai) please see our prior post: Kai - Generative AI Applied to Application Modernization\nThe roadmap is organized by themes of functionality, each focusing on a specific aspect of the project’s development.\nRoadmap Outline:\nPurpose Maturity Guiding Principles Themes Milestones Future Areas to Consider Access to Early Builds Purpose What is the purpose of Konveyor AI? Kai intends to improve the economics of re-platforming and refactoring applications to Kubernetes and cloud-native technologies via use of Generative AI leveraging data in Konveyor.\nMaturity Maturity - Early Development - Pre Alpha Kai is in early stages of development and is NOT suitable for production usage at this time.\nPlease see docs/Evaluation_Builds.md to learn more about early access preview builds. Contributions are encouraged and most welcome, for more information see CONTRIBUTING.md Guiding Principles Model agnostic - Bring Your Own Model We recognize the rapid pace of evolving Large Language Models (LLM), for that reason the team is approaching implementation tasks in a manner to allow swappable artifacts to aid the adoption of various LLM providers.\nAn important aspect of the project’s mission is to empower the end user to experiment with various LLMs and ultimately choose which model they want to leverage. Embrace a Learning Mindset for Experiments to Solidify Knowledge Konveyor AI is committed to a learning mindset in regard to feature development. We recognize that not all features will have an immediate beneficial impact, and the nature of this domain involves making small bets to explore different approaches. The team will focus on experimentation to understand what works and what doesn’t, thereby improving the quality of results.\nThe development team contributing to Konveyor AI acknowledges that this is a rapidly evolving field. To succeed, we must invest in experiments to solidify our understanding, identify effective strategies, and share knowledge from this learning with the community.\nOur approach includes:\nUtilizing Jupyter notebooks to prototype and share new feature experiments, allowing others to explore the methodologies. Including both successful and failed experiments in our project resources to share knowledge and learnings. Implementing feature gates to enable or disable experimental features as needed. Providing configuration options to adjust algorithmic choices and parameters as well as prompt templating to allow rapid prototyping. Themes The below themes are areas of improvement the team has identified for future work.\nUse Konveyor Data to Improve Generated Results Konveyor Integration Repository Level Understanding and Code Generation IDE Integrations User Experience Improvements External Tool Integrations Evaluation Tools to Benchmark Results Scenario Creation to Showcase Capabilities InstructLab Integrations to Aid Fine Tuning Use Konveyor Data to improve generated results Improve the Retrieval Augmented Generation (RAG) approach which uses application modernization data inside of Konveyor to aid shaping code generations for better alignment of how an organization has solved that given problem in the past. Measure confidence of generated results. Track the acceptance | rejection rate of generated results vs known analysis incidents to give end user a sense of confidence for a given generation. Konveyor Integration Sync Konveyor’s application modernization data with Kai’s database Deploy Kai from Konveyor’s Operator Integrate Kai so it is accessible via Konveyor’s endpoint and integrated with Konveyor’s Auth Repository Level Code Generation Techniques to overcome limited context windows with LLM requests to facilitate a broader repository level understanding of code flows and modifications IDE Integrations IDE Integrations VSCode (1st IDE to Integrate) Intelli-J (2nd IDE to Integrate) Eclipse (3rd IDE to Integrate) Real-time updates of static code analysis Integrate analyzer-lsp into the IDE with the ability to perform updates of static code analysis information as each file is modified. Improve UX of IDE Plugin For VSCode IDE Plugin move the Kai specifics out of a separate ‘Kai’ view into the main ‘Explorer’ view User Experience Improvements Developer Workflow Experience Establish a incremental migration experience to aid the scenario of translating an application from one technology to another in a manageable/incremental manner. Create a ChatBot interface to provide an additional method of interaction via a conversational experience External Tool integrations Integrate external tools such as linters, compilers, execution of tests in an agent workflow to iterate and improve on a generated code snippet. Evaluation Tools to Benchmark Results Establish a means of evaluating the quality of generated results to aid learning from experiments Scenario Creation to Showcase Capabilities In order to show the capabilities of Kai artifacts are required such as multiple sample applications and analysis rules. For example, Kai has begun with a scenario of showing Java EE to Quarkus, yet other technologies are possible dependent on the availability of sample applications and analysis rules. InstructLab Integrations to aid Fine Tuning InstructLab is an open-source community focused on aiding fine-tuning of LLMs\nWhile fine-tuning of a LLM is out of scope for Kai itself and is NOT required, we recognize that the community will benefit from leveraging data inside of Konveyor and Kai to aid fine tuning code models for improved modernization activities. Milestones Kai releases will be considered independent of Konveyor releases for the initial period of functionality development. We expect to see Kai releases tied to Konveyor releases in 2025.\nAugust 2024: Prototype October 2024: Kai v0.1.0 Release December 2024: Kai v0.2.0 Release February 2025: Kai v0.3.0 Release 2024 - August: Prototype Summary The prototype is able to sync data from Konveyor or use existing sample data to aid a scenario of Java EE to Quarkus migration. A minimal workflow is provided to aid early evalations by interested contributors.\nWorkflow is focused at level of a single file, i.e. lacks awareness of full context of repository Minimal IDE integration in VSCode, workflow is functional to show a happy path IDE integrates Kantra to find modernization issues via static analysis Forms a prompt to request an updated code fix via a LLM. Augments the prompt with 2 sources of extra data Analysis Hints from Kantra [Optional] information of how similar problems were solved in past. Key Deliverables Konveyor Integration Add the ability to ingest data from Konveyor Hub, able to pull data from Konveyor 0.5.0 A separate process will run that continuously syncs data from Konveyor into Kai Use Konveyor data to improve generated results Analysis Hints - Contextual information associated with Analysis Rules that provide information of a specific incident Prior patterns of solved examples similar to a given incident Introduce ‘Solved Incident Store’ component which will allow Kai to see how an organization has solved a similar analysis problem in the past. The component will work with a LLM to summarize the key pieces of how a similar solution was solved by the organization in the past and include that summarization to help the LLM steer the generated solution closer to a desired result IDE Integration VSCode IDE Integration: Identification of existing modernization issues via static code analyis from Kantra in VSCode IDE Limitation: Analysis will NOT automatically update. Kantra analysis needs to be manually re-run to refresh analysis information. Workflow is the minimal needed to show happy-path, more work is needed before this is ready for the end user October 2024: Kai v0.1.0 Release Summary The v0.1.0 release is focused on improving the user experience from the IDE by providing quicker analysis updates as each file is saved. Additionally, Kai will be integrated into Konveyor, with the ability to be deployed from the Konveyor Operator.\nKey Deliverables Konveyor Integration Kai is deployed within Konveyor from the Konveyor Operator, tentative Konveyor 0.6.0 target IDE is able to communicate with the remote Kai service which is integrated behind Konveyor’s auth IDE Integration VSCode IDE Integration: Real-time updates of analysis information via replacement of Kantra with analyzer-lsp Benefit: Analysis information will be updated automatically as a file is modified. Evaluation Tools to Benchmark Results Establish a benchmark tool to aid scoring generated results against test data. This is first step to aid a metric driven approach to measure experiments to guide result improvements. December 2024: Kai v0.2.0 Release Summary The v0.2.0 release is focused on improving the quality of generated results by establishing an agent workflow that incorporates external tools to iterate on improving a code snippet with a LLM.\nKey Deliverables External Tool integrations Agent Workflow leveraging tools and iterate with LLM to address errors and improve results February 2025: Kai v0.3.0 Release Summary The v0.3.0 release is focused on teaching Kai how to understand repercussions of changes that cascade throughout a repository.\nKey Deliverables Repository Level Code Generation Agent workflow to develop a plan for how to address edits that span throughout the repository, i.e. code changes that have an impact beyond the scope of a single file. Future Areas to Consider The below are additional areas of interest for Konveyor AI, yet are not inline for immediate work. We consider the below as inspirational areas to consider but are not actively working.\nCode Explanation/Summarization Test Generation Generation of OpenRewrite Recipes from Analysis Rules Generate analysis rules based on documentation and changelogs Access to Early Builds Thank you for reading our roadmap plans, please note that Konveyor AI (Kai) is in early development and is not suitable for production usage at this time.\nIf you would like to take a peak at early preview builds visit: https://github.com/konveyor/kai\nFor those who are interested in contributing, please see our CONTRIBUTING.md guide.\nLastly, you may reach us at slack, email, or attending an open community meeting for any questions.\n","categories":"","description":"Konveyor AI (Kai) Roadmap 2024","excerpt":"Konveyor AI (Kai) Roadmap 2024","ref":"/blog/2024/kai-roadmap-2024/","tags":["Konveyor","Kai","GenAI","AppModernization"],"title":"Konveyor AI (Kai) Roadmap 2024"},{"body":"Author: John Matthews\nOverview Konveyor AI or “Kai”, is an early effort of Generative AI applied to Application Modernization being explored under the Konveyor Ecosystem at konveyor-ecosystem/kai.\nKai implements a Retrieval Augmented Generation (RAG) approach that leverages data from Konveyor to help generate code suggestions to aid migrating legacy code bases to a different technology. The intent of this RAG approach is to shape the code suggestions to be similar to how an organization has solved problems in the past, without additional fine-tuning of the model.\nDemonstration Video The team has explored a use case of a Java EE application migrating to Quarkus for it’s first example. Kai can be applied to other domains beyond Java EE -\u003e Quarkus, the only requirement is that analyzer-lsp supports the language and there are sufficient rulesets defined for the target.\nBelow is a high level view of where Kai fits into a typical large scale modernization engagement. What does Kai provide? Konveyor extended with Kai allows developers to work in their IDE to see analysis information and request code suggestions to resolve those migration issues.\nKai’s basic workflow from an IDE is:\nDiscover migrations issues via static code analysis Generate a fix for migration issues Access static code analysis information from Konveyor’s analzyer-lsp inside the IDE For a given issue, ask Kai to generate a code suggestion to solve the migration issue How does Kai work? Kai’s basic workflow is:\nIdentify migration ‘issues’ via static code analysis Look to see how the organization has solved similar problems in past, we call this a ‘Solved Example’ Extract enough contextual info from a ‘Solved Example’ that we can provide the LLM with guidance of how we want this current problem solved Work with a LLM to generate a code suggestion, giving it both the Analysis Information and any extra contextual info from how the organization has solved this problem in the past Surface the code suggestion either via an API call or in a developers IDE RAG Approach Kai uses 2 types of information to help inform the LLM of additional context to improve a response.\nStatic Code Analysis Information Solved Examples Static Code Analysis Information Analysis information is the result of running analyzer-lsp with a set of rules that help discover points of interest when migrating an application to a new technology. These rules may leverage the ~2400+ community contributed rules at konveyor/rulesets or may be organization specific custom rules covering information on proprietary corporate frameworks.\nWe use the analysis information for:\nIdentifying what areas of code need to be updated to move to a new technology Guidance to inform a developer what the issue is and hints of how they may resolve the problem For example we can look at the below snippet to see an example of the YAML data from a specific analysis issue informing the developer about a concern when moving from JMS to Quarkus’ reactive messaging:\nincidents: - uri: file:///src/main/....../service/InventoryNotificationMDB.java message: \"JMS `Topic`s should be replaced with Micrometer `Emitter`s feeding a Channel. See the following example of migrating\\n a Topic to an Emitter:\\n \\n Before:\\n ```\\n @Resource(lookup = \\\"java:/topic/HELLOWORLDMDBTopic\\\")\\n private Topic topic;\\n ```\\n \\n After:\\n ```\\n @Inject\\n @Channel(\\\"HELLOWORLDMDBTopic\\\")\\n Emitter\u003cString\u003e topicEmitter;\\n ```\" lineNumber: 60 Additionally, this analysis information is available in a more human friendly WebUI view Solved Examples What do we mean by ‘Solved Examples’ Application Modernization engagements inside a large organization typically encompass ~100s of applications that need to be migrated to a new technology. Often these applications share a large number of similar issues. As an organization successfully migrates a handful of applications they begin to encounter repeated patterns of issues they have already solved. One of the big goals with Kai is helping an organization tap into these prior solutions and leverage them for new migration needs, hence what we refer to as a ‘Solved Example’.\nHow do we determine if we have a ‘Solved Example’ for a given Analysis Incident Kai will help address similar migration problems by using the data inside of Konveyor which has an organization wide view of the entire application portfolio. Kai is able to find occurrences of when an application previously had the same problem and then was successfully migrated.\nUse of ‘Solved Examples’ in a Few Shot Prompt Once Kai has found how a similar problem has been solved elsewhere in the organization it extracts part of that information and gives it to the LLM to help consider as a ‘few shot’ example.\nLLM Specific Concerns As the team began working with LLMs we identified a few concerns that influenced our approach with Kai.\nLimited Context Size in LLMs Need to handle changes that cascade throughout a repository Desire to work with knowledge not in the model (an organization’s custom frameworks) Model capabilities are quickly improving Iterate on responses with a LLM to improve the solution #1 Limited Context Size in LLMs LLMs have a limitation on the size of data they will consider when forming a response, this is called their context size. This limitation makes it impractical to ingest an entire source code repository into each request for most models. Kai approaches this limitation by leveraging Konveyor’s static code analysis to discover migration issues and use those identified migration issues as the natural boundaries for scoping the problem to smaller subsets. #2 How to handle changes that cascade throughout a repository The first iteration of Kai is focused on splitting work into impacted files as identified from source code analysis. These impacted files are then run through Kai and an updated file is produced. We quickly identified that this approach was lacking the ability to understand changes that will ripple or cascade throughout a repository, such as changing the signature of a method and needing to update each place it is called in external files.\nThis area of handling cascaded changes is what we refer to as ‘Phase 2’ and is the next set of development efforts the team is undertaking. Phase 2, inspired by the work of Microsoft in their CodePlan: Repository-level Coding using LLMs and Planning paper involves cascading changes throughout a repository of code. We can detect what the changed code “touches” by leveraging the LSP server (similar to what we already use in analyzer-lsp). Each of those changes is then fed back into the algorithm, over and over, until no more changes are necessary.\n#3 Desire to work with knowledge not in the model The nature of modernization engagements in organizations is that a large number of the issues faced deal with internal propietary frameworks. It is unlikely that an existing model will have data on these frameworks. We are leveraging the RAG pattern described previously to help mitigate this concern by supplying few-shot prompt examples with extra contextual information of how the organization has solved this problem in the past.\nGoing beyond this RAG pattern we see future work paths to ease the integration of exposing Konveyor’s data into on-premise AI platforms such as Open Data Hub. This integration would support an organization’s ability for fine tuning a local model on the data they have collected in Konveyor.\n#4 Model capabilities are quickly improving We’ve built Kai to be model agnostic so organizations may experiment with new models as they are released.\nKai may be configured with coordinates to a LLM provider, allowing for use with public AI providers, internally hosted LLMs, or even local LLMs. We introduced a concept of being able to tweak generated prompts based on the ‘family’ of LLM being used to help facilitate freedom with exploring various models.\n#5 Iterate on responses with a LLM to improve the solution We are building the notion of an Agent into Kai to help improve the quality of a LLM’s final solution by working with the LLM to iterate on a smaller scoped solution and then running that solution through several tools to help check the validity and go back to the LLM to address problems found.\nNext Steps? The team is working towards integration with Konveyor in Summer 2024, yet plans to remain in the konveyor-ecosystem for a few more months as the solution is implemented and improved, we expect to pursue being an official Konveyor component later in 2024.\nRepositories:\nhttps://github.com/konveyor-ecosystem/kai https://github.com/konveyor-ecosystem/kai-vscode-plugin If you are interested to collaborate or have questions, consider:\nJoining our biweekly community calls Filing a GitHub Issue - https://github.com/konveyor-ecosystem/kai/issues Send an email to the konveyor-dev@googlegroups.com mailing list Chat with us at kubernetes.slack.com #konveyor For slack invites - Join Kubernetes on Slack ","categories":"","description":"Konveyor AI (Kai) Introduction","excerpt":"Konveyor AI (Kai) Introduction","ref":"/blog/2024/kai-deep-dive-2024/","tags":["Konveyor","Kai","GenAI","AppModernization"],"title":"Kai - Generative AI Applied to Application Modernization"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cncf/","tags":"","title":"CNCF"},{"body":"Author: Savitha Raghunathan\nIf you’re planning to attend KubeCon + CloudNativeCon Europe we are eager to meet you!! You’ll have multiple chances to engage with the Konveyor project and connect with other members of the Konveyor community. This event provides an excellent opportunity to ask questions, interact with the Konveyor team in person, provide feedback, and build connections within the Konveyor community. We are looking forward to meeting you there!\nKubeCon + CloudNativeCon Project Pavilion Be sure to visit our kiosk at PP3-A in the project pavilion to learn more about Konveyor and pick up some cool swag and stickers!\nProject Pavilion Hours Thursady, Mar 21\nPart-time - PM Friday, Mar 22\nAll Day Lightning Talk Join us on Tuesday, March 19 @ 12:00 to know about the groundbreaking developments shaping the future of application modernization with Konveyor.\nWant to learn more about Konveyor before Kubecon? Here’s everything you need to know!\nGet Involved To stay updated on the latest Konveyor news, join our community meetings where you can learn more about upcoming features and share your insights. Additionally, you can subscribe to our mailing list for the latest updates on Konveyor or reach out to us in #konveyor on kubernetes.slack.com.\nSee you at KubeCon!\n","categories":"","description":"If you're planning to attend KubeCon + CloudNativeCon Europe 2024, we're excited to meet you! There are several opportunities for you to engage with the Konveyor project and connect with other members of the Konveyor community. This event provides an excellent opportunity to ask questions, interact with the Konveyor team in person, provide feedback, and build connections within the Konveyor community. We are looking forward to meeting you there!","excerpt":"If you're planning to attend KubeCon + CloudNativeCon Europe 2024, …","ref":"/blog/2024/konveyor-kiosk-at-kubecon-europe-2024/","tags":["CNCF","Konveyor","KubeCon","AppModernization"],"title":"Konveyor at KubeCon + CloudNativeCon 2024 EU Paris"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubecon/","tags":"","title":"KubeCon"},{"body":"Organizations take a variety of different approaches to modernizing their applications as they target improving security, reliability, and scalability. Relative to our last survey, two years ago, we’re seeing a greater emphasis on updating legacy applications and infrastructure over building new cloud applications.\nAbout the survey Konveyor community member Red Hat partnered with research firm Illuminas to better understand how organizations plan to approach application modernization and migration—and what they consider success to look like. In all, 1,000 responses were gathered with half coming from the US and the balance split evenly between the United Kingdom (UK) and English-speaking Asia-Pacific (APAC). Half of the respondents were IT decision makers, a quarter were back-end developers, and a quarter software architects. Most respondents were large enterprises with more than 5,000 employees with the rest working for medium enterprises. A variety of industries including retail, software development, finance, and telecoms were represented.\nThis survey was conducted in October and November of 2023. An earlier version was conducted in 2021. Many findings are similar across the two surveys. However, because we made a fair number of methodology adjustments, where there are differences we’re just going to highlight a few major changes.\nWith that background, let’s dive into some of the details about what we found.\nWhy are organizations modernizing? The primary drivers of application modernization—security, reliability, and scalability—are familiar. These drivers, or variants of them, pop up in many of our surveys when we ask questions about things like the expected benefits of digital transformation. Survey respondents almost universally said all three of these drivers were factors in their organization’s decision to modernize their applications. Over 70% also evaluate success based on the results in the same 3 areas.\nHow are they doing? Pretty well.\nIf we look at the drivers individually, the most respondents (58%) have already seen security benefits but the majority have also seen reliability (52%) and scalability (53%) benefits. In the context of application modernization projects that are mostly still ongoing, these should be viewed as a very positive sign of the value that application modernization can bring. It’s even more impressive if we look at whether respondents have seen benefits in at least one category. Viewed that way, almost all (98%) have experienced a benefit in at least a single critical area.\nThere isn’t one definition of application modernization This year respondents said that improving CI/CD (continuous integration/delivery) pipelines was their top definition of application modernization at 68%. This is significant for a couple of reasons.\nFirst, this percentage is markedly higher than in our 2021 survey when containerizing workloads was at the top of the list while improving CI/CD was near the bottom. We rarely see this degree of change from survey to survey. This was one of several responses that suggest on traditional IT fundamentals and at least incrementally reduced emphasis on certain newer technologies.\nSecond, it serves as a useful reminder that not everyone is on the cutting edge. All organizations continue to tweak their CI/CD pipelines to incorporate additional capabilities such as security scanning. However, those who have adopted modern best practices for software development can be reasonably assumed to have solid CI/CD processes in place overall. These numbers suggest that assumption doesn’t apply to everyone. We’ve seen similar trends in other practices over time that have become table stakes for advanced practitioners while others are just dipping their toe in the water.\nAnother common definition is data modernization, commonly described as updating and improving an organization’s data infrastructure, tools, and practices to meet the evolving needs of data-driven business operations and analytics. It’s reasonable to assume that prepping for artificial intelligence/machine learning is an important rationale for thinking about application modernization in these terms. Data modernization also comes into play as part of modernizing the data exchange between applications and their components as they are updated, such as for distributed data in edge applications. (Other surveys we’ve conducted have found that integration issues are a consistent challenge for digital transformation projects.)\nRounding out the most common definitions were automating workloads and serverless computing. How are organizations modernizing? Modernization strategies are diverse and most commonly happen in multiple steps.\nWe asked respondents to answer the question: “What does your organization plan to do with the custom production applications it wants to modernize over the next two years?” The 6 choices we provided followed the 6 Rs framework, a model deriving from the 5 Rs created by market researcher Gartner in 2010 at a time when many businesses were starting to grapple with how best to move their legacy applications to a cloud. (Sometimes you’ll also see a 7 Rs variant.) You’ll find some differences in nomenclature from different sources but the overall framework is widely used.\nThe 6 strategies are:\nRetire: Sunset or decommission applications that are no longer needed\nRetain: Leave critical applications as-is until refactoring is required\nRehost: “Lift-and-shift” applications to a cloud (hosted or on-premise) without architectural changes\nReplatform: Optimize while migrating to the new platform to cloud-enable applications without changing core application code or architecture. (Sometimes referred to as “tweak, lift, and shift.”)\nRefactor: Re-architect as cloud-native, for example, by containerizing workloads or moving them to a serverless architecture\nRepurchase: Move from perpetual licenses to a Software-as-a-Service (SaaS) model\nThe first conclusion we can draw from this survey—as well as others that we’ve done—is that there is no single dominant approach. Replatforming was the most common response at 20% and all the other responses were between 10% and 19%. Modernization strategies for custom applications are diverse and dependent on the applications being modernized, the sophistication of the organization, and even where the respondent sits in the organization. The other conclusion that we can confidently draw is that application modernization is not a one and done project. Rather, there’s a clear incremental path to re-architected applications. Just 15% of respondents plan to go straight to refactoring. But that doesn’t mean refactoring is off the table for the future. Indeed, 47% plan to replatform and then refactor. Another 38% plan to rehost, then replatform, and only then refactor. (Among organizations who describe their application modernization as a continuous process, this last category shoots all the way up to 52%.)\n__\nWhat does application modernization focus on? We can roughly lump application modernization into two buckets: Modernizing existing (legacy) infrastructure/applications\nDelivering or building new infrastructure, cloud services, and modern or native cloud-native applications or other modern IT services. We found that 59% of modernization budgets were in service of the 1st category with the remaining 41% falling into the 2nd. Results across regions were essentially identical.\nRelative to our survey two years ago, the budget allocation for modernization has shifted much more toward updating legacy applications and infrastructure relative to building new applications and infrastructure in the cloud. The numbers roughly flipped between the two surveys and would seem where there is a renewed focus on IT fundamentals.\nDigging deeper into the types of applications being prioritized for modernization reinforces this view.\nCore back-end applications are the top priority at 41% followed by data/analytics/business intelligence applications at 35%. (This again highlights the importance of data for AI and related technologies.) Here, the result did vary by region with APAC approximately flipping the back-end applications and data-related application numbers. Across all geographies, customer-facing applications were a relatively low priority at 14% overall.\nThese numbers provide an interesting counterpoint to the bimodal (or fast/slow) IT idea that was making the rounds about a decade ago. It came from a school of thought that legacy back-end applications would tend to be set off to the side and minimally changed—Retained in 6 Rs terminology—while all the exciting action would be on new customer-facing applications and services. The results from this survey show that, in fact, companies are prioritizing the applications that run their businesses and the data that informs their business decisions. However, new applications and services —including customer-facing applications— are certainly being delivered as well.\n[A focus on legacy apps and infrastructure [slide 21 complete] with a continued focus on core back-end business applications and an increased focus on data/bi [slide 22 complete]]\nWhat are organizations doing to address modernization challenges? Complexity dominates the top organizational challenges with 48% of respondents identifying this particular challenge. In the early stages of application modernization, it’s even higher—58%. In early stages, determining the right approach was also high on the list at 55% although, even beyond early stages, it still ranked high at 41%.\nHowever, companies are taking steps to address the challenges faced at both the organizational and individual levels. One US-based IT decision maker recommended that you “set clear objectives and goals for the modernization process, establish a dedicated modernization team, and conduct a comprehensive assessment of your current application portfolio.” The quantitative results from our survey similarly emphasized up-front planning and a systematic approach with researching/adopting additional tools, API-driven development, building a business case, and implementingDevOps practices at the top of the list when it came to actions their organizations are taking in response to modernization challenges.\nAI has an important emerging role in application modernization The surprise would probably be if AI didn’t somehow factor into application modernization, given its important emerging role in so many areas of IT and elsewhere. Over 75% of organizations surveyed are using AI to support the application modernization process. Using AI to facilitate modernization is most common (53%) but a significant number (42%) is also, or alternatively, adding AI to existing legacy applications to modernize them.\nPerformance optimization is the most common use case by a significant margin—complex distributed systems increasingly exceed the ability of people to effectively tune them by hand, hence AIOps. However, automation both of manual tasks broadly and testing/QA specifically are also important roles for AI. Directly assisting in writing code, on the other hand, seems to still be relatively nascent and was less cited than other uses notwithstanding the buzz (and sometimes controversy) around large language models (LLM) in many developer circles.\nOverall, most organizations are already using AI to support application modernization and the activity is quite consistent across geographies.\nWhat does this mean for you? Our respondents had some sound advice to offer, both about getting started and keeping projects on track along the way.\nWith respect to kicking off an application modernization project, an IT decision maker from APAC emphasized how “setting realistic and clear goals before you start your modernization journey gives you greater control over the upgrade process.” Themes among the decision makers, architects, and developers we spoke with included: up-front planning, incremental approaches, and having the right team that’s well-equipped.\nThe decision makers we spoke with also had advice for once modernization projects were kicked off. One of them from the UK recommended that “firms should invest in monitoring and visualization solutions for more successful application and system modernization. These systems provide more information about the current state and performance of applications.” Another decision maker from the US emphasized training “your workforce to use and maintain new applications through best practices and governance.”\nOverall, there are some clear common themes: have a concrete goal, plan, enable the right team, and continuously monitor (including vulnerability scanning). Interestingly, hiring subject matter experts and providing additional training weren’t the priorities that we’ve seen across other surveys. One reason is that there seems to be at least incrementally more focus on outsourcing—such as managed services—and consulting with communities and vendors on best practices among the respondents here. Skilling up still matters. But so does leaning on third-party expertise and capabilities when available and appropriate.\nSlide deck with all findings State of application modernization 2024 from Konveyor Community\nGordon Haff is Technology Advocate at Red Hat where he works on emerging technology strategy; writes about tech, trends, and their business impact; and is a frequent speaker at customer and industry events. Among the topics he works on are edge, blockchain, AI, quantum, cloud-native platforms, and next-generation application architectures. His books include From Pots and Vats to Programs and Apps and How Open Source Ate Software. His current podcast is Innovate @ Open. Prior to Red Hat, as an industry analyst, Gordon wrote hundreds of research notes, was frequently quoted in major publications on a wide range of IT topics, and advised clients on product and marketing strategies.\n","categories":"","description":"Learn how organizations plan to approach application modernization and migration—and what they consider success to look like. In all, 1,000 responses were gathered with half coming from the US and the balance split evenly between the United Kingdom (UK) and English-speaking Asia-Pacific (APAC).","excerpt":"Learn how organizations plan to approach application modernization and …","ref":"/modernization-report/","tags":"","title":"State of Application Modernization Report 2024"},{"body":"Author: Savitha Raghunathan\nIf you’re planning to attend KubeCon + CloudNativeCon North America, we are eager to meet you!! You’ll have multiple chances to engage with the Konveyor project and connect with other members of the Konveyor community. This event provides an excellent opportunity to ask questions, interact with the Konveyor team in person, provide feedback, and build connections within the Konveyor community. We are looking forward to meeting you there!\nKubeCon + CloudNativeCon Project Pavilion Be sure to visit our kiosk at #F31 in the project pavilion to learn more about Konveyor and pick up some cool swag and stickers!\nProject Pavilion Hours Tuesday, November 7\n10:30 AM – 8:00 PM Wednesday, November 8\n10:30 AM – 5:00 PM Thursday, November 9\n10:30 AM – 2:30 PM Get Involved To stay updated on the latest Konveyor news, join our community meetings where you can learn more about upcoming features and share your insights. Additionally, you can subscribe to our mailing list for the latest updates on Konveyor or reach out to us in #konveyor on kubernetes.slack.com.\nSee you at KubeCon!\n","categories":"","description":"If you're planning to attend KubeCon + CloudNativeCon North America, we're excited to meet you! There are several opportunities for you to engage with the Konveyor project and connect with other members of the Konveyor community. This event provides an excellent opportunity to ask questions, interact with the Konveyor team in person, provide feedback, and build connections within the Konveyor community. We are looking forward to meeting you there!","excerpt":"If you're planning to attend KubeCon + CloudNativeCon North America, …","ref":"/blog/2023/konveyor-kiosk-at-kubecon-north-america-2023/","tags":["CNCF","Konveyor","KubeCon","AppModernization"],"title":"Konveyor Kiosk at KubeCon + CloudNativeCon 2023 NA Chicago"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubernetes/","tags":"","title":"Kubernetes"},{"body":"Author: Savitha Raghunathan\nIt has been an exceptional year for Konveyor since its admission into the CNCF in July 2022. Since then, the project has undergone a series of transformative changes and remarkable achievements, guided by a newly updated charter that has refocused Konveyor’s mission towards refactoring and replatforming legacy applications. In this post, we invite you to join us on a journey down memory lane as we celebrate the numerous milestones, both big and small, that our dedicated contributors have accomplished over the past year.\nIn the initial months following Konveyor’s acceptance into the CNCF sandbox, considerable effort was invested in establishing essential contributor documentation, forming a steering committee, and embracing best practices for repository management. We extend our heartfelt gratitude to Josh Berkus and Lenka Bočincová from Red Hat OSPO for their invaluable guidance in onboarding Konveyor into the CNCF. CNCF TAG-Contribex has developed project templates that played a vital role in expediting our onboarding process. These templates encompassed everything from code of conduct to vulnerability management, demonstrating the thoughtful process of the TAG-Contribex team in helping project maintainers onboard easily.\nWith the revised charter, Konveyor’s mission has now evolved to “Accelerate the adoption of Kubernetes by assisting organizations in modernizing their legacy applications with Kubernetes and cloud-native technologies in a secure and predictable manner at scale, delivering value at every stage of the adoption journey.” The project’s future vision is encapsulated in the Unified Experience, which serves as our guiding light for all development efforts. The evolution of Konveyor’s roadmap is clearly evident, commencing with the initial emphasis on surfacing information in its 0.1 release. Subsequent release 0.2, extended this focus to enable the planning of work in the form of migration waves, culminating in a significant milestone in 0.3 - a comprehensive rewrite of the analysis engine. This rewrite is built upon the Language Server Protocol, laying the foundation for multi-language analysis capabilities. Currently, this feature supports Java and Go, with active development underway to extend this support to .NET/C#.\nWe are excited to share that Claranet is the latest contributor and community supporter of Konveyor. Their collaboration has been invaluable, as they have shared their extensive migration experience and knowledge, greatly enhancing the assessment of applications within the project. Claranet has introduced the Konveyor Add-On for Amazon EKS Blueprints, and they have also packaged Konveyor for effortless deployment on AWS, simplifying the onboarding process for organizations embarking on their modernization journey. If you have migration experiences to share, we encourage you to join the Konveyor community meetings.\nFor the stats enthusiasts among us, here are some interesting metrics from the past year:\nNew Contributors: 38 Pull Requests: 2.44K Issues Addressed: 903 Total Contributions: 21.7K Commits: 3.9K As we reflect on the remarkable progress achieved over the past year, we want to express our heartfelt gratitude to our contributors and supporters. It is your dedication and support that have made these achievements possible. We look forward to another year of growth, collaboration, and success in the Konveyor project.\n","categories":"","description":"It has been an exceptional year for Konveyor since its admission into the CNCF in July 2022. Since then, the project has undergone a series of transformative changes and remarkable achievements, guided by a newly updated charter that has refocused Konveyor's mission towards refactoring and replatforming legacy applications. In this post, we invite you to join us on a journey down memory lane as we celebrate the numerous milestones, both big and small, that our dedicated contributors have accomplished over the past year.","excerpt":"It has been an exceptional year for Konveyor since its admission into …","ref":"/blog/2023/one-year-cncf-sandbox-konveyor-reflective-journey/","tags":["Konveyor","Kubernetes","AppModernization"],"title":"One Year in the CNCF Sandbox- Konveyor's Reflective Journey"},{"body":"Contributing Guide Welcome! We are glad that you want to contribute to our project! 💖\nThe project’s vision can be explored in detail in this enhancement: Unified Experience. Konveyor follows a collaborative and community-driven approach and the project encourages contributions from individuals and organizations alike. Konveyor’s charter outlines the project’s values, guidelines, and expectations for community members, making it easier for new contributors to get involved.\nAs you get started, you are in the best position to give us feedback on areas of our project that we need help with including:\nProblems found during setting up a new developer environment Gaps in our Quickstart Guide or documentation Bugs in our automation scripts If anything doesn’t make sense, or doesn’t work when you run it, please open a bug report and let us know!\nWays to Contribute We welcome many different types of contributions including:\nNew features Builds, CI/CD Bug fixes Documentation Issue Triage Answering questions on Slack/Mailing List UI Communications / Social Media / Blog Posts Release management Not everything happens through a GitHub pull request. Please come to our meetings or contact us and let’s discuss how we can work together.\nCome to Meetings Absolutely everyone is welcome to come to any of our meetings. You never need an invite to join us. In fact, we want you to join us, even if you don’t have anything you feel like you want to contribute. Just being there is enough!\nYou can find out more about our meetings here. You don’t have to turn on your video. The first time you come, introducing yourself is more than enough. Over time, we hope that you feel comfortable voicing your opinions, giving feedback on others’ ideas, and even sharing your own ideas, and experiences.\nFind an Issue We have good first issues for new contributors and help wanted issues suitable for any contributor. good first issue has extra information to help you make your first contribution. help wanted are issues suitable for someone who isn’t a core maintainer and is good to move onto after your first pull request.\nSometimes there won’t be any issues with these labels. That’s ok! There is likely still something for you to work on. If you want to contribute but you don’t know where to start or can’t find a suitable issue, you can express your interest to contribute in our slack channel. One of the Konveyor Maintainers or leads will get in touch with you.\nOnce you see an issue that you’d like to work on, please post a comment saying that you want to work on it. Something like “I want to work on this” is fine.\nAsk for Help The best way to reach us with a question when contributing is to ask on:\nThe original github issue The developer mailing list Our Slack channel Sign Your Commits DCO Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project’s repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project.\nYou sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit.\nThis is my commit message ``` Signed-off-by: Your Name \u003cyour.name@example.com\u003e ``` Git has a -s command line option to do this automatically:\ngit commit -s -m 'This is my commit message'\nIf you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running\ngit commit --amend -s\nThis template has been taken from CNCF’s contributing template and has been modified to suit the needs of Konveyor project.\n","categories":"","description":"","excerpt":"Contributing Guide Welcome! We are glad that you want to contribute to …","ref":"/docs/konveyor/contributetokonveyor/","tags":"","title":"Contributing To Konveyor"},{"body":"Author: Savitha Raghunathan\nKonveyor aims to surface insights on applications at scale to empower enterprise users to make better-informed decisions related to modernization activities. When it comes to migrating applications to Kubernetes (K8s), it’s important to note that some applications can be ported without any major issues. However, there are certain edge cases where applications may encounter difficulties during the migration process. One such edge case we will focus on in this tutorial involves Hazelcast. Hazelcast is an open-source in-memory data grid that provides distributed caching and data synchronization capabilities. In this tutorial, we will use Hazelcast as the session backing store. During the migration of an application utilizing Hazelcast, some issues arise due to the distributed nature of Hazelcast and the dynamic nature of a Kubernetes environment. In this specific case, network connectivity and discovery issues impact the functionality of the petclinic application.\nTo tackle this challenge, the tutorial will not only guide you on how to identify and translate specific edge case issues into custom rules but also emphasize the long-term benefits of sharing this knowledge within the organization. By investing time and effort in creating custom rules for this particular application, you are laying the groundwork for future migrations and modernizations throughout the app portfolio. Once we have integrated these custom rules as targets with Konveyor, we will gain valuable insights that can be applied to other applications in the portfolio. This knowledge-sharing mechanism allows others in the organization to quickly recognize similar issues and provides a clear path for resolving them.\nWhile creating custom rules may require additional effort initially, the resulting knowledge base will serve as a valuable resource for scaling these efforts across teams. It’s not just about solving the problem for one application; it’s about empowering others in the organization to handle future modernizations and migrations more efficiently. By sharing the insights gained from Konveyor and the custom rules, you contribute to the overall growth of knowledge and shorten the learning curve for future projects.\nGoals: This example will explore the reasons behind the failed attempt to lift and shift the petclinic app to Kubernetes. We will leverage the lessons learned from this experience to develop custom rules and a target. We will then use the custom target with the Konveyor Analyzer module to analyze the application. By implementing the recommended changes identified in the preceding step, we will refactor and deploy the app to Kubernetes. Background: The Petclinic app has been modified to replicate the behavior of a real-world enterprise application. In this updated version, Hazelcast is used as the backing store to handle anonymous users’ sessions. The application leverages Spring Session management, which facilitates clustered sessions beyond the limitations of an application container-specific solution. Consequently, multiple instances of the Petclinic application can utilize a shared store for user sessions. In this example, the Petclinic application instances utilize Hazelcast as an HTTP session store, enabling the instances to share sessions. This setup guarantees uninterrupted user sessions even if an instance becomes unavailable.\nPrerequisites EKS v1.25 or above Konveyor Operator Walkthrough: This walkthrough has the following three parts,\nLift and shift to Kubernetes Analyze the app using Konveyor custom rules Refactor and deploy to Kubernetes Part 1: Lift and Shift to Kubernetes In this section, we will take the modified petclinic app and deploy it in a Kubernetes Cluster and explore the reasons why the user session wasn’t replicated.\nDeploy the application to Kubernetes kubectl apply -f https://raw.githubusercontent.com/konveyor/spring-framework-petclinic/legacy/petclinic-legacy.yaml Note: Please edit the ingress annotations to suit your ingress provider. The above deployment uses ALB for ingress.\nAfter deployment, check if the resources are available kubectl -n petclinic-legacy get all,ing The output should resemble the following,\n$ kubectl -n petclinic-legacy get all,ing NAME READY STATUS RESTARTS AGE pod/petclinic-legacy-d854c89f5-cgfch 1/1 Running 0 82s pod/petclinic-legacy-d854c89f5-j6b4l 1/1 Running 0 82s pod/petclinic-legacy-d854c89f5-xw78q 1/1 Running 0 82s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/petclinic-legacy ClusterIP 10.100.93.168 \u003cnone\u003e 8080/TCP 82s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/petclinic-legacy 3/3 3 3 83s NAME DESIRED CURRENT READY AGE replicaset.apps/petclinic-legacy-d854c89f5 3 3 3 83s NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/petclinic-legacy alb * k8s-petclini-petclini-20798acd3f-1994483574.us-east-1.elb.amazonaws.com 80 82s Once the ingress is available, access the application with the format \u003cingress-url\u003e/petclinic. eg: http://k8s-petclini-petclini-20798acd3f-1994483574.us-east-1.elb.amazonaws.com/petclinic/\nHere’s the high-level view of the deployment,\nBrowse through the application. You will notice that the session data changes with every request. Click here to watch the video for part 1 in action - Lift and Shift deployment of Petclinic to K8s Why did the deployment fail? Based on the information from pod logs, it appears that each pod in the Kubernetes cluster has its own Hazelcast cluster. This means that each instance of the application has its own session backing store, resulting in session inconsistency. As requests are served by different pods in a round-robin fashion, the session keeps changing, which can cause issues with session management and data synchronization.\nThe root cause of this problem seems to be a failure in the automatic member discovery feature of Hazelcast. The app relies on TCP/IP for member discovery, but the address specified in the application configuration is not accessible from the Kubernetes cluster.\nSolution To fix this issue, updating the Hazelcast dependency to a version that is compatible with Kubernetes is recommended. Furthermore, the discovery method implemented in the codebase is not performing as expected in the Kubernetes (K8s) environment. Since the app is configured to use embedded Hazelcast and the deployment is not a statefulset, it is not possible to predict the IP addresses of the pods to configure the TCP/IP discovery method. A possible solution would be to configure Multicast discovery but it requires a secure network environment due to the broadcasting of packets, allowing any member with the appropriate cluster name to join the cluster, which grants less control over the cluster. In the given scenario, it is recommended to utilize the Kubernetes discovery plugin for effective embedded Hazelcast cluster management. Note: This is one of the many solutions that are applicable in this scenario. This example aims at walking through the troubleshooting process and converts the knowledge gained into custom rules for future app migrations\nPart 2: Analyze the Petclinic App using Konveyor Custom Rules Step 1 Based on the lessons learned from the previous section, let’s develop custom rules that incorporate the identified issues and their corresponding solutions. \u003c?xml version=\"1.0\"?\u003e \u003cruleset id=\"HZRules\" xmlns=\"http://windup.jboss.org/schema/jboss-ruleset\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://windup.jboss.org/schema/jboss-ruleset http://windup.jboss.org/schema/jboss-ruleset/windup-jboss-ruleset.xsd\"\u003e \u003cmetadata\u003e \u003cdescription\u003e This ruleset detects embedded hazelcast, which may be problematic when migrating an application to a cloud environment. \u003c/description\u003e \u003cdependencies\u003e \u003caddon id=\"org.jboss.windup.rules,windup-rules-javaee,3.0.0.Final\" /\u003e \u003caddon id=\"org.jboss.windup.rules,windup-rules-java,3.0.0.Final\" /\u003e \u003c/dependencies\u003e \u003ctargetTechnology id=\"cloud-readiness\" /\u003e \u003ctag\u003eHazelcast\u003c/tag\u003e \u003c/metadata\u003e \u003crules\u003e \u003crule id=\"hazelcast-cloud-readiness-hz001\"\u003e \u003cwhen\u003e \u003cor\u003e \u003cjavaclass references=\"com.hazelcast.config.JoinConfig.getMulticastConfig({*})\"\u003e \u003clocation\u003eMETHOD_CALL\u003c/location\u003e \u003c/javaclass\u003e \u003cjavaclass references=\"com.hazelcast.config.JoinConfig.getTcpIpConfig({*})\"\u003e \u003clocation\u003eMETHOD_CALL\u003c/location\u003e \u003c/javaclass\u003e \u003c/or\u003e \u003c/when\u003e \u003cperform\u003e \u003chint title=\"Embedded Hazelcast\" category-id=\"cloud-mandatory\" effort=\"3\"\u003e \u003cmessage\u003e Consider using Kubernetes specific configuration. \u003c![CDATA[ // Example using Kubernetes specific configuration JoinConfig joinConfig = config.getNetworkConfig().getJoin(); config.getKubernetesConfig().setEnabled(true) .setProperty(\"namespace\", \"namespace\") .setProperty(\"service-name\", \"hazelcast-service\"); ]]\u003e \u003c/message\u003e \u003c/hint\u003e \u003c/perform\u003e \u003c/rule\u003e \u003crule id=\"hazelcast-cloud-readiness-hz002\"\u003e \u003cwhen\u003e \u003cproject\u003e \u003cartifact groupId=\"com.hazelcast\" artifactId=\"hazelcast\" fromVersion=\"2.0.0\" toVersion=\"4.2.7\" /\u003e \u003c/project\u003e \u003c/when\u003e \u003cperform\u003e \u003chint title=\"Embedded Hazelcast dependencies\" category-id=\"cloud-mandatory\" effort=\"1\"\u003e \u003cmessage\u003eThe project uses hazelcast with the version between 2.0.0 and less than 5.0.0. Please use hazelcast 5.0 or above. \u003c/message\u003eT \u003c/hint\u003e \u003c/perform\u003e \u003c/rule\u003e \u003c/rules\u003e \u003c/ruleset\u003e Rule 1: hazelcast-cloud-readiness-hz001 : Looks for the discovery methods that are not working as expected in Kubernetes and suggest a solution that works in the Kubernetes environment.\nRule 2: hazelcast-cloud-readiness-hz002 : If the Hazelcast dependency is between version 2.0 and 4.2.7, it advises upgrading the dependency to version 5.0.0 or above\nStep 2 Let’s integrate these custom rules into the Konveyor UI, so that it is easily accessible during the application analysis phase. a. Open the Konveyor UI and Navigate to the ‘Administrator’ view b. Click on the Custom Migration Targets link from the side navigation. c. Click on the ‘Create New’ button in the upper right corner to open the New Custom Target wizard.\nDownload the custom rule from here and save it with an extension ‘.windup.xml’. Upload the file to the wizard and click on create button. d. Once created, the custom target is ready to be used in the Analysis of the application. Step 3 Select the ‘Migration’ view and click on the ‘Analysis’ tab a. Create an app entry called petclinic in the Analysis tab of the tackle ui.\nFill in the name of the application Expand the source code and enter the following Repository Type: Git\nSource Repository: https://github.com/konveyor/spring-framework-petclinic.git/\nBranch: legacy\nRoot path: /\nClick on the Create button.\nb. Select the petclinic app and click on the Analyze button.\nAn application analysis wizard will open. Select Source Code in the dropdown and click the Next button\nIn the Set Targets pane, select Containerization as well as ‘Hazelcast Session Management’ options and click Next For scope, select Application and internal dependencies only\nKeep the defaults in the Advanced section\nMove to the Review pane, and click on theRun button c. You will notice that the status of the app has been updated to In Progess. Wait until it changes to Completed.\nd. Expand the app, and click on the Analysis Report hyperlink e. Tackle Analysis dashboard gets opened in a new tab. Step 4 Click on the spring-framework-petclinic hyperlink and a dashboard gets displayed. Navigate to the Issues tab where you can explore the issues. Pay attention to the ones that are categorized as Cloud Mandatory as they have to be addressed for a successful migration to Kubernetes. Step 5 Expand on Embedded Hazelcast and Embedded Hazelcast dependencies to view the issues. These are the result of the custom target we added in Step 2. Click here to watch the video for part 2 in action - Konveyor Analysis with custom rules Part 3: Refactor and deploy to Kubernetes By integrating the custom rules into Konveyor, you are not only addressing the specific edge case issues in the Petclinic app but also preparing to tackle similar patterns across your portfolio of applications. Let’s imagine that you have several other applications in your portfolio that exhibit similar patterns. With the custom rule integrated into Konveyor, other teams and developers within your organization will benefit from the knowledge you’ve shared. As they scan their applications using Konveyor with the custom rules, they will be able to identify similar issues.\nTo better understand the scenario, let’s take a closer look at the original state of the Petclinic application code before the update. As we delved into the analysis process in part 2 of this tutorial, it became apparent that there were certain issues related to Hazelcast that needed attention. These issues were effectively flagged by the Konveyor Analysis module. In this part, we will address those issues in the petclinic app and deploy the refactored app to Kubernetes.\nLet’s apply the changes suggested in part 2 to the application. You can find the source code for the refactored app here\nBefore deploying the app, let’s add the required RBAC permissions that allow Hazelcast to talk to the K8s API\nkubectl apply -f https://raw.githubusercontent.com/konveyor/spring-framework-petclinic/petclinic-kube/hazelcast-kube.yaml Deploy the refactored application kubectl apply -f https://raw.githubusercontent.com/konveyor/spring-framework-petclinic/petclinic-kube/petclinic.yaml Obtain the ingress of the refactored app deployment using kubectl get ing -n petclinic-kube and access the app using the format \u003cing-address\u003e/petclinic\nNavigate through the app and you will notice that the session data remains the same irrespective of the pod that is serving the request. Click here to watch the video for part 3 in action - Refractor and Deploy to K8s Conclusion Throughout this tutorial, we have explored the capabilities of Konveyor in delivering a comprehensive overview of issues, enabling effective planning. Moreover, Konveyor offers the ability to dive deep into each issue, down to the specific line number. This functionality empowers developers to swiftly address and resolve issues without wasting time on locating their occurrence within the codebase. In addition, we have delved into the process of creating custom rules and custom targets, which proved invaluable for conducting thorough application analysis. By investing time in creating custom rules and integrating them into Konveyor, you are not only alleviating the burden of scanning and identifying issues for each application individually but also establishing a knowledge-sharing mechanism that enables other teams to recognize and resolve these concerns efficiently. This collaborative approach accelerates the modernization and migration process, ensuring a smoother transition for all applications within your portfolio. Get involved in the Konveyor Community The Konveyor project is actively seeking additional use cases and migration experiences to enhance the current set of rulesets. If you have valuable insights or experiences to share, we encourage you to join the Migration-experience User Group. To find out more about getting involved, please click here.\n","categories":"","description":"Konveyor aims to surface insights on applications at scale to empower enterprise users to make better-informed decisions related to modernization activities. This tutorial focuses on addressing edge case issues with Hazelcast during Kubernetes migration, emphasizing the benefits of creating custom rules and sharing knowledge to streamline future modernization efforts.","excerpt":"Konveyor aims to surface insights on applications at scale to empower …","ref":"/blog/2023/modernize-petclinic-to-k8s-using-konveyor/","tags":["Konveyor","Kubernetes","AppModernization"],"title":"Modernize Petclinic App to Kubernetes Using Konveyor"},{"body":"We are thrilled to announce the release of Konveyor 0.2.1! This release supports various features, like migration waves, integration with Jira, and other valuable minor updates.\nKey Highlights of Konveyor 0.2.1:\nMigration waves Jira Integration Minor improvements Migration Waves: We are excited to introduce a significant addition to the Konveyor project, aimed at providing enhanced value throughout the migration process. We recognized the need to address the planning aspect required to scale the migration process across entire application portfolio. When dealing with a large-scale portfolio containing hundreds or even thousands of applications, it becomes impractical to execute the migration process in one go. To tackle this challenge, we have introduced the ability for Konveyor to define migration waves, allowing for an iterative and manageable approach. By breaking down the portfolio into waves, we enable a more sophisticated process that in the future we expect will automate the calculation of these waves based on various criteria. This feature empowers users to scale adoption efforts effectively, ensuring a seamless and efficient migration experience.\nJira Integration: Jira Integration is a significant development in response to a key concern raised by top managers involved in modernization and migration initiatives. Understanding the overall project progress is crucial, and recognizing that most organizations utilize their task management tools, we have enabled Konveyor to integrate seamlessly with these existing systems to leverage their capabilities. Our approach entails creating tasks or issues within the respective task/issue managers to represent the work associated with each application in the portfolio. Konveyor will then retrieve real-time information about the current status of each application, providing comprehensive project visibility. We have designed a flexible and adaptable integration layer, ensuring that it remains independent of any specific task/issue manager peculiarities on the Konveyor side. Jira is the first implementation, but others could come in the future with other popular issue managers like the ones available in GitHub and GitLab.\nMinor improvements: These enhancements aim to refine and enhance the overall user experience of our platform. Some of the notable improvements are:\nServiceMonitor for hub metrics: We have introduced ServiceMonitor functionality to capture and monitor hub metrics, providing valuable insights into the performance of your system.\nExpose Hub metrics: With this update, we have exposed hub metrics, allowing users to access and utilize metrics for monitoring and analysis purposes.\nMake Ingress Creation Optional: This update allows Ingress creation optional, providing flexibility and control over your deployment configurations.\nSupport for Amazon Load Balancer Ingress controller: Our platform now supports the Amazon Load Balancer Ingress controller, enabling Konveyor to be easily deployed on EKS clusters.\nProvide openshift-oauth as an authentication option: As part of our ongoing efforts to enhance authentication capabilities, we now offer openshift-oauth as an additional authentication option, providing more choices to meet your specific requirements.\nMake rwx_supported false by default: This feature removes the RWX dependency enabling Konveyor to work without RWX volume by default.\nThese minor improvements reflect our commitment to continuously refining and enhancing our platform to deliver an optimized and user-friendly experience. We value your feedback and encourage you to explore these enhancements as you leverage our latest release. You can install Konveyor 0.2.1 from community operators. If you encounter any issues or have suggestions for features, we encourage you to open an issue here.\nHappy Modernizing!\n","categories":"","description":"Konveyor v0.2.1 Release Announcement","excerpt":"Konveyor v0.2.1 Release Announcement","ref":"/blog/2023/release-announcement-0-2-1/","tags":["Konveyor","Kubernetes","AppModernization"],"title":"Konveyor v0.2.1 Release Announcement"},{"body":"Migrating large infrastructures from an on-premise, monolithic, traditional IT management approach to a modern cloud-native Kubernetes-based ecosystem is a daunting task. Christopher Nuland (Konveyor Ambassador) led a conversation with Jared Burk and Valentina Rodriguez Sosa, Red Hat; James Bench, Maximus about the different migrations they have completed and the challenges they experienced.\nChris and Valentina summarized the key 6 takeaways that combined questions from the audience and a summary of the panel discussion held at the KubeCon 23 in AMSTERDAM. Knowing where to start: Starting with the proper process, the right people, and vision is critical for any organization that wants to implement change, whether it is adopting new technology or any methodology. When thinking about modernization, it is important to start understanding the value that it can provide me, such as hardening security, reducing operational time and costs, and accelerating software development. Connecting these values with my organization’s mission, vision, and values will create the Why for this project and a compelling vision that the teams should be aware of. Understanding the why will help create motivation and commitment and reduce any friction between business priorities and modernization’s value.\nCreate a plan: Align your goals to create metrics to measure success. If cost reduction is one of the key goals of your modernization plan, calculate operational and infrastructure costs before and after any migration.\nCreate a pilot project: Creating a pilot project before the large-scale migration will help reduce any concerns you and your organization might have about costs, risks, timeframes, and outcomes. If your concern is about costs, choose one of the most significant and most complex applications as a pilot to understand the level of complexity and amount of work that will end up as a cost for the migration project.\nYou can add a set of pilot apps with each app having different characteristics, so you can look at different measures such as costs, and risks.\nAfter the pilot has been succeeded, take lessons learned and compare the metrics you created to ensure you obtain the value you were looking for.\nIndustry Compliance When planning a cloud-native migration, it’s essential to have a thorough understanding of your organization’s industry-specific compliance standards, such as HIPAA, NIST, and GDPR. This knowledge is particularly crucial when transitioning from an entirely on-premise system. In these traditional environments, data protection and other compliance standards are often addressed through physical infrastructure and isolation strategies. This approach can lead to a phenomenon known as ‘data gravity’, where some data must remain on-premise due to compliance constraints. Consequently, a hybrid cloud architecture becomes necessary, allowing critical integrations to stay on-premise. Grasping these intricacies and forming a strategic plan around them are key steps towards a smooth migration that maintains industry compliance. This preparation paves the way for a flexible, scalable, and efficient cloud-native system that adheres to industry rules and regulations. A balanced approach like this is essential for successful cloud adoption and robust compliance management. To prevent compliance issues from stalling your migration process, it’s advisable to start understanding and addressing your compliance requirements early in the migration process. This proactive approach will help ensure a seamless transition to the cloud.\nGetting Organizational Buy-In Securing organizational buy-in during a large-scale enterprise migration to a cloud-native k8s platform is crucial for the project’s success. A typical obstacle to smooth transitions is the presence of siloed organizational structures, where information and processes are isolated, causing difficulties in coordinating and unifying migration efforts. These siloed operations can slow down the migration process, inhibit the efficient use of cloud functionalities, and even lead to costly delays. To overcome these challenges, buy-in should start at the highest levels of the organization, with the CTO and CEO setting the strategic direction and demonstrating commitment. The C-suite’s endorsement enables a culture shift, fostering a more collaborative environment conducive to such a significant cloud-native transformation. Furthermore, their role extends to ensuring the allocation of appropriate resources and investment in upskilling to enable the organization for the change needed for cloud-native transformation. This top-down approach needs to cascade to individual contributors, including engineers, who will be integral in the technical aspects of the migration. These individuals should understand and endorse the strategic benefits of the migration, and feel empowered to collaborate across different units. In this way, the entire organization can work cohesively towards a successful migration, while reaping the many benefits that a cloud-native platform can offer such as scalability, reliability, and cost efficiency.\nDon’t reinvent the wheel When talking about migrations, patterns are critical from the applications to the methodologies and tooling. We have seen cases where companies are trying to reinvent the wheel, leading to a significant spend of $$$, resources, and sometimes to the right solution for them. We recommend researching and getting involved with community and modernization experts, who can help you find the best path quickly and save costs for your organization.\nhttps://youtu.be/k-Pvzg9eosQ\nHelp improve the Konveyor project by sharing your application modernization problems and solutions here.\nYour learnings will be used to improve the Konveyor project and help ease others modernize.\n","categories":"","description":"Migrating large infrastructures from an on-premise, monolithic, traditional IT management approach to a modern cloud-native Kubernetes-based ecosystem is a daunting task. Christopher Nuland (Konveyor Ambassador) led a conversation with Jared Burk and Valentina Rodriguez Sosa,  Red Hat; James Bench, Maximus\n about the different migrations they have completed and the challenges they experienced.","excerpt":"Migrating large infrastructures from an on-premise, monolithic, …","ref":"/blog/2023/reflecting-on-large-scale-cloud-native-modernizations-at-kubecon-23/","tags":["Konveyor","AppModernization","Kubernetes"],"title":"Reflecting on Large-Scale Cloud-Native Modernizations at KubeCon 23"},{"body":"Konveyor Konveyor aims to surface insights on applications at scale to empower enterprise architects to make better-informed decisions related to modernization activities\nOverview Konveyor is an open-source application modernization platform that helps organizations safely and predictably modernize applications to new technologies, with an initial focus on accelerating the adoption of legacy applications to Kubernetes.\nThe project’s design incorporates years of experience with consulting engagements successfully helping companies move existing applications to new technologies, such as Kubernetes. The patterns and processes that led to successful, predictable, and safe modernization engagements were distilled into our methodology. This methodology is not required to be used with Konveyor but remains available to help guide others embarking on their own modernization journey who don’t have a preferred methodology.\nKonveyor aims to make the modernization of legacy applications to Kubernetes its #1 priority while realizing that technology is evolving and the need for modernizing to new technologies will remain in the future. We believe the basic patterns to address modernization needs can be built generically into a platform with technology-specific information added as new technologies emerge, thereby allowing the platform to evolve over time to address new needs.\nVision Statement To become the ultimate open-source application modernization platform, helping organizations safely and predictably modernize their portfolios to evolving technology needs.\nMission Statement Accelerate the adoption of Kubernetes by helping organizations modernize their legacy applications to Kubernetes and cloud-native technologies in a safe and predictable manner at scale, providing value at each phase of the adoption journey.\nCommunity Join our Community to stay up to date on developments.\nJoin https://groups.google.com/g/konveyorio for information on new Releases and MeetUps. Join https://groups.google.com/g/konveyor-community for deeper insights into the development work, with recordings and invites to our public Thursday community meetings. See our prior community meetings on YouTube for demonstrations of new features Share your experiences with modernization problems and participate in our migration experience user group focused on bringing learnings back into Konveyor to help ease future migration work. Join as a contributor to coding, documentation, or other interests via our Special Interest Groups. Ask us a question in kubernetes.slack.com: (Get a slack invite via: https://slack.k8s.io/) #konveyor: General questions and discussions #konveyor-dev: Developer questions on contributing to Konveyor Next steps to read more about Konveyor See an example of Konveyor in Action: konveyor/example-applications/example-1 Get Started by deploying the Konveyor Operator Read about Konveyor’s Unified Experience Read the project’s Charter.md Read Konveyor’s technical design documentation in konveyor/enhancements/enhancements See existing Konveyor RFEs konveyor/enhancements/issues YouTube Konveyor’s YouTube channel CNCF Sandbox Project https://www.cncf.io/projects/konveyor/ Konveyor was accepted to CNCF on July 26, 2022 and is at the Sandbox project maturity level. Source\n","categories":"","description":"","excerpt":"Konveyor Konveyor aims to surface insights on applications at scale to …","ref":"/docs/","tags":"","title":"Konveyor Docs"},{"body":"In the latest community update, we revealed upcoming changes to the Konveyor Project. These adjustments are designed to solve the most important application modernizations problem better and to encourage participation with our community. Here’s what we announced:\nWe will solely focus on solving for the replatforming and refactoring use cases. It’s easier to contribute and collaborate with Konveyor thanks to our new special interest group (SIG) model. You can learn the Konveyor project, see upcoming features, and give us feedback at our workshop — part of the OpenShift Commons Gathering at KubeCon NA 2022. Register for the Gathering to secure your spot. Tackle 2.1 is available, future Tackle releases will be branded Konveyor. Konveyor roadmap includes migration wave management, multi language support, and integrations with modernization tools. You can get the details of each announcement below.\nWe are refocusing efforts on replatforming and refactoring use cases A key insight the State of Application Modernization Report surfaced is that users are more interested in replatforming and refactoring then just rehosting workloads or “lifting and shifting”. Paired with feedback from users, this revealed a gap in the tooling for refactoring which Konveyor is now addressing\nOriginally, Konveyor took a “big tent” approach to rehosting, replatforming and refactoring. We brought the tooling to a community of engineers, developers, and users to figure out what problems they need to solve. Now, due to their feedback, Konveyor is refocusing on just replatforming and refactoring, meaning:\nPelorus, Tackle and Move2Kube will be integrated and moved into a single project, now called Konveyor. Crane (rehosting from one Kubernetes cluster to another) will be moved into a separate GitHub organization. Forklift (rehosting to VMs) will move into a KubeVirt project focused on running VM’s with Kubernetes. Crane and Forklift will continue to exist but will no longer be part of the core Konveyor toolset.\nSpecial interest group (SIG) model makes it easier to contribute and collaborate with the project One of the major changes to the Konveyor project is the refocusing on the major areas of code, UI, analyzers, and add-ons, each of which is represented by a special interest group (SIG) modeled after the SIGs created by the Kubernetes community. These groups are responsible for the development, integration, maintenance, and collaboration of the project.\nThe SIG Core is responsible for all of the core features of the Konveyor platform. These features include but are not limited to modernization, report generation, integration of program management tools, and the design of implementation of the hub itself.\nSIG UI is responsible for the user interface and experience across the platform. This SIG is also responsible for creating a pattern where all of the add-ons can be dynamically integrated into the hub.\nThe SIG Analyzers are working on creating an extensible pattern and standardization of input and output for the community to collaborate on the analyzers. The analyzers will each target specific programming languages.\nFinally SIG Add-ons are responsible for the development and integration of add-ons that will enhance Konveyor’s capabilities.\nThe community meetings bring together contributors and collaborators to provide a forum to discuss topics and new ideas and generate innovation, and happen on every first and third Thursday at 9 AM ET (meetings are recorded). Topics include milestones and features of upcoming releases, contributor showcases with member feedback. Above all, these meetings are intended to foster the community around Konveyor.\nThere are also many asynchronous opportunities to communicate and collaborate around Konveyor.\nThere is a #konveyor channel on the Kubernetes Slack channel. You can use it to ask any question about the project. Contributors can subscribe to the Konveyor-community email list for community meeting invites and discussions to influence the direction of the community. To get general updates, you can subscribe to the email list. Or for more information, look at the Konveyor community repo — it includes information around governance documents, contributor letter, Get Started Guide, meeting agendas and developer notes. Attend the Konveyor workshop at KubeCon North America 2022 We will run a workshop in the OpenShift Commons Gathering on 25 October, 2022. At this free, hands-on workshop, workshop leaders will take you through an assesment and analysis of a sample application using the Konveyor platform, followed by discussion of the Konveyor Roadmap and Q\u0026A. Whether you are starting or continuing your application modernization journey, this is a great opportunity to learn how to contribute, or just how to use the tools. If you have any questions or just want to connect with the community or workshop, please reach out to Savita Raghunathan at sraghuna@redhat.com or on Slack.\nHow to register: Visit registration site and follow the directions below Click “register” Select “Konveyor workshop” Click “continue” Provide your info and submit Tackle 2.1 is available, future Tackle releases will be branded Konveyor Tackle 2.1 is available, with the ability to upgrade your current Tackle 2.0. Tackle 2.1 will be the final Tackle release. Future releases will bear the name Konveyor, and will begin with version 3.\nThe release of Konveyor 3 is envisioned for Q1 2023 pending the restructuring of the architecture, and some specific components to make contribution easier. For some of the more visionary components of Konveyor 4, the goal is to have these features ready for release in 2H 2023.\nIn Tackle 2.1 there are several important features:\nIntegration with DiVA (Data intensive Validity Advisor), which is now seamlessly integrated with Tackle: everything is available in the domain user interface to enable the new analysis mode. A transcription report tab is now included in analysis reports with detail on the data layer of your applications.\nThe next important feature is the ability to bulk delete applications. Before 2.1 it was possible to import applications in bulk, but not delete them. It will also include automated creation of missing entities on CSV import.\nNew rules will be included in the next patch to 2.1.1, allowing new migration paths for open JDK 11 to 17 rules, and with Microsoft as a new major player in the Konveyor community, migration to EAP and Spring Boot applications on top of Azure App Service is supported.\nFinally, the Konveyor community did a ton of bug fixing and optimization making this release much more stable than Tackle 2.0.\nKonveyor roadmap Looking forward to next year, several important features are visualized for the first release of Konveyor 3. First of these is Migration Waves Management. No one does a “big bang migration” when dealing with thousands of applications, but rather distributes application portfolio modernization across multiple waves. This new feature offers an assisted method for managing these waves, integrated into the main Konveyor user interface.\nFull integration with Tackle Test Generator and Tackle Container Advisor in the main user interface is envisioned for this release as well. With Tackle Test Generator, tests for an application can be stored in your Git repository with a single click.\nTackle Container Advisor will leverage its’ entity standardization capability using the natural language description of the technology stack of the application, making it easier to ingest data at scale, simplifying CSV imports and Excel importing, and eliminating the need to code the technology stack as a series of separate tasks\nIntegration with Jira will allow the user to track and manage the whole migration process. Tackle will allow delegation of implementation to external issue managers or task managers. By creating issues in Jira related to the applications within the portfolio, a project leader can then assign them to developers to perform changes in the application portfolio.\nCustom Migration Targets will be improved by abstracting users away from the complexities of having to assign custom rules without having to deal with these custom rules directly. Information and status on this feature should be available soon.\nCustom rule upload to repositories will be integrated going forward to allow these rules to be uploaded automatically, and to be retrieved to and from source code repositories.\nTag management will also be improved, with numerous tags assigned to applications it is difficult for the UI to scale this information or consume it. This feature will help make these tags easier to consume.\nLooking even further into the future, Konveyor 4 will bring a whole new set of advanced features including multi language support for analysis, manifest generation and enhanced assessment module.\nEvery time Konveyor has been presented to an organization, invariably there is a request for multi language support. In an early prototype the Konveyor community has demonstrated the ability to use language server technology from Microsoft, decoupling the analysis from data models supporting analysis results. It’s very possible that this feature will be ready to ship with Konveyor 4 in May.\nManifest generation is another important feature: once you’ve successfully adapted source code to run on containers, this will then install the manifest on Kubernetes.\nFinally, feedback on the current Assessment Module is that it’s too opinionated, so the goal is to make it more intelligent and flexible.\nAltogether, the Konveyor Project has undergone a major transformation improving and simplifying, and creating new and better opportunities for building a community around Application Modernization centered around replatforming and refactoring.\n","categories":"","description":"We recently revealed upcoming changes to the Konveyor Project. These adjustments are designed to solve the most important application modernizations problem better and to encourage participation with our community.","excerpt":"We recently revealed upcoming changes to the Konveyor Project. These …","ref":"/blog/2022/community-update-konveyor-refocuses-efforts/","tags":["Crane","Forklift","Move2Kube","Pelorus","Tackle"],"title":"Community Update: Konveyor Refocuses Efforts on Replatforming and Refactoring Use Cases"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/crane/","tags":"","title":"Crane"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/forklift/","tags":"","title":"Forklift"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/move2kube/","tags":"","title":"Move2Kube"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pelorus/","tags":"","title":"Pelorus"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tackle/","tags":"","title":"Tackle"},{"body":"After the major milestone that Tackle 2.0 represented, the community has continued gathering feedback from the field to further polish the feature set and provide value in the context of modernization, migration and adoption projects. This has led to Tackle 2.1, available now in operatorhub.io, a minor release that has been focused on stabilizing the project and providing a more streamlined user experience, while still delivering some new tooling. The new features include the following:\nIntegration with the Tackle DiVA project: New report available in the application analysis results providing detailed information about the application data layer, including an extensive inventory of transactions and accessed databases with detailed stack traces for each transaction. Automated creation of missing entities on CSV import: Tags and Business Services are not required to exist in Tackle prior to the import anymore. This simplifies dealing with large CSV files and the import process overall. Bulk delete of applications: Applications can now be deleted at bulk in the application inventory. This is especially useful when applications have been imported at bulk by mistake or with some errors. Once again, I would like to congratulate all the different teams involved in the implementation, testing and documentation of this new release of Tackle. We continue building this project together in the community, so any feedback or feature request is definitely welcome. If you feel like something is missing or some feature is not behaving as expected, don’t hesitate to open an issue in the Tackle main repository.\nHappy modernizing!\n","categories":"","description":"New features include a new report that gives details about the application data layer, easier CSV importing, bulk deletion of applications, and a more streamlined user experience.","excerpt":"New features include a new report that gives details about the …","ref":"/blog/2022/tackle-2-1-release/","tags":["Tackle"],"title":"Tackle 2.1 release"},{"body":"This tutorial steps through the entire workflow of migratinging a Cloud Foundry application with several micro-services to run on Kubernetes.\nWe will be using the enterprise-app which is a retail website for shopping online. The website shows some products for sale and you can create orders by adding things to cart and checking out.\nThis application consists of five different services:\nFrontend: Website content written using React and Patternfly meant to be run on an Nginx server. Gateway: Portal to all the API servers that aggregates the orders and customer information and acts as a circuit breaker in case one of the API servers start to fail. (Written using the Java Spring Boot and PostGreSQL stack.) Customers: Manages everything related to customers. (Runs using Tomcat and PostGreSQL for the database.) Orders: Manages everything related to orders. (Written using Spring Boot and PostGreSQL for the database.) Inventory: Manages everything related to products. (Written using Spring Boot and PostGreSQL for the database.) This tutorial will go through the workflow for containerizing this application using Move2Kube to get it running on Kubernetes. This tutorial is split into sections to make it easy to skip around.\nSource\n","categories":"","description":"","excerpt":"This tutorial steps through the entire workflow of migratinging a …","ref":"/docs/move2kube/tutorials/cfappstok8/","tags":"","title":"Migrating Enterprise Scale Cloud Foundry Apps to Kubernetes"},{"body":"Move2Kube can be consumed as a command line tool or as a web-based interface for creating the Kubernetes/OpenShift deployment artifacts.\nSource\n","categories":"","description":"","excerpt":"Move2Kube can be consumed as a command line tool or as a web-based …","ref":"/docs/move2kube/installation/","tags":"","title":"Installation"},{"body":"Move2Kube creates all resources required for deploying applications into Kubernetes including containerization and Kubernetes resources. It supports translating from Docker swarm/Docker-compose, Cloud Foundry, and other non-containerized applications. If the application does not use any of those or is not containerized, it can still be transformed by Move2Kube.\nNote: Use these commands to view the options available in Move2Kube.\n-h, --help help for move2kube --log-file string File to store the logs in. By default it only prints to console. --log-level string Set logging levels. (default \"info\") Move2Kube commands There are four phases of the Move2Kube processes that are created, configured, and run using four commands and their options:\nMove2Kube collect - Collects and processes metadata from multiple sources. Move2Kube plan - Plans the deployment of an application into Kubernetes. Move2Kube transform - Transforms the application using the results of the Move2Kube plan stage. Move2Kube version - Assigns the application version information. Move2Kube collect command Move2Kube collects metadata from multiple sources (cluster, image repo etc.), then filters and summarizes them into a YAML.\nmove2kube collect [flags] Collect options Move2Kube provides the following options for collecting.\n-a, --annotations string Specify annotations to select collector subset. -h, --help Help for the collect command. -o, --output string Specify the output directory for collect. (default \".\"). -s, --source string Specify the source directory for the artifacts to be considered while collecting. Options inherited from parent commands --log-file string File to store the logs. By default it only prints to console. --log-level string Set logging levels. (default \"info\") Move2Kube plan Move2Kube discovers and creates a plan file based on an input directory built after running the collect command.\nmove2kube plan [flags] Plan options Move2Kube provides the following options for planning.\n-f, --config strings Specify config file locations. -c, --customizations string Specify the directory where customizations are stored. --disable-local-execution Allow files to be executed locally. -h, --help Help for the plan command. -n, --name string Specify the project name. (default \"myproject\") -p, --plan string Specify a file path to save plan to. (default \"m2k.plan\") --preset strings Specify preset config to use. --set-config stringArray Specify config key-value pairs. -s, --source string Specify source directory. (default \".\") -t, --transformer-selector string Specify the transformer selector. Options inherited from parent commands --log-file string File to store the logs in. By default it only prints to console. --log-level string Set logging levels. (default \"info\") Move2Kube transform Transform functionality modifies artifacts using the Move2Kube plan file.\nmove2kube transform [flags] Transform options Move2Kube provides the following options for transform.\n-f, --config strings Specify config file locations. --config-out string Specify config file output location. (default \".\") -c, --customizations string Specify directory where customizations are stored. --disable-local-execution Allow files to be executed locally. -h, --help Help for the transform command. --ignore-env Ignore data from local machine. -n, --name string Specify the project name. (default \"myproject\") -o, --output string Path for output. Default will be directory with the project name. (default \".\") --overwrite Overwrite the output directory if it exists. By default it does not overwrite. -p, --plan string Specify a plan file to execute. (default \"m2k.plan\") --preset strings Specify preset config to use. --qa-cache-out string Specify cache file output location. (default \".\") --qa-persist-passwords Stores passwords too in the config. --qa-skip Enable/disable the default answers to questions posed in QA Cli sub-system. --set-config stringArray Specify config key-value pairs. -s, --source string Specify source directory to transform. If there already is a m2k.plan then this will override the sourceDir value specified in that plan. -t, --transformer-selector string Specify the transformer selector. Options inherited from parent commands --log-file string File to store the logs in. By default it only prints to console. --log-level string Set logging levels. (default \"info\") Move2Kube version Version functionality prints the application version information.\nmove2kube version [flags] Version options Move2Kube provides the following options for versioning.\n-h, --help Help for version command. -l, --long Print the version details. Options inherited from parent commands --log-file string File to store the logs in. By default it only prints to console. --log-level string Set logging levels. (default \"info\") Source\n","categories":"","description":"","excerpt":"Move2Kube creates all resources required for deploying applications …","ref":"/docs/move2kube/commands/","tags":"","title":"Commands"},{"body":"Move2Kube uses a suite of transformers to modify objects. To customize the output artifacts generated for a specific input, these transformers can be configured or new custom transformers can be created to achieve the required result. Transformer behavior and configuration is determined by the Transformer Class it uses. Though all the transformer classes are equal internally in Move2Kube, from a usage perspective, they are classified into three categories.\nPurpose Built - Has a specific job and the customization allows for changing the parameters/configuration required for performing the specific job. Ex: Kubernetes, Parameterizer, GolangDockerfileGenerator, etc.. External - Allows you to write custom transformers performing any behavior. It exposes the internal functions of the transformer class through different interfaces to be implemented by the transformer externally. Ex: Starlark, Executable Special - These classes allow special behaviors. Ex: Router Purpose Built These transformer classes do a specific job, and the customization allows for changing the parameters/configuration required for performing the specific job.\nIn most cases, these classes have one internal implementation and a transformer configuration can be found here.\nThe general steps to use these transformer classes are:\nFind the internal implementation here. Copy the directory. Change the configuration like the transformer name, templates, etc.. Set it to override the internal implementation. The custom transformer tutorial that uses node.js/Kubernetes transformers and the parameterization tutorial that uses the parameterization transformer are examples.\nTo understand the configuration provided by each of these transformers, the YAML in the built-in transformer should have a good overview. The other location to look for is the structure in the transformer implementation. In most classes which have a configuration, there will be a structure with a name ending in YamlConfig. For example, the Kubernetes transformer class has KubernetesYamlConfig. This is the configuation specified in the spec.config field.\nParameterizer Find the source code here, and follow this tutorial to customize paramaterization.\nSyntax for parameterizing specific fields Move2Kube provides a way to parameterize any field in the Kubernetes YAML files, Helm charts, and Openshift Templates that Move2Kube generates. It can also parameterize fields in Kubernetes YAMLs found in the source directory.\nIn order to parameterize a specific field, Move2Kube needs to use a custom transformer which means a directory with some YAML files inside it needs to be created. Below is an example parameterizer in detail:\n$ ls README.md deployment-parameterizers.yaml parameterizers.yaml Look at the deployment-parameterizers.yaml to understand the syntax.\napiVersion: move2kube.konveyor.io/v1alpha1 kind: Parameterizer metadata: name: deployment-parameterizers spec: parameterizers: - target: \"spec.replicas\" template: \"${common.replicas}\" default: 10 filters: - kind: Deployment apiVersion: \".*/v1.*\" Now the `values.yaml` looks like this - target: 'spec.template.spec.containers.[containerName:name].image' template: '${imageregistry.url}/${imageregistry.namespace}/${services.$(metadataName).containers.$(containerName).image.name}:${services.$(metadataName).containers.$(containerName).image.tag}' default: quay.io/konveyor/myimage:latest filters: - kind: Deployment apiVersion: \".*/v1.*\" parameters: - name: services.$(metadataName).containers.$(containerName).image.name values: - envs: [dev, staging, prod] metadataName: frontend value: frontend - envs: [prod] metadataName: orders value: orders custom: containerName: orders Notice the parameterizer YAML follows the same conventions as Kubernetes YAMLs.\napiVersion : string - The version of the Move2Kube API being used which is currently set at move2kube.konveyor.io/v1alpha1. kind : string - Tells Move2Kube the YAML file type which in this case it is set at Parameterizer. metadata : object name : string - Name of the parameterizer. spec : object parameterizers : array[object] - List of parameterizer objects. target : string - Sets the field to parameterize. The syntax is the same as yq dot notation which is explained in more detail in a later section.\ntemplate : string - Specifies how the field should get its value. For example: \"${common.replicas}\" means the generated Helm chart which contains the values.yaml there is the common field, and inside it replicas as shown below.\n$ cat values.yaml common: replicas: 10 The value of this field will be the same as the value in the original YAML, but it can be overriden using the default parameter.\ndefault : any Can be used to override the default value for the field being parameterized. For example: if the original value of spec.replicas was 2, then the values.yaml would look like this:\n$ cat values.yaml common: replicas: 2 To set a different value like 10, specify default: 10 in the parameterizer YAML and now the values.yaml looks like this:\n$ cat values.yaml common: replicas: 10 filters : array[object] - Used to filter the Kubernetes YAML files that being targeted.\nkind : string - Only parameterizes Kubernetes YAMLs that match this kind field. Specify a regex to match multiple kinds. apiVersion : string - Only parameterizers Kubernetes YAMLs that match this apiVersion field. A regex can be specified. name : string - Only parameterizes Kubernetes YAMLs that have the same metadata.name field. A regex can be specified. envs : array[string] - Only apply this parameterization when targeting one of the environments listed here. parameters : array[object] - Can be used to specify defaults for each parameter inside the template.\nname : string - Name of a parameter inside the template. default : string - Default value for this parameter. Template parameter When parameterizing a specific a field in a Kubernetes YAML, it can be templatized in different ways. If the template parameter is not specified, it will default to the target parameter.\nExample 1 For example: target: \"spec.replicas\" would cause the spec.replicas to be parameterized (probably in Deployment YAMLs). Move2Kube would parameterize it under the key \u003ckind\u003e.\u003capiVersion\u003e.\u003cmetadata.name\u003e.spec.replicas and the values.yaml would look like this:\nDeployment: v1: myDeploymentName: spec: replicas: 2 In this example myDeploymentName is the metadata.name field in the Deployment YAML and 2 is the original value for spec.replicas from the YAML file.\nBy specifying template: \"${common.replicas}\" the default key is overridden and now Move2Kube puts the following in the values.yaml\ncommon: replicas: 2 By specifying default: 10 the default value is overridden and now Move2Kube puts the following in the values.yaml\ncommon: replicas: 10 Example 2 This example is a more complicated scenario of parameterizing the image name of some container in a Deployment YAML. First, because the containers field in the Deployment YAML is a list, use the syntax [\u003cindex\u003e] to parameterize a single element in the list.\nUse target: \"spec.template.spec.containers.[0].image\" to parameterize the first container in the Deployment. The values.yaml looks like this:\nDeployment: v1: myDeploymentName: spec: template: spec: containers: - image: 'my-repo.com/my-namespace/my-image-name:my-image-tag' This may not be enough and may need to parameterize the container image registry URL, registry namespace, image name, and image tag separately.\nTo do this use:\ntemplate: '${imageregistry.url}/${imageregistry.namespace}/${containers.[0].image.name}:${containers.[0].image.tag}' This will cause the values.yaml to look like this:\nimageregistry: url: 'my-repo.com' namespace: 'my-namespace' containers: - image: name: 'my-image-name' tag: 'my-image-tag' The Helm template will look like this:\n{% raw %} spec: template: spec: containers: - image: '{{ index .Values \"imageregistry\" \"url\" }}/{{ index .Values \"imageregistry\" \"namespace\" }}/{{ index .Values \"containers\" \"[0]\" \"image\" \"name\" }}:{{ index .Values \"containers\" \"[0]\" \"image\" \"tag\" }}' {% endraw %} Example 3 This is an even more complicated scenario continuing from Example 2 that adds a dynamic key in the values.yaml.\nTo do this use the [] square brackets and $ dollar sign syntax:\ntarget: 'spec.template.spec.containers.[containerName:name].image' template: '${imageregistry.url}/${imageregistry.namespace}/${containers.$(containerName).image.name}:${containers.$(containerName).image.tag}' Here [containerName:name] in target tells Move2Kube to extract the name field from the container object in the Deployment YAML and make it available as containerName. The $(containerName) in template gets replaced by the name that was extracted.\nThe values.yaml looks like this:\nimageregistry: url: 'my-repo.com' namespace: 'my-namespace' containers: myContainerName1: image: name: 'my-image-name' tag: 'my-image-tag' myContainerName2: image: name: 'my-image-name-2' tag: 'my-image-tag-2' This provides a very powerful way to parameterize the image name of containers and simultaneously have a common registry URL and namespace for all the images while also parameterizing the image name and tag for each container separately.\nExternal These transformer classes allow you to write custom transformers performing any behavior. It exposes the internal functions of the transformer class through different interfaces to be implemented by the transformer externally.\nExecutable An Executable class based on a transformer can be configured to run commands locally or as containers. These transformers essentially implement the DirectoryDetect and Transform functions of the transformer as executable commands. This allows using any language to write the function and makes them very powerful.\nStarlark A Starlark class based transformer allows for writing a full fledged transformer in Starlark by implementing the directory_detect and transform functions.\nSee examples of using this transform class:\nadd-custom-kubernetes-annotation add-custom-files-directories-in-custom-locations. Special These classes allow special behaviors.\nRouter The Router transformer directs an artifact to one of the eligible transformers, like choosing the server for a WAR file.\nWarRouter and EarRouter are examples of using this transformer class.\nSource\n","categories":"","description":"","excerpt":"Move2Kube uses a suite of transformers to modify objects. To customize …","ref":"/docs/move2kube/transformers/","tags":"","title":"Transformers"},{"body":"Big news! Konveyor was recently accepted as a Cloud Native Computing Foundation (CNCF) Sandbox project 🎉\nWhy are we so excited about this? This is a huge milestone in our mission to accelerate the movement of applications to run on Kubernetes with an open-source approach. With the added credibility of our sandbox status, we can more easily attract and convince others to adopt and contribute to Konveyor. The result will be more effective open source tools that everyone can use to adopt Kubernetes and cloud-native technology faster!\nWhat happens now? We’re trying to get the word out about our new status.\nWith regards to tools, we’re rethinking where to focus based on interest and usage. More on that to come later. For now, sharing the news with your network would be the most helpful.\nAnd lastly, thank you to all involved for helping this happen:\nKarena Angell and Josh Berkus for guiding us through the application process. IBM Research and Red Hat for contributing tools and helping to spread the word about the community. Microsoft for contributing PRs and providing feedback to the community. And to all of you on this email list for attending meetups, using the tools, giving us feedback, and spreading the word. Happy migrating,\nKonveyor team\n","categories":"","description":"Big news! Konveyor was recently accepted as a Cloud Native Computing Foundation (CNCF) Sandbox project.","excerpt":"Big news! Konveyor was recently accepted as a Cloud Native Computing …","ref":"/blog/2022/konveyor-is-a-cncf-sandbox-project/","tags":["CNCF"],"title":"Konveyor is now a Cloud Native Computing Foundation (CNCF) Sandbox project"},{"body":"Move2Kube allows custom template files to be added to any directory. This example illustrates this by adding a custom Helm chart.\nStart by creating an empty workspace directory named workspace and make it the current working directory. Asume all commands are executed within this directory. $ mkdir workspace \u0026\u0026 cd workspace Use the enterprise-app as input for this flow. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/src -r move2kube-demos $ ls src README.md\tconfig-utils\tcustomers\tdocs\tfrontend\tgateway\torders In this project, all the apps have a pom.xml file. Use a custom transformer to place a Helm chart created from a template into each of those project directories.\nUse the Starlark based custom transformer located here. We copy it into the customizations sub-directory. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d add-custom-files-directories-in-custom-locations -r move2kube-transformers -o customizations Transform using this customization and specify the customization using the -c flag. $ move2kube transform -s src/ -c customizations/ --qa-skip Once the output is generated, one Helm chart was generated for each service and placed within the service directory. Also, note that every Helm chart project is named after the service it is meant for. The contents are shown below for reference:\n{% raw %} $ tree myproject myproject/ ├── config-utils │ ├── helm-chart │ │ └── config-utils │ │ ├── Chart.yaml │ │ ├── templates │ │ │ ├── config-utils-deployment.yaml │ │ │ ├── config-utils-ingress.yaml │ │ │ └── config-utils-service.yaml │ │ └── values.yaml │ ├── pom.xml │ └── src │ └── main │ └── java │ └── io │ └── konveyor │ └── demo │ └── config │ └── ApplicationConfiguration.java ├── customers │ ├── Makefile │ ├── helm-chart │ │ └── customers │ │ ├── Chart.yaml │ │ ├── templates │ │ │ ├── customers-deployment.yaml │ │ │ ├── customers-ingress.yaml │ │ │ └── customers-service.yaml │ │ └── values.yaml │ ├── pom.xml │ └── src │ └── main │ ├── java │ │ └── io │ │ └── konveyor │ │ └── demo │ │ └── ordermanagement │ │ ├── OrderManagementAppInitializer.java │ │ ├── config │ │ │ ├── PersistenceConfig.java │ │ │ └── WebConfig.java │ │ ├── controller │ │ │ └── CustomerController.java │ │ ├── exception │ │ │ ├── ResourceNotFoundException.java │ │ │ └── handler │ │ │ └── ExceptionHandlingController.java │ │ ├── model │ │ │ └── Customer.java │ │ ├── repository │ │ │ └── CustomerRepository.java │ │ └── service │ │ └── CustomerService.java │ └── resources │ ├── import.sql │ └── persistence.properties └── gateway ├── helm-chart │ └── snowdrop-dependencies │ ├── Chart.yaml │ ├── templates │ │ ├── snowdrop-dependencies-deployment.yaml │ │ ├── snowdrop-dependencies-ingress.yaml │ │ └── snowdrop-dependencies-service.yaml │ └── values.yaml ├── manifest.yml ├── pom.xml └── src ├── main │ ├── java │ │ ├── META-INF │ │ │ └── MANIFEST.MF │ │ └── io │ │ └── konveyor │ │ └── demo │ │ └── gateway │ │ ├── Application.java │ │ ├── command │ │ │ └── ProductCommand.java │ │ ├── controller │ │ │ ├── CustomersController.java │ │ │ ├── InventoryController.java │ │ │ └── OrdersController.java │ │ ├── exception │ │ │ ├── ResourceNotFoundException.java │ │ │ └── handler │ │ │ └── ExceptionHandlingController.java │ │ ├── model │ │ │ ├── Customer.java │ │ │ ├── Order.java │ │ │ ├── OrderItem.java │ │ │ ├── OrderSummary.java │ │ │ └── Product.java │ │ ├── repository │ │ │ ├── CustomerRepository.java │ │ │ ├── GenericRepository.java │ │ │ ├── InventoryRepository.java │ │ │ └── OrderRepository.java │ │ ├── serialization │ │ │ ├── CustomerDeserializer.java │ │ │ └── ProductDeserializer.java │ │ └── service │ │ ├── CustomersService.java │ │ ├── InventoryService.java │ │ └── OrdersService.java │ └── resources │ ├── application-local.properties │ ├── application-openshift.properties │ └── bootstrap.properties └── test ├── java │ └── io │ └── konveyor │ └── demo │ └── gateway │ ├── controller │ │ └── OrdersControllerTest.java │ ├── model │ │ └── OrderTest.java │ ├── repository │ │ ├── CustomerRepositoryTest.java │ │ ├── InventoryRepositoryTest.java │ │ └── OrderRepositoryTest.java │ └── service │ └── OrdersServiceTest.java └── resources ├── application-test.properties └── bootstrap.properties {% endraw %} Anatomy of transformer in add-custom-files-directories-in-custom-locations This custom transformer is more advanced compared to previous cases. It uses a Starlark script (customhelmchartgen.star) and several templatization features to achieve the per-service Helm chart requirement. Notice the {% raw %}{{\\ .ServiceName\\ }}{% endraw %} template in the file names of custom Helm chart template in the templates sub-directory. The contents of the add-custom-files-directories-in-custom-locations custom transformer are shown below:\n{% raw %}customization/add-custom-files-directories-in-custom-locations/ ├── customhelmchartgen.star ├── customhelmchartgen.yaml └── templates └── helm-chart └── {{\\ .ServiceName\\ }} ├── Chart.yaml ├── templates │ ├── {{\\ .ServiceName\\ }}-deployment.yaml │ ├── {{\\ .ServiceName\\ }}-ingress.yaml │ └── {{\\ .ServiceName\\ }}-service.yaml └── values.yaml{% endraw %} The code of the Starlark script (cat customizations/add-custom-files-directories-in-custom-locations/customhelmchartgen.star) is shown below. At a high level, the custom transformer detects a Java project if it finds pom.xml in the directory when the directory_detect() function is invoked in the detect phase. Once the Java project is detected, the corresponding project path and service name are passed to the transform phase through Move2Kube.\nIn the transform phase, the transform() function is invoked with the discovered service artifacts from the detect phase. These artifacts are used to fill the Helm chart templates shown above and produced as the output in a per-service directory structure.\n{% raw %}PomFile = \"pom.xml\" # Performs the detection of pom file and extracts service name def directory_detect(dir): dataFilePath = fs.pathjoin(dir, PomFile) if fs.exists(dataFilePath): serviceName = getServiceName(dataFilePath) return {serviceName: [{ \"paths\": {\"ProjectPath\": [dir]} }] } # Creates the customized helm chart for every service def transform(new_artifacts, old_artifacts): pathMappings = [] artifacts = [] pathTemplate = \"{{ SourceRel .ServiceFsPath }}\" for v in new_artifacts: serviceName = v[\"configs\"][\"Service\"][\"serviceName\"] dir = v['paths']['ProjectPath'][0] # Create a path template for the service pathTemplateName = serviceName.replace(\"-\", \"\") + 'path' tplPathData = {'ServiceFsPath': dir, 'PathTemplateName': pathTemplateName} pathMappings.append({'type': 'PathTemplate', \\ 'sourcePath': pathTemplate, \\ 'templateConfig': tplPathData}) # Since the helm chart uses the same templating character {{ }} as Golang templates, # we use `SpecialTemplate` type here where the templating character is \u003c~ ~\u003e. # The `Template` type can be used for all normal cases pathMappings.append({'type': 'SpecialTemplate', \\ 'destinationPath': \"{{ .\" + pathTemplateName + \" }}\", \\ 'templateConfig': {'ServiceFsPath': dir, 'ServiceName': serviceName}}) pathMappings.append({'type': 'Source', \\ 'sourcePath': \"{{ .\" + pathTemplateName + \" }}\", 'destinationPath': \"{{ .\" + pathTemplateName + \" }}\"}) return {'pathMappings': pathMappings, 'artifacts': artifacts} # Extracts service name from pom file def getServiceName(filePath): data = fs.read(filePath) lines = data.splitlines() for l in lines: if 'artifactId' in l: t = l.split('\u003e') t2 = t[1].split('\u003c') return t2[0]{% endraw %} Source\n","categories":"","description":"","excerpt":"Move2Kube allows custom template files to be added to any directory. …","ref":"/docs/move2kube/tutorials/customizeoutput/addcustfiledir/","tags":"","title":"Add custom files and directories in custom locations"},{"body":"This tutorial illustrates how to parameterize a custom field in the Helm chart generated by Move2Kube.\nStart by creating an empty workspace directory named workspace and make it the current working directory. Assume all commands are executed within this directory. $ mkdir workspace \u0026\u0026 cd workspace Use the enterprise-app as input for this flow. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/src -r move2kube-demos $ ls src README.md\tconfig-utils\tcustomers\tdocs\tfrontend\tgateway\torders Run Move2Kube without any customization. The relevant snippet from the deployment YAML generated in the path myproject/deploy/yamls-parameterized/helm-chart/myproject/templates/orders-deployment.yaml is shown below. When complete, delete the myproject directory.\n$ move2kube transform -s src/ --qa-skip \u0026\u0026 cat myproject/deploy/yamls-parameterized/helm-chart/myproject/templates/orders-deployment.yaml \u0026\u0026 rm -rf myproject apiVersion: apps/v1 kind: Deployment metadata: annotations: move2kube.konveyor.io/service.expose: \"true\" creationTimestamp: null labels: move2kube.konveyor.io/service: orders name: orders spec: progressDeadlineSeconds: 600 replicas: {% raw %}{{ index .Values \"common\" \"replicas\" }}{% endraw %} revisionHistoryLimit: 10 selector: matchLabels: move2kube.konveyor.io/service: orders strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: annotations: move2kube.konveyor.io/service.expose: \"true\" creationTimestamp: null labels: move2kube.konveyor.io/service: orders name: orders spec: containers: - env: - name: PORT value: \"8080\" image: quay.io/myproject/orders:latest imagePullPolicy: Always name: orders ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: quay-io-imagepullsecret restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: {} Notice that except for the replicas field, no other field is parameterized.\nUse a custom configured version of the parameterizer transformer to view the parameterizing the other fields in the transformer. Then copy it into the customizations sub-directory. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d custom-helm-kustomize-octemplates-parameterization -r move2kube-transformers -o customizations Transform using this customization and specify the customization using the -c flag. $ move2kube transform -s src/ -c customizations/ --qa-skip When the output is generated, observe the same deployment as mentioned before.\n$ cat myproject/deploy/yamls-parameterized/helm-chart/myproject/templates/orders-deployment.yaml {% raw %}apiVersion: apps/v1 kind: Deployment metadata: annotations: move2kube.konveyor.io/service.expose: \"true\" creationTimestamp: null labels: move2kube.konveyor.io/service: orders name: orders spec: progressDeadlineSeconds: 600 replicas: {{ index .Values \"common\" \"replicas\" }} revisionHistoryLimit: 10 selector: matchLabels: move2kube.konveyor.io/service: orders strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: annotations: move2kube.konveyor.io/service.expose: \"true\" creationTimestamp: null labels: move2kube.konveyor.io/service: orders name: orders spec: containers: - env: - name: PORT value: \"8080\" image: {{ index .Values \"imageregistry\" \"url\" }}/{{ index .Values \"imageregistry\" \"namespace\" }}/{{ index .Values \"services\" \"orders\" \"containers\" \"orders\" \"image\" \"name\" }}:{{ index .Values \"services\" \"orders\" \"containers\" \"orders\" \"image\" \"tag\" }} imagePullPolicy: Always name: orders ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: quay-io-imagepullsecret restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: {} {% endraw %} A few of the parameterized YAMLs are:\n$ cat myproject/deploy/yamls-parameterized/helm-chart/myproject/values-prod.yaml common: replicas: 10 imageregistry: namespace: konveyor url: quay.io services: config-utils: containers: config-utils: image: name: myimage tag: latest customers: containers: customers: image: name: myimage tag: latest frontend: containers: frontend: image: name: frontend tag: latest gateway: containers: gateway: image: name: myimage tag: latest orders: containers: orders: image: name: orders tag: latest $ cat myproject/deploy/yamls-parameterized/kustomize/overlays/prod/apps-v1-deployment-orders.yaml - op: replace path: /spec/replicas value: 10 - op: replace path: /spec/template/spec/containers/0/image value: orders $ cat myproject/deploy/yamls-parameterized/openshift-template/parameters-prod.yaml COMMON_REPLICAS=10 IMAGEREGISTRY_URL=quay.io IMAGEREGISTRY_NAMESPACE=konveyor SERVICES_FRONTEND_CONTAINERS_FRONTEND_IMAGE_TAG=latest SERVICES_GATEWAY_CONTAINERS_GATEWAY_IMAGE_NAME=myimage SERVICES_ORDERS_CONTAINERS_ORDERS_IMAGE_NAME=orders SERVICES_ORDERS_CONTAINERS_ORDERS_IMAGE_TAG=latest SERVICES_CUSTOMERS_TOMCAT_CONTAINERS_CUSTOMERS_TOMCAT_IMAGE_NAME=myimage SERVICES_CUSTOMERS_TOMCAT_CONTAINERS_CUSTOMERS_TOMCAT_IMAGE_TAG=latest SERVICES_FRONTEND_CONTAINERS_FRONTEND_IMAGE_NAME=frontend SERVICES_CONFIG_UTILS_CONTAINERS_CONFIG_UTILS_IMAGE_NAME=myimage SERVICES_CONFIG_UTILS_CONTAINERS_CONFIG_UTILS_IMAGE_TAG=latest SERVICES_GATEWAY_CONTAINERS_GATEWAY_IMAGE_TAG=latest Anatomy of the parameterizer The contents of parameterizer are as shown below:\n$ ls customizations/custom-helm-kustomize-octemplates-parameterization/ README.md\tdeployment-parameterizers.yaml\tparameterizers.yaml The transformer configuration is in parameterizers.yaml and the parameterization config is in deployment-parameterizers.yaml. The configuration below specifies the fields that need to be parameterized, how to parameterize them, and the default values those fields should take. This can be extended to parameterize any field in any Kubernetes Yaml, and generate the appropriate helm chart, Kustomize YAMLs and Openshift templates.\n$ cat customizations/custom-helm-kustomize-octemplates-parameterization/deployment-parameterizers.yaml {% raw %}apiVersion: move2kube.konveyor.io/v1alpha1 kind: Parameterizer metadata: name: deployment-parameterizers spec: parameterizers: - target: \"spec.replicas\" template: \"${common.replicas}\" default: 10 filters: - kind: Deployment apiVersion: \".*/v1.*\" - target: 'spec.template.spec.containers.[containerName:name].image' template: '${imageregistry.url}/${imageregistry.namespace}/${services.$(metadataName).containers.$(containerName).image.name}:${services.$(metadataName).containers.$(containerName).image.tag}' default: quay.io/konveyor/myimage:latest filters: - kind: Deployment apiVersion: \".*/v1.*\" parameters: - name: services.$(metadataName).containers.$(containerName).image.name values: - envs: [dev, staging, prod] metadataName: frontend value: frontend - envs: [prod] metadataName: orders value: orders custom: containerName: orders {% endraw %} Next step is adding custom files and directories in custom locations.\nSource\n","categories":"","description":"","excerpt":"This tutorial illustrates how to parameterize a custom field in the …","ref":"/docs/move2kube/tutorials/customizeoutput/paramcustomfieldshelm/","tags":"","title":"Parameterizing custom fields in Helm Chart, Kustomize, OC templates"},{"body":"Move2Kube generates Kubernetes YAMLs based on the needs of the application, but there might be situations that might require specific fields to be different in the output. This example illustrates how to add an annotation to the Ingress YAML specifying an ingress class.\nCreate an empty workspace directory named workspace and make it the current working directory. Assume all commands are executed within this directory. $ mkdir workspace \u0026\u0026 cd workspace Use the enterprise-app as the input for this flow. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/src -r move2kube-demos $ ls src README.md\tconfig-utils\tcustomers\tdocs\tfrontend\tgateway\torders Run Move2Kube without any customization and the output ingress does not have any annotation. Once done, delete the myproject directory. $ move2kube transform -s src/ --qa-skip \u0026\u0026 cat myproject/deploy/yamls/myproject-ingress.yaml \u0026\u0026 rm -rf myproject apiVersion: networking.k8s.io/v1 kind: Ingress metadata: creationTimestamp: null labels: move2kube.konveyor.io/service: myproject name: myproject Get the Starlark based custom transformer here and copy it into the customizations sub-directory. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d add-custom-kubernetes-annotation -r move2kube-transformers -o customizations Transform using this customization and specify it using the -c flag. $ move2kube transform -s src/ -c customizations/ --qa-skip Once the output is generated, note in the snippet of the ingress file (myproject/deploy/yamls/myproject-ingress.yaml) that there is an annotation for the ingress class added (kubernetes.io/ingress.class: haproxy):\n$ cat myproject/deploy/yamls/myproject-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: haproxy creationTimestamp: null labels: move2kube.konveyor.io/service: myproject name: myproject Anatomy of ingress annotator transformer This custom transformer uses a configuration YAML (ingress-annotator.yaml) and a Starlark script (ingress-annotator.star) to add an annotation to the ingress YAML. The contents of custom transformer are shown below:\n$ ls customizations/add-custom-kubernetes-annotation ingress-annotator.star ingress-annotator.yaml The configuration YAML specifies that the custom transformer consumes and produces a Kubernetes YAML artifact type as shown in the consumes and produces section.\n$ cat customizations/add-custom-kubernetes-annotation/ingress-annotator.yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: Transformer metadata: name: IngressAnnotator labels: move2kube.konveyor.io/built-in: false spec: class: \"Starlark\" consumes: KubernetesYamls: merge: false # Ensures a artifact of this type gets immediately intercepted by this transformer mode: \"MandatoryPassThrough\" produces: KubernetesYamls: disabled: false config: starFile: \"ingress-annotator.star\" The code of the Starlark script is shown below. At a high-level, the code requires only the transform() function as it acts upon any Kubernetes YAML generated within Move2Kube. The transform() function loops through every YAML generated for every detected service, checks whether it is an ingress YAML, and if so adds the annotation. The path mappings are meant to persist these changes.\n$ cat customizations/add-custom-kubernetes-annotation/ingress-annotator.star {% raw %}def transform(new_artifacts, old_artifacts): pathMappings = [] artifacts = [] for a in new_artifacts: yamlsPath = a[\"paths\"][\"KubernetesYamls\"][0] serviceName = a[\"name\"] artifacts.append(a) fileList = fs.readdir(yamlsPath) yamlsBasePath = yamlsPath.split(\"/\")[-1] # Create a custom path template for the service, whose values gets filled and can be used in other pathmappings pathTemplateName = serviceName.replace(\"-\", \"\") + yamlsBasePath tplPathData = {'PathTemplateName': pathTemplateName} pathMappings.append({'type': 'PathTemplate', \\ 'sourcePath': \"{{ OutputRel \\\"\" + yamlsPath + \"\\\" }}\", \\ 'templateConfig': tplPathData}) for f in fileList: filePath = fs.pathjoin(yamlsPath, f) s = fs.read(filePath) yamlData = yaml.loads(s) if yamlData['kind'] != 'Ingress': continue if 'annotations' not in yamlData['metadata']: yamlData['metadata']['annotations'] = {'kubernetes.io/ingress.class': 'haproxy'} else: yamlData['metadata']['annotations']['kubernetes.io/ingress.class'] = 'haproxy' s = yaml.dumps(yamlData) fs.write(filePath, s) pathMappings.append({'type': 'Default', \\ 'sourcePath': yamlsPath, \\ 'destinationPath': \"{{ .\" + pathTemplateName + \" }}\"}) return {'pathMappings': pathMappings, 'artifacts': artifacts}{% endraw %} This tutorial can be replicated in the UI by uploading the zip file of the custom transformer as a customization. Obtain the zip of the source and customization by adding a -z to the end of the commands used in steps 2 and 4.\nThe next step is parameterizing custom fields in Helm Chart, Kustomize, and OC Templates.\nSource\n","categories":"","description":"","excerpt":"Move2Kube generates Kubernetes YAMLs based on the needs of the …","ref":"/docs/move2kube/tutorials/customizeoutput/customannotationsyaml/","tags":"","title":"Add custom annotations to Kubernetes YAMLs"},{"body":"In this tutorial, Move2Kube will add a custom Dockerfile, and a custom file.\nCreate an empty workspace directory named workspace and make it the current working directory. Assume all commands are executed within this directory. $ mkdir workspace \u0026\u0026 cd workspace Use the enterprise-app as input for this flow. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/enterprise-app/src -r move2kube-demos $ ls src README.md\tconfig-utils\tcustomers\tdocs\tfrontend\tgateway\torders Run Move2Kube without any customization. If the Dockerfile is generated for the frontend app, it uses registry.access.redhat.com/ubi8/nodejs-12 as the base image. There are no scripts named start-nodejs.sh in the frontend service directory. The Kubernetes YAMLs are generated in myproject/deploy/yamls directory. $ move2kube transform -s src/ --qa-skip \u0026\u0026 ls myproject/source/frontend \u0026\u0026 cat myproject/source/frontend/Dockerfile \u0026\u0026 ls myproject/deploy \u0026\u0026 rm -rf myproject Dockerfile\tREADME.md\tdr-surge.js\tmanifest.yml\tpackage-lock.json\tserver.js\tstories\ttest-setup.js\twebpack.common.js\twebpack.prod.js LICENSE\t__mocks__\tjest.config.js\tnodemon.json\tpackage.json\tsrc\tstylePaths.js\ttsconfig.json\twebpack.dev.js FROM registry.access.redhat.com/ubi8/nodejs-12 COPY . . RUN npm install RUN npm run build EXPOSE 8080 CMD npm run start cicd\tcompose\tknative\tknative-parameterized\tyamls\tyamls-parameterized The next steps will:\nThe base image of the Dockerfile generated for Node.js from registry.access.redhat.com/ubi8/nodejs-12 to quay.io/konveyor/nodejs-12. Add a new script named start-nodejs.sh in the Node.js app directories along with the Dockerfile in the frontend directory. Change the location of Kubernetes YAMLs from myproject/deploy/yamls to myproject/yamls-elsewhere. Use a custom configured version of the Node.js built-in transformer and the Kubernetes built-in transformer to achieve this. Copy it into the customizations sub-directory. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d custom-dockerfile-change-built-in-behavior -r move2kube-transformers -o customizations Transform using this customization and specify the customization using the -c flag. $ move2kube transform -s src/ -c customizations/ --qa-skip Once the output is generated, note the following:\nThe Dockerfile generated for the frontend app contains the custom base image. A new file named start-nodejs.sh was generated in the frontend directory. The Kubernetes YAMLs are now generated in myproject/yamls-elsewhere directory and the parameterized YAMLs are also in myproject/yamls-elsewhere-parameterized directory. $ ls myproject/source/frontend Dockerfile\tREADME.md\tdr-surge.js\tmanifest.yml\tpackage-lock.json\tserver.js\tstart-nodejs.sh\tstylePaths.js\ttsconfig.json\twebpack.dev.js LICENSE\t__mocks__\tjest.config.js\tnodemon.json\tpackage.json\tsrc\tstories\ttest-setup.js\twebpack.common.js\twebpack.prod.js $ cat myproject/source/frontend/Dockerfile FROM quay.io/konveyor/nodejs-12 COPY . . RUN npm install RUN npm run build EXPOSE 8080 CMD sh start-nodejs.sh $ ls myproject Readme.md\tdeploy\tscripts\tsource\tyamls-elsewhere\tyamls-elsewhere-parameterized Anatomy of transformers in custom-dockerfile-change-built-in-behavior The two customized transformers in the directory are nodejs and kubernetes. The contents of custom-dockerfile-custom-files are shown below:\n$ tree customizations customizations └── custom-dockerfile-change-built-in-behavior ├── kubernetes │ └── kubernetes.yaml └── nodejs ├── nodejs.yaml └── templates ├── Dockerfile └── start-nodejs.sh To custom configure a built-in transformer, copy the built-in transformer’s configuration directory from the move2kube source, change the configurations, use it as a customization, and make it override the built-in transformer using the override config in the yaml.\nIn this case, change the Dockerfile template, add a script, and change the transformer configuration YAML.\nTo change the template, the custom template is placed in customizations/custom-dockerfile-change-built-in-behavior/nodejs/templates/Dockerfile. The template is the same as the one used in the built-in transformer, except it is a custom base image and a custom CMD. {% raw %}$ cat customizations/custom-dockerfile-change-built-in-behavior/nodejs/templates/Dockerfile FROM quay.io/konveyor/nodejs-12 COPY . . RUN npm install {{- if .Build }} RUN npm run build {{- end}} EXPOSE {{ .Port }} CMD sh start-nodejs.sh{% endraw %} Add customizations/custom-dockerfile-change-built-in-behavior/nodejs/templates/start-nodejs.sh. $ ls customizations/custom-dockerfile-change-built-in-behavior/nodejs/templates/ Dockerfile\tstart-nodejs.sh The transformer.yaml is the transformer configuration with two changes compared to the built-in transformer: The custom transformer name is Nodejs-CustomFiles (see name field in the metadata section). Specify an override section which is asking Move2Kube to disable the transformer named Nodejs-Dockerfile if it is present. $ cat customizations/custom-dockerfile-change-built-in-behavior/nodejs/nodejs.yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: Transformer metadata: name: Nodejs-CustomFiles labels: move2kube.konveyor.io/task: containerization move2kube.konveyor.io/built-in: true spec: class: \"NodejsDockerfileGenerator\" directoryDetect: levels: -1 consumes: Service: merge: false produces: Dockerfile: disabled: false DockerfileForService: disabled: false override: matchLabels: move2kube.konveyor.io/name: Nodejs-Dockerfile config: defaultNodejsVersion: \"12\" In the kubernetes transformer, change the name and override the config. Also change the default behavior of the transformer, which is to put the Kubernetes yamls in deploy/yamls directory by changing the spec.config.outputPath to yamls-elsewhere. $ cat customizations/custom-dockerfile-change-built-in-behavior/kubernetes/kubernetes.yaml {% raw %}apiVersion: move2kube.konveyor.io/v1alpha1 kind: Transformer metadata: name: KubernetesWithCustomOutputDirectory labels: move2kube.konveyor.io/built-in: true spec: class: \"Kubernetes\" directoryDetect: levels: 0 consumes: IR: merge: true produces: KubernetesYamls: disabled: false override: matchLabels: move2kube.konveyor.io/name: Kubernetes dependency: matchLabels: move2kube.konveyor.io/kubernetesclusterselector: \"true\" config: outputPath: \"yamls-elsewhere\" ingressName: \"{{ .ProjectName }}\"{% endraw %} The next step is adding custom annotations to Kubernetes YAMLs.\nSource\n","categories":"","description":"","excerpt":"In this tutorial, Move2Kube will add a custom Dockerfile, and a custom …","ref":"/docs/move2kube/tutorials/customizeoutput/customgendockerfile/","tags":"","title":"Customize generated Dockerfile and built-in transformer behavior"},{"body":"This tutorial picks up after the migration workflow. After examining the output that Move2Kube generated for the application, there might be some things to change.\nThis section looks at how to customize the output of Move2Kube to the needs using Transformers.\nSource\n","categories":"","description":"","excerpt":"This tutorial picks up after the migration workflow. After examining …","ref":"/docs/move2kube/tutorials/customizeoutput/","tags":"","title":"Customizing the output"},{"body":"Move2Kube already supports targeting across multiple clusters including: Kubernetes, Openshift, IBM-IKS, IBM-Openshift, Azure-EKS, Azure-AKS and GCP-GKS. There might be situations that require generating Kubernetes YAMLs to target a particular cluster. This tutorial shows how to use Konveyor Move2Kube to change the versions of existing Kubernetes resources to target a particular cluster. Move2Kube can also be customized to generate Kubernetes YAMLS deployable on a particular cluster.\nPrerequisites\nInstall the Move2Kube CLI tool.\nInstall Kubectl.\nUse the kubernetes-to-kubernetes sample. This directory has some Kubernetes YAMLs that deploy a web app with multiple services. There are three services: a frontend website in PHP, a backend API in NodeJS, and a cache service using Redis.\nProcedure\nCollect data about the Kubernetes cluster using move2kube collect. Limit the collection to only cluster information using the -a k8s annotation flag. Note: Before running the below command, log in to the target cluster. To verify, run kubectl get pods.\n$ move2kube collect -a k8s INFO[0000] Begin collection INFO[0000] [*collector.ClusterCollector] Begin collection INFO[0006] [*collector.ClusterCollector] Done INFO[0006] [*collector.ImagesCollector] Begin collection INFO[0006] [*collector.ImagesCollector] Done INFO[0006] Collection done INFO[0006] Collect Output in [/Users/user/m2k_collect]. Copy this directory into the source directory to be used for planning. The data collected will be stored in a new directory called ./m2k_collect.\n$ ls m2k_collect cf The ./m2k_collect/clusters directory contains the YAML file which has the cluster application information including the buildpacks that are supported, the memory, the number of instances and the ports that are supported. If there are environment variables, it collects that information too. The name of the cluster can be found in the metadata.name file, and can be renamed in the YAML file.\nFor this tutorial, the move2kube collect output YAML file which contains the cluster related information is stored here. The cluster metadata.name was renamed as my-kubernetes-cluster in the YAML file.\nUse a custom configured version of the clusterselector transformer.\nDownload the custom-cluster-selector transformer into the customizations sub-directory. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d custom-cluster-selector -r move2kube-transformers -o customizations The customizations/custom-cluster-selector/transformer.yaml is the transformer configuration. There are two changes in the custom-cluster-selector/transformer.yaml compared to the built-in clusterselector/transformer.yaml :\nThe name of the custom transformer is CustomClusterSelector (see name field in the metadata section). Specify an override section which is asking Move2Kube to disable the transformer named ClusterSelector if CustomClusterSelector transformer is present. $ cat customizations/custom-cluster-selector/transformer.yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: Transformer metadata: name: CustomClusterSelector labels: move2kube.konveyor.io/built-in: true move2kube.konveyor.io/kubernetesclusterselector: true spec: class: \"ClusterSelectorTransformer\" directoryDetect: levels: 0 consumes: IR: merge: true mode: OnDemandPassThrough KubernetesOrgYamlsInSource: merge: false mode: OnDemandPassThrough produces: IR: disabled: false KubernetesOrgYamlsInSource: disabled: false override: matchLabels: move2kube.konveyor.io/name: ClusterSelector To check what information move2kube collect -a k8s collected, view the content inside the my-kubernetes-cluster.yaml.\n$ cat customizations/custom-cluster-selector/clusters/my-kubernetes-cluster.yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: ClusterMetadata metadata: name: my-kubernetes-cluster spec: storageClasses: - default - ibmc-file-bronze - ibmc-file-bronze-gid - ibmc-file-custom - ibmc-file-gold - ibmc-file-gold-gid - ibmc-file-retain-bronze - ibmc-file-retain-custom - ibmc-file-retain-gold - ibmc-file-retain-silver - ibmc-file-silver - ibmc-file-silver-gid apiKindVersionMap: APIService: - apiregistration.k8s.io/v1 BGPConfiguration: - crd.projectcalico.org/v1 BGPPeer: - crd.projectcalico.org/v1 Binding: - v1 BlockAffinity: - crd.projectcalico.org/v1 CSIDriver: - storage.k8s.io/v1 CSINode: - storage.k8s.io/v1 CSIStorageCapacity: - storage.k8s.io/v1beta1 CatalogSource: - operators.coreos.com/v1alpha1 CertificateSigningRequest: - certificates.k8s.io/v1 ClusterInformation: - crd.projectcalico.org/v1 ClusterRole: - rbac.authorization.k8s.io/v1 ClusterRoleBinding: - rbac.authorization.k8s.io/v1 ClusterServiceVersion: - operators.coreos.com/v1alpha1 ComponentStatus: - v1 ConfigMap: - v1 ControllerRevision: - apps/v1 CronJob: - batch/v1 - batch/v1beta1 CustomResourceDefinition: - apiextensions.k8s.io/v1 DaemonSet: - apps/v1 Deployment: - apps/v1 EndpointSlice: - discovery.k8s.io/v1 - discovery.k8s.io/v1beta1 Endpoints: - v1 Event: - events.k8s.io/v1 - events.k8s.io/v1beta1 - v1 Eviction: - v1 FelixConfiguration: - crd.projectcalico.org/v1 FlowSchema: - flowcontrol.apiserver.k8s.io/v1beta1 GlobalNetworkPolicy: - crd.projectcalico.org/v1 GlobalNetworkSet: - crd.projectcalico.org/v1 GuestBook: - webapp.metamagical.dev/v1 HorizontalPodAutoscaler: - autoscaling/v1 - autoscaling/v2beta1 - autoscaling/v2beta2 HostEndpoint: - crd.projectcalico.org/v1 IPAMBlock: - crd.projectcalico.org/v1 IPAMConfig: - crd.projectcalico.org/v1 IPAMHandle: - crd.projectcalico.org/v1 IPPool: - crd.projectcalico.org/v1 Ingress: - networking.k8s.io/v1 IngressClass: - networking.k8s.io/v1 InstallPlan: - operators.coreos.com/v1alpha1 Job: - batch/v1 KubeControllersConfiguration: - crd.projectcalico.org/v1 Lease: - coordination.k8s.io/v1 LimitRange: - v1 LocalSubjectAccessReview: - authorization.k8s.io/v1 MutatingWebhookConfiguration: - admissionregistration.k8s.io/v1 Namespace: - v1 NetworkPolicy: - networking.k8s.io/v1 - crd.projectcalico.org/v1 NetworkSet: - crd.projectcalico.org/v1 Node: - v1 NodeMetrics: - metrics.k8s.io/v1beta1 NodeProxyOptions: - v1 Operator: - operators.coreos.com/v1 OperatorGroup: - operators.coreos.com/v1 - operators.coreos.com/v1alpha2 PersistentVolume: - v1 PersistentVolumeClaim: - v1 Pod: - v1 PodAttachOptions: - v1 PodDisruptionBudget: - policy/v1 - policy/v1beta1 PodExecOptions: - v1 PodMetrics: - metrics.k8s.io/v1beta1 PodPortForwardOptions: - v1 PodProxyOptions: - v1 PodSecurityPolicy: - policy/v1beta1 PodTemplate: - v1 PriorityClass: - scheduling.k8s.io/v1 PriorityLevelConfiguration: - flowcontrol.apiserver.k8s.io/v1beta1 RBACSync: - ibm.com/v1alpha1 Redis: - webapp.metamagical.dev/v1 ReplicaSet: - apps/v1 ReplicationController: - v1 ResourceQuota: - v1 Role: - rbac.authorization.k8s.io/v1 RoleBinding: - rbac.authorization.k8s.io/v1 RuntimeClass: - node.k8s.io/v1 - node.k8s.io/v1beta1 Scale: - apps/v1 - v1 Secret: - v1 SelfSubjectAccessReview: - authorization.k8s.io/v1 SelfSubjectRulesReview: - authorization.k8s.io/v1 Service: - v1 ServiceAccount: - v1 ServiceProxyOptions: - v1 StatefulSet: - apps/v1 StorageClass: - storage.k8s.io/v1 SubjectAccessReview: - authorization.k8s.io/v1 Subscription: - operators.coreos.com/v1alpha1 TokenRequest: - v1 TokenReview: - authentication.k8s.io/v1 ValidatingWebhookConfiguration: - admissionregistration.k8s.io/v1 VolumeAttachment: - storage.k8s.io/v1 VolumeSnapshot: - snapshot.storage.k8s.io/v1 - snapshot.storage.k8s.io/v1beta1 VolumeSnapshotClass: - snapshot.storage.k8s.io/v1 - snapshot.storage.k8s.io/v1beta1 VolumeSnapshotContent: - snapshot.storage.k8s.io/v1 - snapshot.storage.k8s.io/v1beta1 The my-kubernetes-cluster.yaml has information about the target cluster under the spec field, including the storageClasses, Deployment, Service, Ingress and NetworkPolicy versions supported by the target cluster. Move2Kube will use this information and generate the Kubernetes YAMLs which are tailored for the given target cluster.\nDownload the kubernetes-to-kubernetes sample. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/kubernetes-to-kubernetes -r move2kube-demos $ ls kubernetes-to-kubernetes/ api-deployment.yaml api-service.yaml redis-deployment.yaml redis-service.yaml web-deployment.yaml web-ingress.yaml web-service.yaml Generate a plan file by providing the custom-cluster-selector transformer inside the downloaded customizations folder to Move2Kube using the -c flag. $ move2kube plan -s kubernetes-to-kubernetes -c customizations INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [CloudFoundry] Planning transformation INFO[0000] [CloudFoundry] Done INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [Base Directory] Identified 0 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in . INFO[0000] Identified 1 named services and 0 to-be-named services in . INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 1 named services and 1 to-be-named services INFO[0000] [Named Services] Identified 1 named services INFO[0000] No of services identified : 1 INFO[0000] Plan can be found at [/Users/user/m2k.plan]. View the generated plan file in YAML format. Notice Move2Kube has detected the custom-cluster-selector as customizationsDir which will be used during the Transform phase. $ cat m2k.plan apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: kubernetes-to-kubernetes customizationsDir: custom-cluster-selector apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: kubernetes-to-kubernetes customizationsDir: custom-cluster-selector services: move2kube-transformers: - transformerName: Parameterizer paths: KubernetesYamls: - . ServiceDirPath: - . - transformerName: KubernetesVersionChanger type: KubernetesOrgYamlsInSource paths: KubernetesYamls: - . ServiceDirPath: - . transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/transformer.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/transformer.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/transformer.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/transformer.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimagespushscript/transformer.yaml CustomClusterSelector: m2kassets/custom/transformer.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/transformer.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/transformer.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/transformer.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/transformer.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/transformer.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/transformer.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/transformer.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/transformer.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/transformer.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/transformer.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/transformer.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/transformer.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/transformer.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/transformer.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/transformer.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/transformer.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/transformer.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/transformer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/transformer.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/transformer.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/transformer.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/transformer.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/transformer.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/transformer.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/transformer.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/transformer.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/transformer.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/transformer.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/transformer.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/transformer.yaml Run the transformation using move2kube transform. $ move2kube transform INFO[0000] Detected a plan file at path /home/user/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: ID: move2kube.transformers.types Hints: [Services that don't support any of the transformer types you are interested in will be ignored.] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] \u003e [✓] Maven [✓] Rust-Dockerfile [✓] Tomcat [✓] ZuulAnalyser [✓] CustomClusterSelector [✓] Jboss [✓] Parameterizer [✓] ReadMeGenerator [✓] WarRouter [✓] DockerfileDetector [✓] Gradle [✓] Python-Dockerfile [✓] WinConsoleApp-Dockerfile [✓] Buildconfig [✓] EarAnalyser [✓] Knative [✓] Nodejs-Dockerfile [✓] WinSLWebApp-Dockerfile [✓] ContainerImagesPushScriptGenerator [✓] DockerfileImageBuildScript [✓] Tekton [✓] DotNetCore-Dockerfile [✓] Liberty [✓] DockerfileParser [✓] Golang-Dockerfile [✓] WinWebApp-Dockerfile [✓] CloudFoundry [✓] ComposeGenerator [✓] Kubernetes [✓] Ruby-Dockerfile [✓] WarAnalyser [✓] ComposeAnalyser [✓] Jar [✓] PHP-Dockerfile [✓] EarRouter [✓] KubernetesVersionChanger Accept the default by pressing the Enter key. ? Select all services that are needed: ID: move2kube.services.[].enable Hints: [The services unselected here will be ignored.] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] \u003e [✓] myproject Accept the detected service. INFO[0017] Starting Plan Transformation INFO[0017] Iteration 1 INFO[0017] Iteration 2 - 1 artifacts to process INFO[0017] Transformer CustomClusterSelector processing 1 artifacts ? Choose the cluster type: ID: move2kube.target.clustertype Hints: [Choose the cluster type you would like to target] [Use arrows to move, type to filter] GCP-GKE IBM-IKS IBM-Openshift Kubernetes \u003e my-kubernetes-cluster Openshift AWS-EKS Openshift Select my-kubernetes-cluster as the target cluster type to deploy to. INFO[0393] Transformer CustomClusterSelector Done INFO[0393] Transformer KubernetesVersionChanger processing 1 artifacts INFO[0393] Transformer KubernetesVersionChanger Done INFO[0393] Created 1 pathMappings and 1 artifacts. Total Path Mappings : 1. Total Artifacts : 1. INFO[0393] Iteration 3 - 1 artifacts to process INFO[0393] Transformer Parameterizer processing 1 artifacts INFO[0393] Transformer Parameterizer Done INFO[0393] Plan Transformation done INFO[0393] Transformed target artifacts can be found at [/Users/user/myproject]. The transformation has completed and generated a directory called myproject. Note: The name of the output directory is the same as the project name (by default myproject). The project name can be changed using the -n flag.\n$ ls customizations kubernetes-to-kubernetes m2k.plan m2kconfig.yaml m2kqacache.yaml myproject $ ls myproject/source kubernetes-to-kubernetes-versionchanged kubernetes-to-kubernetes-versionchanged-parameterized The applications can now be deployed to Kubernetes using these generated artifacts.\nExploring the output The full structure of the output directory can be seen by executing the tree command.\n$ tree myproject/ myproject └── source ├── kubernetes-to-kubernetes-versionchanged │ ├── api-deployment.yaml │ ├── api-service.yaml │ ├── redis-deployment.yaml │ ├── redis-service.yaml │ ├── web-deployment.yaml │ ├── web-ingress.yaml │ └── web-service.yaml └── kubernetes-to-kubernetes-versionchanged-parameterized ├── helm-chart │ └── move2kube-transformers │ ├── Chart.yaml │ ├── templates │ │ ├── api-deployment.yaml │ │ ├── api-service.yaml │ │ ├── redis-deployment.yaml │ │ ├── redis-service.yaml │ │ ├── web-deployment.yaml │ │ ├── web-ingress.yaml │ │ └── web-service.yaml │ ├── values-dev.yaml │ ├── values-prod.yaml │ └── values-staging.yaml ├── kustomize │ ├── base │ │ ├── api-deployment.yaml │ │ ├── api-service.yaml │ │ ├── kustomization.yaml │ │ ├── redis-deployment.yaml │ │ ├── redis-service.yaml │ │ ├── web-deployment.yaml │ │ ├── web-ingress.yaml │ │ └── web-service.yaml │ └── overlays │ ├── dev │ │ ├── apps-v1-deployment-api.yaml │ │ ├── apps-v1-deployment-redis.yaml │ │ ├── apps-v1-deployment-web.yaml │ │ └── kustomization.yaml │ ├── prod │ │ ├── apps-v1-deployment-api.yaml │ │ ├── apps-v1-deployment-redis.yaml │ │ ├── apps-v1-deployment-web.yaml │ │ └── kustomization.yaml │ └── staging │ ├── apps-v1-deployment-api.yaml │ ├── apps-v1-deployment-redis.yaml │ ├── apps-v1-deployment-web.yaml │ └── kustomization.yaml └── openshift-template ├── parameters-dev.yaml ├── parameters-prod.yaml ├── parameters-staging.yaml └── template.yaml 13 directories, 42 files The myproject/source/kubernetes-to-kubernetes-versionchanged directory has the new Kubernetes YAMLs (deployment/service/ingress/etc.) which are tailored to meet the target cluster requirements.\nSome other things to observe:\nThe Helm chart in the source/kubernetes-to-kubernetes-versionchanged-parameterized/helm-chart directory. The Kustomize YAMLs in the source/kubernetes-to-kubernetes-versionchanged-parameterized/kustomize directory. The Openshift Template in the source/kubernetes-to-kubernetes-versionchanged-parameterized/openshift-template directory. For more details on how to customize the parameterization look at the documentation.\nConclusion Follow these steps to complete the tutorial.\nRun move2kube collect -a k8s after logging in to the cluster to collect the cluster-related information inside the m2k_collect folder. Copy-paste the generated YAML file inside m2k_collect/cluster/ to custom-cluster-selector/clusters/. While running move2kube plan -s src -c custom-cluster-selector (or transform move2kube transform -s src -c custom-cluster-selector), provide the customization folder to Move2Kube using the -c flag. Select the target cluster in the QA during Transform phase. Move2Kube will generate the Kubernetes YAMLs for the target cluster. Given Kubernetes YAMLs, this tutorial showed how Move2Kube can create/transform the Kubernetes YAMLs to target a specific cluster.\nSource\n","categories":"","description":"","excerpt":"Move2Kube already supports targeting across multiple clusters …","ref":"/docs/move2kube/tutorials/customkubeyaml/","tags":"","title":"Customize Kubernetes YAMLs to target specific clusters"},{"body":"This tutorial will show how to transform a set of Kubernetes YAMLs by parameterizing them by using Move2Kube to generate parameterized Helm charts, Kustomize and Openshift Templates from the Kubernetes YAMLs. Move2Kube can also change the version of Kubernetes resources to target particular clusters.\nPrerequisites\nMove2Kube CLI tool is installed. Use a kubernetes-to-kubernetes sample. The kubernetes-to-kubernetes directory has some Kubernetes YAMLs that deploy a web app with multiple services. There are three services: a frontend website in PHP, a backend API in Node.JS, and a cache service using Redis. Procedure\nDownload the kubernetes-to-kubernetes sample. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/kubernetes-to-kubernetes -r move2kube-demos $ ls kubernetes-to-kubernetes/ api-deployment.yaml api-service.yaml redis-deployment.yaml redis-service.yaml web-deployment.yaml web-ingress.yaml web-service.yaml Run move2kube plan -s kubernetes-to-kubernetes/ to generate a plan file. $ move2kube plan -s kubernetes-to-kubernetes/ INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [CloudFoundry] Planning transformation INFO[0000] [CloudFoundry] Done INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [Base Directory] Identified 0 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in . INFO[0000] Identified 1 named services and 0 to-be-named services in . INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 1 named services and 1 to-be-named services INFO[0000] [Named Services] Identified 1 named services INFO[0000] No of services identified : 1 INFO[0000] Plan can be found at [/home/user/m2k.plan]. Look at the generated plan file in YAML format. $ ls kubernetes-to-kubernetes\tm2k.plan $ cat m2k.plan Notice that Move2Kube has detected all the different services, one for each web app.\naapiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: kubernetes-to-kubernetes services: myproject: - transformerName: KubernetesVersionChanger type: KubernetesOrgYamlsInSource paths: KubernetesYamls: - . ServiceDirPath: - . - transformerName: Parameterizer paths: KubernetesYamls: - . ServiceDirPath: - . transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/buildconfig.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/cloudfoundry.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/clusterselector.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/composeanalyser.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/composegenerator.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimage/containerimagespushscript/containerimagespushscript.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/dockerfiledetector.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/dockerfilebuildscriptgenerator.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/dockerfileparser.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/dotnetcore.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/ear.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/earrouter.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/golang.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/gradle.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/jar.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/jboss.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/knative.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/kubernetes.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/kubernetesversionchanger.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/liberty.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/maven.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/nodejs.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/php.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/parameterizer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/python.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/readmegenerator.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/ruby.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/rust.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/tekton.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/tomcat.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/war.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/warrouter.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/winconsole.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/winsilverlightweb.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/winweb.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/zuulanalyser.yaml Run the transformation using move2kube transform. $ move2kube transform INFO[0000] Detected a plan file at path /home/user/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: ID: move2kube.transformers.types Hints: [Services that don't support any of the transformer types you are interested in will be ignored.] ComposeAnalyser, PHP-Dockerfile, ReadMeGenerator, Ruby-Dockerfile, Tekton, Buildconfig, Golang-Dockerfile, Jar, Knative, Nodejs-Dockerfile, Parameterizer, CloudFoundry, DockerfileDetector, Kubernetes, Maven, WinWebApp-Dockerfile, Gradle, KubernetesVersionChanger, WarAnalyser, Rust-Dockerfile, WarRouter, ZuulAnalyser, DotNetCore-Dockerfile,EarRouter, Liberty, Python-Dockerfile, Tomcat, ContainerImagesPushScriptGenerator, DockerfileImageBuildScript, DockerfileParser, ClusterSelector, ComposeGenerator, EarAnalyser, Jboss, WinConsoleApp-Dockerfile, WinSLWebApp-Dockerfile ? Select all services that are needed: ID: move2kube.services.[].enable Hints: [The services unselected here will be ignored.] myproject INFO[0005] Starting Plan Transformation INFO[0005] Iteration 1 INFO[0005] Iteration 2 - 1 artifacts to process INFO[0005] Transformer ClusterSelector processing 1 artifacts ? Choose the cluster type: ID: move2kube.target.clustertype Hints: [Choose the cluster type you would like to target] Kubernetes INFO[0006] Transformer ClusterSelector Done INFO[0006] Transformer KubernetesVersionChanger processing 1 artifacts INFO[0006] Transformer KubernetesVersionChanger Done INFO[0006] Created 1 pathMappings and 1 artifacts. Total Path Mappings : 1. Total Artifacts : 1. INFO[0006] Iteration 3 - 1 artifacts to process INFO[0006] Transformer Parameterizer processing 1 artifacts INFO[0006] Transformer Parameterizer Done INFO[0006] Plan Transformation done INFO[0006] Transformed target artifacts can be found at [/home/user/myproject]. It will take a few minutes for it to finish processing and generate a directory called myproject. The name of the output directory is the same as the project name (by default myproject). The project name can be changed using the -n flag.\n$ ls kubernetes-to-kubernetes m2k.plan m2kconfig.yaml m2kqacache.yaml myproject $ ls myproject/ source The applications can now be deployed to Kubernetes using these generated artifacts.\nExploring the output The full structure of the output directory can be seen by executing the tree command.\n$ cd myproject/ $ tree . └── source ├── kubernetes-to-kubernetes-versionchanged │ ├── api-deployment.yaml │ ├── api-service.yaml │ ├── redis-deployment.yaml │ ├── redis-service.yaml │ ├── web-deployment.yaml │ ├── web-ingress.yaml │ └── web-service.yaml └── kubernetes-to-kubernetes-versionchanged-parameterized ├── helm-chart │ └── myproject │ ├── Chart.yaml │ ├── templates │ │ ├── api-deployment.yaml │ │ ├── api-service.yaml │ │ ├── redis-deployment.yaml │ │ ├── redis-service.yaml │ │ ├── web-deployment.yaml │ │ ├── web-ingress.yaml │ │ └── web-service.yaml │ ├── values-dev.yaml │ ├── values-prod.yaml │ └── values-staging.yaml ├── kustomize │ ├── base │ │ ├── api-deployment.yaml │ │ ├── api-service.yaml │ │ ├── kustomization.yaml │ │ ├── redis-deployment.yaml │ │ ├── redis-service.yaml │ │ ├── web-deployment.yaml │ │ ├── web-ingress.yaml │ │ └── web-service.yaml │ └── overlays │ ├── dev │ │ ├── apps-v1-deployment-api.yaml │ │ ├── apps-v1-deployment-redis.yaml │ │ ├── apps-v1-deployment-web.yaml │ │ └── kustomization.yaml │ ├── prod │ │ ├── apps-v1-deployment-api.yaml │ │ ├── apps-v1-deployment-redis.yaml │ │ ├── apps-v1-deployment-web.yaml │ │ └── kustomization.yaml │ └── staging │ ├── apps-v1-deployment-api.yaml │ ├── apps-v1-deployment-redis.yaml │ ├── apps-v1-deployment-web.yaml │ └── kustomization.yaml └── openshift-template ├── parameters-dev.yaml ├── parameters-prod.yaml ├── parameters-staging.yaml └── template.yaml 13 directories, 42 files Some things to note:\nThe Helm chart in the source/kubernetes-to-kubernetes-versionchanged-parameterized/helm-chart directory. The Kustomize YAMLs in the source/kubernetes-to-kubernetes-versionchanged-parameterized/kustomize directory. The Openshift Template in the source/kubernetes-to-kubernetes-versionchanged-parameterized/openshift-template directory. In each case, there are three environments dev, staging and prod. It is possible to have different parameterizations for each environment. Notice that the directory name has versionchanged in it. This is because two transformers are currently in play here:\nThe KubernetesVersionChanger transformer, which was asking for the Kubernetes version to target, and which created the version changed YAMLs to suit the target cluster supported kinds and versions. The Parameterizer transformer, which was taking the version changed YAMLs and creating Helm Charts, Kustomize overlays, and OC templates. If the intention was to retain the kind and verisons of Kubernetes YAMLs, disable the KubernetesVersionChanger transformer either in the QA, configuration, or in the plan, and then the YAMLs will be parameterized as is.\nFor more details on how to customize the parameterization that Move2Kube does look at the parameterization documentation.\nConclusion Given Kubernetes YAMLs, Move2Kube helps parameterize them and generate Helm charts, Kustomize, Openshift Templates, etc. Move2Kube is also capable of changing the versions of various Kubernetes resources to match the target cluster.\nSource\n","categories":"","description":"","excerpt":"This tutorial will show how to transform a set of Kubernetes YAMLs by …","ref":"/docs/move2kube/tutorials/createhelmchartskustomize/","tags":"","title":"Create Helm-charts, Kustomize overlays from Kubernetes Yamls"},{"body":".NET applications in 4.x framework This tutorial shows how containerize .NET applications developed for 4.x versions of .NET framework using Windows containers and deploy them to Kubernetes cluster using Move2Kube. This tutorial uses the sample WCF service from samples/wcfservice.\nPrerequisites Install Move2Kube.\nConfigure Kubernetes with windows node support.\nDownload the samples/wcfservice sample from move2kube-demos repository.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/wcfservice -r move2kube-demos $ tree -L 2 wcfservice/ wcfservice/ ├── wcfservice │ ├── App.config │ ├── IWindowsSampleService.cs │ ├── Properties │ ├── WindowsSampleService.cs │ ├── wcfservice.cs │ └── wcfservice.csproj └── wcfservice.sln 2 directories, 6 files Generating target artifacts This procedure uses a two stage process for the transformation: plan and transform. Run these steps from the directory containing the ./wcfservice/ directory:\nFirst create a plan of how to transform the applications to run on Kubernetes. In this phase, Move2Kube goes through the source artifacts to develop a plan. $ move2kube plan -s wcfservice INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [CloudFoundry] Planning transformation INFO[0000] [CloudFoundry] Done INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [Base Directory] Identified 0 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in . WARN[0000] Unable to find compatible ASP.NET Core target framework hence skipping. INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 1 named services and 0 to-be-named services INFO[0000] [Named Services] Identified 1 named services INFO[0000] No of services identified : 1 INFO[0000] Plan can be found at [/Users/padmanabha/go/src/github.com/seshapad/workdir/dotnet-legacy-test/m2k.plan]. Move2Kube has created a m2k.plan which is essentially a YAML file. Details of the plan file are shown below.\napiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: wcfservice services: wcfservice: - transformerName: WinConsoleApp-Dockerfile paths: AppConfigFilePathList: - wcfservice/App.config ServiceDirPath: - . transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/transformer.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/transformer.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/transformer.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/transformer.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/transformer.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimagespushscript/transformer.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/transformer.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/transformer.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/transformer.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/transformer.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/transformer.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/transformer.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/transformer.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/transformer.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/transformer.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/transformer.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/transformer.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/transformer.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/transformer.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/transformer.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/transformer.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/transformer.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/transformer.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/transformer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/transformer.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/transformer.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/transformer.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/transformer.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/transformer.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/transformer.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/transformer.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/transformer.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/transformer.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/transformer.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/transformer.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/transformer.yaml In the plan, notice that Move2Kube has detected the WCF services (wcfservice) and the relative path of the detected /App.config. The plan file indicates that the applications can be transformed using Move2Kube’s built-in WinConsoleApp-Dockerfile transformer. Invoke move2kube transform on this plan. $ move2kube transform INFO[0000] Detected a plan file at path /Users/padmanabha/go/src/github.com/seshapad/workdir/dotnet-legacy-test/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: ID: move2kube.transformers.types Hints: [Services that don't support any of the transformer types you are interested in will be ignored.] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] [✓] PHP-Dockerfile [✓] Gradle [✓] Kubernetes [✓] Knative [✓] Nodejs-Dockerfile [✓] Tekton [✓] ZuulAnalyser [✓] ComposeAnalyser [✓] Jar [✓] Liberty [✓] Python-Dockerfile [✓] ContainerImagesPushScriptGenerator [✓] Jboss [✓] Parameterizer [✓] WinSLWebApp-Dockerfile [✓] Buildconfig [✓] CloudFoundry [✓] Ruby-Dockerfile [✓] WinWebApp-Dockerfile [✓] DockerfileDetector [✓] Golang-Dockerfile [✓] WinConsoleApp-Dockerfile [✓] EarAnalyser [✓] KubernetesVersionChanger [✓] Maven [✓] ClusterSelector [✓] EarRouter [✓] DockerfileParser [✓] DotNetCore-Dockerfile [✓] ReadMeGenerator [✓] Rust-Dockerfile [✓] Tomcat [✓] WarAnalyser [✓] ComposeGenerator [✓] DockerfileImageBuildScript [✓] WarRouter Accept the default by pressing Enter key. Hints: [Services that don't support any of the transformer types you are interested in will be ignored.] PHP-Dockerfile, Gradle, Kubernetes, Knative, Nodejs-Dockerfile, Tekton, ZuulAnalyser, ComposeAnalyser, Jar, Liberty, Python-Dockerfile, ContainerImagesPushScriptGenerator, Jboss, Parameterizer, WinSLWebApp-Dockerfile, Buildconfig, CloudFoundry, Ruby-Dockerfile, WinWebApp-Dockerfile, DockerfileDetector, Golang-Dockerfile, WinConsoleApp-Dockerfile, EarAnalyser, KubernetesVersionChanger, Maven, ClusterSelector, EarRouter, DockerfileParser, DotNetCore-Dockerfile, ReadMeGenerator, Rust-Dockerfile, Tomcat, WarAnalyser, ComposeGenerator, DockerfileImageBuildScript, WarRouter ? Select all services that are needed: ID: move2kube.services.[].enable Hints: [The services unselected here will be ignored.] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] [✓] wcfservice Select all the services. ? Select all services that are needed: ID: move2kube.services.[].enable Hints: [The services unselected here will be ignored.] wcfservice INFO[0233] Starting Plan Transformation INFO[0233] Iteration 1 INFO[0233] Iteration 2 - 1 artifacts to process INFO[0233] Transformer WinConsoleApp-Dockerfile processing 1 artifacts INFO[0233] Transformer WinConsoleApp-Dockerfile Done INFO[0233] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 2. Total Artifacts : 1. INFO[0233] Iteration 3 - 2 artifacts to process INFO[0233] Transformer DockerfileParser processing 1 artifacts WARN[0233] Unable to find ports in Dockerfile : /var/folders/45/5wf_qgcs06gd_xpg6rzvbx0r0000gn/T/move2kube2980808479/environment-DockerfileParser-1624209065/2318519572/source/Dockerfile. Using default port INFO[0233] Transformer ZuulAnalyser processing 2 artifacts INFO[0233] Transformer ZuulAnalyser Done INFO[0233] Transformer DockerfileParser Done INFO[0233] Transformer DockerfileImageBuildScript processing 2 artifacts ? Select the container runtime to use : ID: move2kube.containerruntime Hints: [The container runtime selected will be used in the scripts] [Use arrows to move, type to filter] \u003e docker podman Select runtime of choice. In this case, select docker. Note: At this point, the default port 8080 is detected and the user is prompted whether to expose this port.\nINFO[0346] Transformer DockerfileImageBuildScript Done INFO[0346] Created 1 pathMappings and 4 artifacts. Total Path Mappings : 3. Total Artifacts : 3. INFO[0346] Iteration 4 - 4 artifacts to process INFO[0346] Transformer ComposeGenerator processing 2 artifacts ? What URL/path should we expose the service wcfservice's 8080 port on? ID: move2kube.services.\"wcfservice\".\"8080\".urlpath Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/wcfservice) wcfservice Leave out the leading / to use the first part wcfservice as subdomain (as specified in the Hints). wcfservice ? Provide the minimum number of replicas each service should have ID: move2kube.minreplicas Hints: [If the value is 0 pods won't be started by default] (2) Accept the default answer for two replicas for the given service. 2 ? Enter the URL of the image registry : Hints: [You can always change it later by changing the yamls.] [Use arrows to move, type to filter] Other (specify custom option) index.docker.io \u003e quay.io us.icr.io Enter quay.io for the image registry host. Select ‘Other’ if the registry name is not here. quay.io ? Enter the namespace where the new images should be pushed : Hints: [Ex : myproject] (myproject) m2k-tutorial No authentication INFO[0793] Transformer ComposeGenerator Done INFO[0793] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: ID: move2kube.target.clustertype Hints: [Choose the cluster type you would like to target] [Use arrows to move, type to filter] Openshift AWS-EKS Azure-AKS GCP-GKE IBM-IKS IBM-Openshift \u003e Kubernetes Select the Kubernetes cluster type to deploy to. Kubernetes INFO[0863] Transformer ClusterSelector Done INFO[0863] Transformer Knative processing 2 artifacts INFO[0863] Transformer Knative Done INFO[0863] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[0863] Transformer ContainerImagesPushScriptGenerator Done INFO[0863] Transformer ClusterSelector processing 2 artifacts INFO[0863] Transformer ClusterSelector Done INFO[0863] Transformer Buildconfig processing 2 artifacts INFO[0863] Transformer Buildconfig Done INFO[0863] Transformer ClusterSelector processing 2 artifacts INFO[0863] Transformer ClusterSelector Done INFO[0863] Transformer Kubernetes processing 2 artifacts ? Provide the ingress host domain ID: move2kube.target.ingress.host Hints: [Ingress host domain is part of service URL] my-cluster-ingress-host-domain.com Indicate the ingress hosting domain which can be copied from the cluster you are deploying to. The ingress hosting domain will differ based on the cluster being fetched from. my-cluster-ingress-host-domain.com ? Provide the TLS secret for ingress ID: move2kube.target.ingress.tls Hints: [Leave empty to use http] Accept the by-default for the TLS secret by pressing the Enter key. INFO[0934] Transformer Kubernetes Done INFO[0934] Transformer ClusterSelector processing 2 artifacts INFO[0934] Transformer ClusterSelector Done INFO[0934] Transformer Tekton processing 2 artifacts INFO[0934] Transformer Tekton Done INFO[0934] Created 27 pathMappings and 7 artifacts. Total Path Mappings : 30. Total Artifacts : 7. INFO[0934] Iteration 5 - 7 artifacts to process INFO[0934] Transformer Parameterizer processing 4 artifacts INFO[0934] Transformer Parameterizer Done INFO[0934] Transformer ReadMeGenerator processing 5 artifacts INFO[0934] Transformer ReadMeGenerator Done INFO[0935] Plan Transformation done INFO[0935] Transformed target artifacts can be found at [/Users/padmanabha/go/src/github.com/seshapad/workdir/dotnet-legacy-test/myproject]. The transformation is successful and the target artifacts can be found inside the ./myproject directory. The overview of the structure of the ./myproject directory can be seen by executing the below command.\n$ tree -L 3 myproject myproject/ ├── Readme.md ├── deploy │ ├── cicd │ │ ├── tekton │ │ └── tekton-parameterized │ ├── compose │ │ └── docker-compose.yaml │ ├── knative │ │ └── wcfservice-service.yaml │ ├── knative-parameterized │ │ ├── helm-chart │ │ ├── kustomize │ │ └── openshift-template │ ├── yamls │ │ ├── myproject-ingress.yaml │ │ ├── wcfservice-deployment.yaml │ │ └── wcfservice-service.yaml │ └── yamls-parameterized │ ├── helm-chart │ ├── kustomize │ └── openshift-template ├── scripts │ ├── builddockerimages.bat │ ├── builddockerimages.sh │ ├── pushimages.bat │ └── pushimages.sh └── source ├── Dockerfile ├── wcfservice │ ├── App.config │ ├── IWindowsSampleService.cs │ ├── Properties │ ├── WindowsSampleService.cs │ ├── wcfservice.cs │ └── wcfservice.csproj └── wcfservice.sln 19 directories, 17 files Move2Kube has created all the deployment artifacts which are present inside the ./myproject directory. The ./myproject/source directory looks very similar to the wcfservice directory given as input to Move2Kube. But, Move2Kube has placed additional files in the source code. For example, it has added the Dockerfile for each of the transformed services, and with these Dockerfiles, the applications can be containerized and then deployed to a Kubernetes cluster.\nDifferences in Windows applications There are two main differences in Move2Kube Windows application transformations.\nThe first difference is in the deployment YAML of the Windows container images (see myproject/deploy/yamls/wcfservice-deployment.yaml in the above example) . The nodeSelector and tolerations ensure that the image is instantiated on a Windows node in the Kubernetes cluster.\nnodeSelector: kubernetes.io/os: windows restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: os value: Windows The second difference is in the Dockerfile (see myproject/source/Dockerfile in the above example) for the Windows service as shown below. The FROM instruction refers to Windows container images and the --platform indicates that these images have to be built for a Windows platform.\nFROM --platform=windows/amd64 mcr.microsoft.com/dotnet/framework/sdk:4.8 As builder WORKDIR /app COPY . . RUN msbuild /p:Configuration=Release /p:OutputPath=/app/output FROM --platform=windows/amd64 mcr.microsoft.com/dotnet/framework/runtime:4.8 WORKDIR /app COPY --from=builder /app/output/ . CMD wcfservice.exe Deploying the application to Kubernetes with the generated target artifacts The steps involved to deploy a Windows application is the same as any other application except the container images have to be built on a Windows machine with Docker Desktop set to Windows container mode. Refer to .NET core deployment section for details.\nConclusion This tutorial shows how to migrate multiple .NET applications developed on 4.x .NET framework to Kubernetes using the target artifacts generated by Move2Kube.\nSource\n","categories":"","description":"","excerpt":".NET applications in 4.x framework This tutorial shows how …","ref":"/docs/move2kube/tutorials/createwincontainersnet/","tags":"","title":"Create and deploy Windows .NET containers"},{"body":"This tutorial shows how to migrate and deploy .NET Core applications to a Kubernetes cluster using the target artifacts generated by Move2Kube with the data from samples/dotnet5.\nPrerequisites Install Move2Kube.\nInstall a container runtime: Docker or Podman.\nInstall Kubectl.\nVerify the dependencies were correctly installed.\n$ docker version or\n$ podman info $ kubectl version Download the samples/dotnet5 sample from move2kube-demos repository.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/dotnet5 -r move2kube-demos View the structure inside the ./dotnet5 directory and the four different applications: dotnetwebapp - an ASP.NET Core web application.\ndotnetangular - an ASP.NET Core application with Angular for client-side code.\ndotnetreact - an ASP.NET Core application with React for client-side code.\ndotnetreact-redux - an ASP.NET Core application with React and Redux for client-side code.\n$ tree dotnet5 -L 2 dotnet5 ├── dotnet5angular │ ├── ClientApp │ ├── Controllers │ ├── Pages │ ├── Program.cs │ ├── Properties │ ├── Startup.cs │ ├── WeatherForecast.cs │ ├── appsettings.Development.json │ ├── appsettings.json │ ├── dotnet5angular.csproj │ ├── dotnet5angular.sln │ └── wwwroot ├── dotnet5react │ ├── ClientApp │ ├── Controllers │ ├── Pages │ ├── Program.cs │ ├── Properties │ ├── Startup.cs │ ├── WeatherForecast.cs │ ├── appsettings.Development.json │ ├── appsettings.json │ ├── dotnet5react.csproj │ ├── dotnet5react.sln │ ├── log.cs │ └── obj ├── dotnet5react-redux │ ├── ClientApp │ ├── Controllers │ ├── Pages │ ├── Program.cs │ ├── Properties │ ├── Startup.cs │ ├── WeatherForecast.cs │ ├── appsettings.Development.json │ ├── appsettings.json │ ├── dotnet5react-redux.csproj │ └── dotnet5react-redux.sln └── dotnet5webapp ├── Pages ├── Program.cs ├── Properties ├── Startup.cs ├── appsettings.Development.json ├── appsettings.json ├── dotnet5webapp.csproj ├── dotnet5webapp.sln └── wwwroot 21 directories, 28 files Generating target artifacts This tutorial will use the two stage process (plan and transform) for the transformation. Run these steps from the directory containing the ./dotnet5/ directory:\nCreate a plan on how to transform the applications to run on Kubernetes. In the plan phase, Move2Kube will go through the source artifacts and generate a plan. $ cd samples $ move2kube plan -s dotnet5 INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [CloudFoundry] Planning transformation INFO[0000] [CloudFoundry] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [Base Directory] Identified 0 named services and 0 to-be-named services INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 named services and 0 to-be-named services in dotnet5angular INFO[0000] Identified 1 named services and 0 to-be-named services in dotnet5react INFO[0000] Identified 1 named services and 0 to-be-named services in dotnet5react-redux INFO[0000] Identified 1 named services and 0 to-be-named services in dotnet5webapp INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 4 named services and 0 to-be-named services INFO[0000] [Named Services] Identified 4 named services INFO[0000] No of services identified : 4 INFO[0000] Plan can be found at [/Users/user/github/move2kube-demos/samples/m2k.plan]. Move2Kube has created a m2k.plan which is essentially a YAML file.\nView what is inside the plan file. apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: dotnet5 services: dotnet5angular: - transformerName: DotNetCore-Dockerfile paths: DotNetCoreCsprojPathType: - dotnet5angular/dotnet5angular.csproj DotNetCoreSolutionPathType: - dotnet5angular/dotnet5angular.sln ServiceDirPath: - dotnet5angular dotnet5react: - transformerName: DotNetCore-Dockerfile paths: DotNetCoreCsprojPathType: - dotnet5react/dotnet5react.csproj DotNetCoreSolutionPathType: - dotnet5react/dotnet5react.sln ServiceDirPath: - dotnet5react dotnet5react-redux: - transformerName: DotNetCore-Dockerfile paths: DotNetCoreCsprojPathType: - dotnet5react-redux/dotnet5react-redux.csproj DotNetCoreSolutionPathType: - dotnet5react-redux/dotnet5react-redux.sln ServiceDirPath: - dotnet5react-redux dotnet5webapp: - transformerName: DotNetCore-Dockerfile paths: DotNetCoreCsprojPathType: - dotnet5webapp/dotnet5webapp.csproj DotNetCoreSolutionPathType: - dotnet5webapp/dotnet5webapp.sln ServiceDirPath: - dotnet5webapp transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/buildconfig.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/cloudfoundry.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/clusterselector.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/composeanalyser.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/composegenerator.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimage/containerimagespushscript/containerimagespushscript.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/dockerfiledetector.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/dockerfilebuildscriptgenerator.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/dockerfileparser.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/dotnetcore.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/ear.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/earrouter.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/golang.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/gradle.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/jar.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/jboss.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/knative.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/kubernetes.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/kubernetesversionchanger.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/liberty.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/maven.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/nodejs.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/php.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/parameterizer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/python.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/readmegenerator.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/ruby.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/rust.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/tekton.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/tomcat.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/war.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/warrouter.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/winconsole.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/winsilverlightweb.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/winweb.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/zuulanalyser.yaml A few things to note:\nMove2Kube has detected all the four services (dotnet5webapp, dotnet5angular, dotnet5react-redux, dotnet5react) and relative paths of the detected .csproj and/or .sln files for each of the services. The plan file indicates the applications can be transformed using Move2Kube’s built-in DotNetCore-Dockerfile transformer. Run move2kube transform on this plan. $ move2kube transform INFO[0000] Detected a plan file at path /Users/username/github/move2kube-demos/samples/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: Hints: [Services that don't support any of the transformer types you are interested in will be ignored.] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] \u003e [✓] Jboss [✓] Kubernetes [✓] Liberty [✓] ZuulAnalyser [✓] CloudFoundry [✓] DockerfileParser [✓] Golang-Dockerfile [✓] Gradle [✓] Python-Dockerfile [✓] ReadMeGenerator [✓] ContainerImagesPushScriptGenerator [✓] DotNetCore-Dockerfile [✓] Parameterizer [✓] Tekton [✓] WinSLWebApp-Dockerfile [✓] Buildconfig [✓] ClusterSelector [✓] ComposeAnalyser [✓] PHP-Dockerfile [✓] Ruby-Dockerfile [✓] WinConsoleApp-Dockerfile [✓] DockerfileImageBuildScript [✓] EarRouter [✓] Rust-Dockerfile [✓] Tomcat [✓] WarAnalyser [✓] EarAnalyser [✓] Knative [✓] Maven [✓] WinWebApp-Dockerfile [✓] ComposeGenerator [✓] Jar [✓] DockerfileDetector [✓] KubernetesVersionChanger [✓] Nodejs-Dockerfile [✓] WarRouter Accpet the default by pressing the Enter key. Jboss, Kubernetes, Liberty, ZuulAnalyser, CloudFoundry, DockerfileParser, Golang-Dockerfile, Gradle, Python-Dockerfile, ReadMeGenerator, ContainerImagesPushScriptGenerator, DotNetCore-Dockerfile, Parameterizer, Tekton, WinSLWebApp-Dockerfile, Buildconfig, ClusterSelector, ComposeAnalyser, PHP-Dockerfile, Ruby-Dockerfile, WinConsoleApp-Dockerfile, DockerfileImageBuildScript, EarRouter, Rust-Dockerfile, Tomcat, WarAnalyser, EarAnalyser, Knative, Maven, WinWebApp-Dockerfile, ComposeGenerator, Jar, DockerfileDetector, KubernetesVersionChanger, Nodejs-Dockerfile, WarRouter ? Select all services that are needed: Hints: [The services unselected here will be ignored.] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] \u003e [✓] dotnet5angular [✓] dotnet5react [✓] dotnet5react-redux [✓] dotnet5webapp Select all the services. dotnet5angular, dotnet5react, dotnet5react-redux, dotnet5webapp INFO[0068] Starting Plan Transformation INFO[0068] Iteration 1 INFO[0068] Iteration 2 - 4 artifacts to process INFO[0068] Transformer DotNetCore-Dockerfile processing 4 artifacts ? Select ports to be exposed for the service dotnet5angular : Hints: [Select Other if you want to add more ports] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] \u003e [✓] 5000 [ ] Other (specify custom option) Select port 5000 detected in the source code to expose the dotnet5angular service. 5000 ? Select ports to be exposed for the service dotnet5react : Hints: [Select Other if you want to add more ports] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] \u003e [✓] 5000 [ ] Other (specify custom option) Select port 5000 for the dotnet5react service. 5000 ? Select ports to be exposed for the service dotnet5react-redux : Hints: [Select Other if you want to add more ports] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] \u003e [✓] 5000 [ ] Other (specify custom option) Accept the default for the dotnet5react-redux service. 5000 ? Select ports to be exposed for the service dotnet5webapp : Hints: [Select Other if you want to add more ports] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] \u003e [✓] 5000 [ ] Other (specify custom option) Accept the default for the dotnetwebapp service. 5000 INFO[1152] Transformer DotNetCore-Dockerfile Done INFO[1152] Created 8 pathMappings and 8 artifacts. Total Path Mappings : 8. Total Artifacts : 4. INFO[1152] Iteration 3 - 8 artifacts to process INFO[1152] Transformer DockerfileImageBuildScript processing 5 artifacts ? Select the container runtime to use : Hints: [The container runtime selected will be used in the scripts] [Use arrows to move, type to filter] docker \u003e podman Select the container runtime to use. For this tutorial, select podman as the container runtime. podman INFO[1274] Transformer DockerfileImageBuildScript Done INFO[1274] Transformer DockerfileParser processing 4 artifacts INFO[1274] Transformer ZuulAnalyser processing 2 artifacts INFO[1274] Transformer ZuulAnalyser Done INFO[1274] Transformer DockerfileParser Done INFO[1275] Created 1 pathMappings and 7 artifacts. Total Path Mappings : 9. Total Artifacts : 12. INFO[1275] Iteration 4 - 7 artifacts to process INFO[1275] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: Hints: [Choose the cluster type you would like to target] [Use arrows to move, type to filter] IBM-IKS IBM-Openshift \u003e Kubernetes Openshift AWS-EKS Azure-AKS GCP-GKE Select the Kubernetes cluster type. Kubernetes INFO[1331] Transformer ClusterSelector Done INFO[1331] Transformer Kubernetes processing 2 artifacts ? What URL/path should we expose the service dotnet5webapp's 5000 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/dotnet5webapp) dotnetwebapp Leave out the leading / to use the first part dotnetwebapp as subdomain (as specified in the Hints). dotnetwebapp ? What URL/path should we expose the service dotnet5react-redux's 5000 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/dotnet5react-redux) dotnet5react-redux Leave out the leading / to use the first part dotnetreact-redux as subdomain. dotnet5react-redux ? What URL/path should we expose the service dotnet5react's 5000 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/dotnet5react) dotnet5react Leave out the leading / for the dotnet5react service to use the first part dotnetreact as subdomain. dotnet5react ? What URL/path should we expose the service dotnet5angular's 5000 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/dotnet5angular) dotnet5angular Leave out the leading / to use the first part dotnetangular as subdomain. dotnet5angular ? Provide the minimum number of replicas each service should have Hints: [If the value is 0 pods won't be started by default] (2) Accept the default for two replicas for each service. 2 ? Enter the URL of the image registry : Hints: [You can always change it later by changing the yamls.] [Use arrows to move, type to filter] Other (specify custom option) index.docker.io \u003e quay.io us.icr.io Select quay.io for the image registry host. Select Other if the registry name is not here. quay.io ? Enter the namespace where the new images should be pushed : Hints: [Ex : myproject] (myproject) m2k-tutorial Input the namespace to deploy- m2k-tutorial. (For example, namespace m2k-tutorial is in quay.io) m2k-tutorial ? [quay.io] What type of container registry login do you want to use? Hints: [Docker login from config mode, will use the default config from your local machine.] [Use arrows to move, type to filter] Use existing pull secret \u003e No authentication UserName/Password Select the container registry login type. No authentication ? Provide the ingress host domain Hints: [Ingress host domain is part of service URL] (myproject.com) my-cluster-ingress-host-domain.com Enter the ingress hosting domain from the cluster being deployed to. The ingress hosting domain will differ based on the cluster being fetched from. my-cluster-ingress-host-domain.com ? Provide the TLS secret for ingress Hints: [Leave empty to use http] Accept the by-default TLS secret by pressing the Enter key. INFO[1850] Transformer Kubernetes Done INFO[1850] Transformer ComposeGenerator processing 2 artifacts INFO[1851] Transformer ComposeGenerator Done INFO[1851] Transformer ClusterSelector processing 2 artifacts INFO[1851] Transformer ClusterSelector Done INFO[1851] Transformer Buildconfig processing 2 artifacts INFO[1851] Transformer Buildconfig Done INFO[1851] Transformer ClusterSelector processing 2 artifacts INFO[1851] Transformer ClusterSelector Done INFO[1851] Transformer Knative processing 2 artifacts INFO[1851] Transformer Knative Done INFO[1851] Transformer ClusterSelector processing 2 artifacts INFO[1851] Transformer ClusterSelector Done INFO[1851] Transformer Tekton processing 2 artifacts INFO[1851] Transformer Tekton Done INFO[1851] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[1851] Transformer ContainerImagesPushScriptGenerator Done INFO[1852] Created 33 pathMappings and 7 artifacts. Total Path Mappings : 42. Total Artifacts : 19. INFO[1852] Iteration 5 - 7 artifacts to process INFO[1852] Transformer Parameterizer processing 4 artifacts INFO[1852] Transformer Parameterizer Done INFO[1852] Transformer ReadMeGenerator processing 5 artifacts INFO[1852] Transformer ReadMeGenerator Done INFO[1852] Plan Transformation done INFO[1852] Transformed target artifacts can be found at [/Users/username/github/move2kube-demos/samples/myproject]. The transformation is successful and the target artifacts can be found inside the ./myproject directory.\nView the ./myproject directory structure. $ tree myproject -L 3 myproject ├── Readme.md ├── deploy │ ├── cicd │ │ ├── tekton │ │ └── tekton-parameterized │ ├── compose │ │ └── docker-compose.yaml │ ├── knative │ │ ├── dotnet5angular-service.yaml │ │ ├── dotnet5react-redux-service.yaml │ │ ├── dotnet5react-service.yaml │ │ └── dotnet5webapp-service.yaml │ ├── knative-parameterized │ │ ├── helm-chart │ │ ├── kustomize │ │ └── openshift-template │ ├── yamls │ │ ├── dotnet5angular-deployment.yaml │ │ ├── dotnet5angular-service.yaml │ │ ├── dotnet5react-deployment.yaml │ │ ├── dotnet5react-redux-deployment.yaml │ │ ├── dotnet5react-redux-service.yaml │ │ ├── dotnet5react-service.yaml │ │ ├── dotnet5webapp-deployment.yaml │ │ ├── dotnet5webapp-service.yaml │ │ └── myproject-ingress.yaml │ └── yamls-parameterized │ ├── helm-chart │ ├── kustomize │ └── openshift-template ├── scripts │ ├── builddockerimages.bat │ ├── builddockerimages.sh │ ├── pushimages.bat │ └── pushimages.sh └── source ├── dotnet5angular │ ├── ClientApp │ ├── Controllers │ ├── Dockerfile │ ├── Pages │ ├── Program.cs │ ├── Properties │ ├── Startup.cs │ ├── WeatherForecast.cs │ ├── appsettings.Development.json │ ├── appsettings.json │ ├── dotnet5angular.csproj │ ├── dotnet5angular.sln │ └── wwwroot ├── dotnet5react │ ├── ClientApp │ ├── Controllers │ ├── Dockerfile │ ├── Pages │ ├── Program.cs │ ├── Properties │ ├── Startup.cs │ ├── WeatherForecast.cs │ ├── appsettings.Development.json │ ├── appsettings.json │ ├── dotnet5react.csproj │ ├── dotnet5react.sln │ ├── log.cs │ └── obj ├── dotnet5react-redux │ ├── ClientApp │ ├── Controllers │ ├── Dockerfile │ ├── Pages │ ├── Program.cs │ ├── Properties │ ├── Startup.cs │ ├── WeatherForecast.cs │ ├── appsettings.Development.json │ ├── appsettings.json │ ├── dotnet5react-redux.csproj │ └── dotnet5react-redux.sln └── dotnet5webapp ├── Dockerfile ├── Pages ├── Program.cs ├── Properties ├── Startup.cs ├── appsettings.Development.json ├── appsettings.json ├── dotnet5webapp.csproj ├── dotnet5webapp.sln └── wwwroot 38 directories, 51 files Move2Kube has created all the deployment artifacts inside the ./myproject directory. The ./myproject/source directory looks very similar to the input directory dotnet5, but Move2Kube has added files to the source code.\nFor example, it has added the Dockerfile for each of the transformed services, and with these dockerfiles, the applications can be containerized and then deployed to a Kubernetes cluster.\nDeploying the application to Kubernetes with the generated target artifacts View the ./myproject directory. $ cd myproject/ $ ls Readme.md deploy scripts source Run the builddockerimages.sh script inside the ./myproject/scripts directory. This step may take some time to complete. $ cd scripts $ ./builddockerimages.sh [1/2] STEP 1/7: FROM mcr.microsoft.com/dotnet/sdk:5.0 AS builder [1/2] STEP 2/7: WORKDIR /src --\u003e Using cache d1926570d7a610945da9057c04ddf60c23a1030f344dc62eb82a31ba0d42bed2 --\u003e d1926570d7a [1/2] STEP 3/7: COPY . . --\u003e Using cache 3593fc41a77a76df0e1bc15715990cecffdf4ae24dcd57df772b3465d9d10f53 --\u003e 3593fc41a77 [1/2] STEP 4/7: RUN mkdir app --\u003e Using cache 6b5c375d46df984c99adabf28bedab6afd5a309b8ce3a372fad5929da741ba37 --\u003e 6b5c375d46d [1/2] STEP 5/7: RUN dotnet restore dotnet5angular.csproj --\u003e Using cache d9f3086339e4fa9510696cc33d6af9b3e022c3ab6a3797b20744c3616b821d0c --\u003e d9f3086339e [1/2] STEP 6/7: RUN curl https://deb.nodesource.com/setup_10.x -o setup_10.x \u0026\u0026 bash setup_10.x \u0026\u0026 apt-get install -y build-essential nodejs --\u003e Using cache 71a9b96d0549f313f09b5e90c23cd580b66ec1d829e0a380a8db41e6d4198c10 --\u003e 71a9b96d054 [1/2] STEP 7/7: RUN dotnet publish dotnet5angular.csproj -c Release -o /src/app/publish --\u003e Using cache 7f5deecfba502398540b90a73541874bf7a4a50a07524cc8320fcf46f8c82ee9 --\u003e 7f5deecfba5 [2/2] STEP 1/6: FROM mcr.microsoft.com/dotnet/aspnet:5.0 [2/2] STEP 2/6: WORKDIR /app --\u003e Using cache c843aafccaaed89f76c5b5f906d811f5bc2c4316856e3daa9b0761c2672f222c --\u003e c843aafccaa [2/2] STEP 3/6: EXPOSE 5000 --\u003e Using cache 54a3b726dcfb236ec8a6b474d45354fe9076015389503e09f5f356226bf459cc --\u003e 54a3b726dcf [2/2] STEP 4/6: ENV ASPNETCORE_URLS=http://+:5000 --\u003e Using cache f821c90b7bea4c8473d0f0565c8afcc1709a85de1bfe4458211956c6b79a67ef --\u003e f821c90b7be [2/2] STEP 5/6: COPY --from=builder /src/app/publish . --\u003e Using cache 4a7ffdd712359b6b018b26f2ebb09e01ba400b6862507bd70223852a1285c642 --\u003e 4a7ffdd7123 [2/2] STEP 6/6: CMD [\"dotnet\", \"dotnet5angular.dll\"] --\u003e Using cache bfaead65def42103a1d5aaf4fa1b06d6be874b216e190614618a37ead13248fb [2/2] COMMIT dotnet5angular --\u003e bfaead65def Successfully tagged localhost/dotnet5angular:latest bfaead65def42103a1d5aaf4fa1b06d6be874b216e190614618a37ead13248fb /Users/username/github/move2kube-demos/samples/myproject [1/2] STEP 1/7: FROM mcr.microsoft.com/dotnet/sdk:5.0 AS builder [1/2] STEP 2/7: WORKDIR /src --\u003e Using cache d1926570d7a610945da9057c04ddf60c23a1030f344dc62eb82a31ba0d42bed2 --\u003e d1926570d7a [1/2] STEP 3/7: COPY . . --\u003e Using cache 92870100a90703664900aebacef35d0104f40154dd24df7d227c3de8af935d97 --\u003e 92870100a90 [1/2] STEP 4/7: RUN mkdir app --\u003e Using cache 2c049f9bd9efa5d3c847e1de3b68514158c45e1c5754f5180a3a1cf4b418dd92 --\u003e 2c049f9bd9e [1/2] STEP 5/7: RUN dotnet restore dotnet5react.csproj --\u003e Using cache 8f181b1b25a49666a999edd22a7f8d5b7b334d2961cb795133e372c78c0eae96 --\u003e 8f181b1b25a [1/2] STEP 6/7: RUN curl https://deb.nodesource.com/setup_10.x -o setup_10.x \u0026\u0026 bash setup_10.x \u0026\u0026 apt-get install -y build-essential nodejs --\u003e Using cache deba3a713958ae310b04f93c92b138ab33d39c5198c03ff9f2c62dda1a5e6095 --\u003e deba3a71395 [1/2] STEP 7/7: RUN dotnet publish dotnet5react.csproj -c Release -o /src/app/publish --\u003e Using cache a44da7d550a108d66cf8311f1c3f4813520b777df9d69d7b7c8e0ebebd5b92d1 --\u003e a44da7d550a [2/2] STEP 1/6: FROM mcr.microsoft.com/dotnet/aspnet:5.0 [2/2] STEP 2/6: WORKDIR /app --\u003e Using cache c843aafccaaed89f76c5b5f906d811f5bc2c4316856e3daa9b0761c2672f222c --\u003e c843aafccaa [2/2] STEP 3/6: EXPOSE 5000 --\u003e Using cache 54a3b726dcfb236ec8a6b474d45354fe9076015389503e09f5f356226bf459cc --\u003e 54a3b726dcf [2/2] STEP 4/6: ENV ASPNETCORE_URLS=http://+:5000 --\u003e Using cache f821c90b7bea4c8473d0f0565c8afcc1709a85de1bfe4458211956c6b79a67ef --\u003e f821c90b7be [2/2] STEP 5/6: COPY --from=builder /src/app/publish . --\u003e Using cache 219fed27ab510fc9767f69f733443ef07483027be1f96fcc252e46ac4f5b1b70 --\u003e 219fed27ab5 [2/2] STEP 6/6: CMD [\"dotnet\", \"dotnet5react.dll\"] --\u003e Using cache a2f7eae2981cc0c7a8c843c017457e761b55afc26321616e6d99b31a2eadffa4 [2/2] COMMIT dotnet5react --\u003e a2f7eae2981 Successfully tagged localhost/dotnet5react:latest a2f7eae2981cc0c7a8c843c017457e761b55afc26321616e6d99b31a2eadffa4 /Users/username/github/move2kube-demos/samples/myproject [1/2] STEP 1/7: FROM mcr.microsoft.com/dotnet/sdk:5.0 AS builder [1/2] STEP 2/7: WORKDIR /src --\u003e Using cache d1926570d7a610945da9057c04ddf60c23a1030f344dc62eb82a31ba0d42bed2 --\u003e d1926570d7a [1/2] STEP 3/7: COPY . . --\u003e Using cache 8ebf36b68a9eba59618911849554f61a5040b11102fb498ed54d110549b4a1dc --\u003e 8ebf36b68a9 [1/2] STEP 4/7: RUN mkdir app --\u003e Using cache 8336566b56a2d64f6198107edbfd29a62fe34f9e3827a4fdad7a268b61302d8c --\u003e 8336566b56a [1/2] STEP 5/7: RUN dotnet restore dotnet5react-redux.csproj --\u003e Using cache f27fbe5bd7809033734ed557ea7b180266865ea62756bc5160faacc0fc68f2e0 --\u003e f27fbe5bd78 [1/2] STEP 6/7: RUN curl https://deb.nodesource.com/setup_10.x -o setup_10.x \u0026\u0026 bash setup_10.x \u0026\u0026 apt-get install -y build-essential nodejs --\u003e Using cache 7891b1551dc508625ef6ca1767e63fc745e1fd2fd1194ec3b66075b259ad1368 --\u003e 7891b1551dc [1/2] STEP 7/7: RUN dotnet publish dotnet5react-redux.csproj -c Release -o /src/app/publish --\u003e Using cache f3f81341502db6f921de5d49f10d9ef8295de7b288021559b58f681cd33a5151 --\u003e f3f81341502 [2/2] STEP 1/6: FROM mcr.microsoft.com/dotnet/aspnet:5.0 [2/2] STEP 2/6: WORKDIR /app --\u003e Using cache c843aafccaaed89f76c5b5f906d811f5bc2c4316856e3daa9b0761c2672f222c --\u003e c843aafccaa [2/2] STEP 3/6: EXPOSE 5000 --\u003e Using cache 54a3b726dcfb236ec8a6b474d45354fe9076015389503e09f5f356226bf459cc --\u003e 54a3b726dcf [2/2] STEP 4/6: ENV ASPNETCORE_URLS=http://+:5000 --\u003e Using cache f821c90b7bea4c8473d0f0565c8afcc1709a85de1bfe4458211956c6b79a67ef --\u003e f821c90b7be [2/2] STEP 5/6: COPY --from=builder /src/app/publish . --\u003e Using cache ec3f3ff41508d0f77d2295ec1b8a1aba5e87dffe277a0624fda4564c0ad67f54 --\u003e ec3f3ff4150 [2/2] STEP 6/6: CMD [\"dotnet\", \"dotnet5react-redux.dll\"] --\u003e Using cache ca7f612ddd72d54352a86780e5d2afe90d19a3a28bca0c2dcaf308d51f38f745 [2/2] COMMIT dotnet5react-redux --\u003e ca7f612ddd7 Successfully tagged localhost/dotnet5react-redux:latest ca7f612ddd72d54352a86780e5d2afe90d19a3a28bca0c2dcaf308d51f38f745 /Users/username/github/move2kube-demos/samples/myproject [1/2] STEP 1/6: FROM mcr.microsoft.com/dotnet/sdk:5.0 AS builder [1/2] STEP 2/6: WORKDIR /src --\u003e Using cache d1926570d7a610945da9057c04ddf60c23a1030f344dc62eb82a31ba0d42bed2 --\u003e d1926570d7a [1/2] STEP 3/6: COPY . . --\u003e Using cache 595b1552996ad3e8b3645b83871fc31bd6fe414a4225f25354f9c64e1aaeb05b --\u003e 595b1552996 [1/2] STEP 4/6: RUN mkdir app --\u003e Using cache dc8a29f1f85b391c9eb80c04b3d70c45491fef8ff6d7bb28a56fcb1aee4053e9 --\u003e dc8a29f1f85 [1/2] STEP 5/6: RUN dotnet restore dotnet5webapp.csproj --\u003e Using cache e6041315f657d10dabd5efe12c3f870486906750219405007b02464521aa3950 --\u003e e6041315f65 [1/2] STEP 6/6: RUN dotnet publish dotnet5webapp.csproj -c Release -o /src/app/publish --\u003e Using cache f50e0d617960c1c3f4cfdfbb39d413cd537b89c5422a45cf8738464c8e5b1f3d --\u003e f50e0d61796 [2/2] STEP 1/6: FROM mcr.microsoft.com/dotnet/aspnet:5.0 [2/2] STEP 2/6: WORKDIR /app --\u003e Using cache c843aafccaaed89f76c5b5f906d811f5bc2c4316856e3daa9b0761c2672f222c --\u003e c843aafccaa [2/2] STEP 3/6: EXPOSE 5000 --\u003e Using cache 54a3b726dcfb236ec8a6b474d45354fe9076015389503e09f5f356226bf459cc --\u003e 54a3b726dcf [2/2] STEP 4/6: ENV ASPNETCORE_URLS=http://+:5000 --\u003e Using cache f821c90b7bea4c8473d0f0565c8afcc1709a85de1bfe4458211956c6b79a67ef --\u003e f821c90b7be [2/2] STEP 5/6: COPY --from=builder /src/app/publish . --\u003e Using cache fc48c345e77eadbf994b567db6cae594c1212cb768b392b86470521be13fbe1e --\u003e fc48c345e77 [2/2] STEP 6/6: CMD [\"dotnet\", \"dotnet5webapp.dll\"] --\u003e Using cache 564f1f4a6cb6437c428d72f0c607281bacd9ffe3913c8dde6b64ad9c32832c7a [2/2] COMMIT dotnet5webapp --\u003e 564f1f4a6cb Successfully tagged localhost/dotnet5webapp:latest 564f1f4a6cb6437c428d72f0c607281bacd9ffe3913c8dde6b64ad9c32832c7a /Users/username/github/move2kube-demos/samples/myproject done Run the pushimages.sh script to push the application images to the registry specified during the transform phase. For this step, it is required to log in to the Docker registry. To log in to quay.io, run podman login quay.io (or docker login quay.io, depending upon the container runtime selected during transform phase). To log in to IBM Cloud us.icr.io registry refer here. $ ./pushimages.sh Note: If the image repository has been pushed to quay.io, then in the Repository’s Visibility in quay.io repository’s Settings, select whether repository is public or private so that it can be properly accessed by the Kubernetes cluster.\nDeploy the application with kubectl apply command using the YAML files which Move2Kube created inside the ./myproject/deploy/yamls directory including deployment artifacts, service artifacts, and ingress artifact for all the services. $ cd .. $ kubectl apply -f deploy/yamls deployment.apps/dotnet5angular created service/dotnet5angular created deployment.apps/dotnet5react created deployment.apps/dotnet5react-redux created service/dotnet5react-redux created service/dotnet5react created deployment.apps/dotnet5webapp created service/dotnet5webapp created ingress.networking.k8s.io/myproject created The application is now accessible on the cluster.\nCheck the pod status. $ kubectl get pods NAME READY STATUS RESTARTS AGE dotnet5angular-746b886f85-jjprw 1/1 Running 0 45s dotnet5angular-746b886f85-tfv8r 1/1 Running 0 45s dotnet5react-866bdb54cf-87n6v 1/1 Running 0 38s dotnet5react-866bdb54cf-g9vbk 1/1 Running 0 38s dotnet5react-redux-5d95d8789-v4dfn 1/1 Running 0 34s dotnet5react-redux-5d95d8789-vz47d 1/1 Running 0 34s dotnet5webapp-5c88c67bf-cdmf2 1/1 Running 0 26s dotnet5webapp-5c88c67bf-j68hd 1/1 Running 0 26s The deployment and service and ingress have been created.\nGet the ingress to see the URL where the app has been deployed to. $ kubectl get ingress myproject Conclusion This tutorial showed how to migrate multiple .NET Core applications to Kubernetes using the target artifacts generated by Move2Kube.\nSource\n","categories":"","description":"","excerpt":"This tutorial shows how to migrate and deploy .NET Core applications …","ref":"/docs/move2kube/tutorials/migratedeploynetcore/","tags":"","title":"Migrate and deploy .NET Core applications to Kubernetes"},{"body":"Move2Kube automatically analyzes all the YAML files in the docker-compose directory and transforms and creates all artifacts required for deploying the application in Kubernetes using the transform command.\n$ move2kube transform -s docker-compose Prerequisites Install the Move2Kube CLI tool. Note: This tutorial has been created with v0.3.3-rc.2 version of Move2Kube.\n$ MOVE2KUBE_TAG='v0.3.3-rc.2' bash \u003c(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Install a Kubernetes cluster from MiniKube. Overview This tutorial shows how to migrate an application written for Docker Compose to run on Kubernetes using the two Docker Compose samples from the move2kube-demos repo.\nSample 1 is a web app with a single service using Nginx and a prebuilt image.\nSample 2 is more complicated. It is also a web app, but it has three services.\nA frontend written in PHP for Apache. An API backend written for NodeJS. A service for caching the calculations performed by the backend. For the cache service use a prebuilt Redis image for this tutorial.\nBelow are the steps for migrating the second sample. The steps for the first sample are similar except that since it uses prebuilt images, skip the build and push the images portion.\nProcedure\nDownload the samples/docker-compose/multiple-services sample. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/docker-compose/multiple-services -r move2kube-demos $ ls multiple-services Run the planning phase. $ move2kube plan -s multiple-services/ INFO[0000] Configuration loading done INFO[0000] Start planning INFO[0000] Planning started on the base directory INFO[0000] [CloudFoundry] Planning INFO[0000] [CloudFoundry] Done INFO[0000] [ComposeAnalyser] Planning INFO[0000] Identified 3 named services and 0 to-be-named services INFO[0000] [ComposeAnalyser] Done INFO[0000] [DockerfileDetector] Planning INFO[0000] Identified 1 named services and 1 to-be-named services INFO[0000] [DockerfileDetector] Done INFO[0000] [Base Directory] Identified 4 named services and 1 to-be-named services INFO[0000] Planning finished on the base directory INFO[0000] Planning started on its sub directories INFO[0000] Identified 1 named services and 0 to-be-named services in api INFO[0000] Identified 1 named services and 0 to-be-named services in web INFO[0000] Planning finished on its sub directories INFO[0000] [Directory Walk] Identified 4 named services and 2 to-be-named services INFO[0000] [Named Services] Identified 3 named services INFO[0000] Planning done INFO[0000] No of services identified : 3 INFO[0000] Plan can be found at [/Users/user/Desktop/tutorial/m2k.plan] Inspect the plan to verify all three services were detected. $ cat m2k.plan apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: multiple-services services: api: - transformerName: ComposeAnalyser paths: DockerCompose: - docker-compose.yaml Dockerfile: - api/Dockerfile ServiceDirectories: - api configs: ComposeService: serviceName: api - transformerName: Nodejs-Dockerfile paths: ServiceDirectories: - api - transformerName: DockerfileDetector paths: Dockerfile: - api/Dockerfile ServiceDirectories: - api redis: - transformerName: ComposeAnalyser paths: DockerCompose: - docker-compose.yaml configs: ComposeService: serviceName: redis web: - transformerName: ComposeAnalyser paths: DockerCompose: - docker-compose.yaml Dockerfile: - web/Dockerfile ServiceDirectories: - web configs: ComposeService: serviceName: web - transformerName: DockerfileDetector paths: Dockerfile: - web/Dockerfile ServiceDirectories: - web - transformerName: PHP-Dockerfile paths: ServiceDirectories: - web transformers: ArgoCD: m2kassets/built-in/transformers/kubernetes/argocd/transformer.yaml Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/transformer.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/transformer.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/transformer.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/transformer.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/transformer.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimagespushscript/transformer.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/transformer.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/transformer.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/transformer.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/transformer.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/transformer.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/transformer.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/transformer.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/transformer.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/transformer.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/transformer.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/transformer.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/transformer.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/transformer.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/transformer.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/transformer.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/transformer.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/transformer.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/transformer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/transformer.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/transformer.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/transformer.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/transformer.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/transformer.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/transformer.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/transformer.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/transformer.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/transformer.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/transformer.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/transformer.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/transformer.yaml Run the transformation phase. Important: For most prompts, accept the default in this tutorial. However, some prompts to watch out for are:\nKind of service/ingress created for the redis service:. Select ClusterIP so the service port will not be exposed via the Ingress. Exposed ‘web’ service URL path: Since most website frontends are built to be served under / use that instead of /web. Image registry URL and image registry namespace: The image registry URL is where the container images will be pushed after building Docker Hub (index.docker.io), Quay (quay.io), IBM Cloud Container Registry (us.icr.io), etc. The namespace here means the username on the target image registry and not the Kubernetes cluster namespace. Ingress host and TLS secret: If deploying to MiniKube, use localhost as the ingress host domain. If deploying to a Kubernetes cluster on IBM Cloud, then find the ingress subdomain on the cluster on IBM Cloud as shown here. Leave the TLS secret blank. $ move2kube transform INFO[0000] Detected a plan file at path /Users/user/Desktop/tutorial/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: ID: move2kube.transformers.types Hints: - Services that don't support any of the transformer types you are interested in will be ignored. ArgoCD, Buildconfig, CloudFoundry, ClusterSelector, ComposeAnalyser, ComposeGenerator, ContainerImagesPushScriptGenerator, DockerfileDetector, DockerfileImageBuildScript, DockerfileParser, DotNetCore-Dockerfile, EarAnalyser, EarRouter, Golang-Dockerfile, Gradle, Jar, Jboss, Knative, Kubernetes, KubernetesVersionChanger, Liberty, Maven, Nodejs-Dockerfile, PHP-Dockerfile, Parameterizer, Python-Dockerfile, ReadMeGenerator, Ruby-Dockerfile, Rust-Dockerfile, Tekton, Tomcat, WarAnalyser, WarRouter, WinConsoleApp-Dockerfile, WinSLWebApp-Dockerfile, WinWebApp-Dockerfile, ZuulAnalyser ? Select all services that are needed: ID: move2kube.services.[].enable Hints: - The services unselected here will be ignored. api, redis, web INFO[0133] Iteration 1 INFO[0133] Iteration 2 - 3 artifacts to process INFO[0133] Transformer ComposeAnalyser processing 3 artifacts INFO[0133] Transformer ZuulAnalyser processing 2 artifacts INFO[0133] Transformer ZuulAnalyser Done INFO[0133] Transformer ComposeAnalyser Done INFO[0133] Created 2 pathMappings and 4 artifacts. Total Path Mappings : 2. Total Artifacts : 3. INFO[0133] Iteration 3 - 4 artifacts to process INFO[0133] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: ID: move2kube.target.clustertype Hints: - Choose the cluster type you would like to target Kubernetes INFO[0179] Transformer ClusterSelector Done INFO[0179] Transformer ArgoCD processing 2 artifacts ? What kind of service/ingress should be created for the service redis's 6379 port? ID: move2kube.services.\"redis\".\"6379\".servicetype Hints: - Choose Ingress if you want a ingress/route resource to be created ClusterIP ? What kind of service/ingress should be created for the service api's 1234 port? ID: move2kube.services.\"api\".\"1234\".servicetype Hints: - Choose Ingress if you want a ingress/route resource to be created Ingress ? Specify the ingress path to expose the service api's 1234 port on? ID: move2kube.services.\"api\".\"1234\".urlpath Hints: - Leave out leading / to use first part as subdomain /api ? What kind of service/ingress should be created for the service web's 8080 port? ID: move2kube.services.\"web\".\"8080\".servicetype Hints: - Choose Ingress if you want a ingress/route resource to be created Ingress ? Specify the ingress path to expose the service web's 8080 port on? ID: move2kube.services.\"web\".\"8080\".urlpath Hints: - Leave out leading / to use first part as subdomain / ? Provide the minimum number of replicas each service should have ID: move2kube.minreplicas Hints: - If the value is 0 pods won't be started by default 2 ? Enter the URL of the image registry : ID: move2kube.target.imageregistry.url Hints: - You can always change it later by changing the yamls. quay.io ? Enter the namespace where the new images should be pushed : ID: move2kube.target.imageregistry.namespace Hints: - Ex : myproject move2kube ? [quay.io] What type of container registry login do you want to use? ID: move2kube.target.imageregistry.logintype Hints: - Docker login from config mode, will use the default config from your local machine. No authentication INFO[1487] Transformer ArgoCD Done INFO[1487] Transformer ClusterSelector processing 2 artifacts INFO[1487] Transformer ClusterSelector Done INFO[1487] Transformer Buildconfig processing 2 artifacts INFO[1487] Transformer Buildconfig Done INFO[1487] Transformer ComposeGenerator processing 2 artifacts INFO[1487] Transformer ComposeGenerator Done INFO[1487] Transformer DockerfileImageBuildScript processing 3 artifacts ? Select the container runtime to use : ID: move2kube.containerruntime Hints: - The container runtime selected will be used in the scripts docker INFO[1492] Transformer DockerfileImageBuildScript Done INFO[1492] Transformer ClusterSelector processing 2 artifacts INFO[1492] Transformer ClusterSelector Done INFO[1492] Transformer Knative processing 2 artifacts INFO[1492] Transformer Knative Done INFO[1492] Transformer ClusterSelector processing 2 artifacts INFO[1492] Transformer ClusterSelector Done INFO[1492] Transformer Kubernetes processing 2 artifacts ? Provide the ingress host domain ID: move2kube.target.ingress.host Hints: - Ingress host domain is part of service URL localhost ? Provide the TLS secret for ingress ID: move2kube.target.ingress.tls Hints: - Leave empty to use http INFO[1499] Transformer Kubernetes Done INFO[1499] Transformer ClusterSelector processing 2 artifacts INFO[1499] Transformer ClusterSelector Done INFO[1499] Transformer Tekton processing 2 artifacts INFO[1499] Transformer Tekton Done INFO[1499] Created 33 pathMappings and 11 artifacts. Total Path Mappings : 35. Total Artifacts : 7. INFO[1499] Iteration 4 - 11 artifacts to process INFO[1499] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[1499] Transformer ContainerImagesPushScriptGenerator Done INFO[1499] Transformer Parameterizer processing 5 artifacts INFO[1499] Transformer Parameterizer Done INFO[1499] Transformer ReadMeGenerator processing 5 artifacts INFO[1500] Transformer ReadMeGenerator Done INFO[1500] Created 17 pathMappings and 1 artifacts. Total Path Mappings : 52. Total Artifacts : 18. INFO[1500] Iteration 5 - 1 artifacts to process INFO[1500] Transformer ReadMeGenerator processing 2 artifacts INFO[1500] Transformer ReadMeGenerator Done INFO[1500] Transformation done INFO[1500] Transformed target artifacts can be found at [/Users/user/Desktop/tutorial/myproject]. The tranformatin is complete.\nView the transformation output. # click to see the output $ ls docker-compose\tm2k.plan\tm2kqacache.yaml\tmyproject docker-compose.zip\tm2kconfig.yaml\tmultiple-services $ tree myproject/ myproject/ ├── Readme.md ├── deploy │ ├── cicd │ │ ├── argocd │ │ │ └── myproject-deploy-application.yaml │ │ ├── argocd-parameterized │ │ │ ├── helm-chart │ │ │ │ └── myproject │ │ │ │ ├── Chart.yaml │ │ │ │ └── templates │ │ │ │ └── myproject-deploy-application.yaml │ │ │ ├── kustomize │ │ │ │ └── base │ │ │ │ ├── kustomization.yaml │ │ │ │ └── myproject-deploy-application.yaml │ │ │ └── openshift-template │ │ │ └── template.yaml │ │ ├── tekton │ │ │ ├── myproject-clone-build-push-pipeline.yaml │ │ │ ├── myproject-clone-push-serviceaccount.yaml │ │ │ ├── myproject-git-event-triggerbinding.yaml │ │ │ ├── myproject-git-repo-eventlistener.yaml │ │ │ ├── myproject-image-registry-secret.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-run-clone-build-push-triggertemplate.yaml │ │ │ ├── myproject-tekton-triggers-admin-role.yaml │ │ │ ├── myproject-tekton-triggers-admin-rolebinding.yaml │ │ │ └── myproject-tekton-triggers-admin-serviceaccount.yaml │ │ └── tekton-parameterized │ │ ├── helm-chart │ │ │ └── myproject │ │ │ ├── Chart.yaml │ │ │ └── templates │ │ │ ├── myproject-clone-build-push-pipeline.yaml │ │ │ ├── myproject-clone-push-serviceaccount.yaml │ │ │ ├── myproject-git-event-triggerbinding.yaml │ │ │ ├── myproject-git-repo-eventlistener.yaml │ │ │ ├── myproject-image-registry-secret.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-run-clone-build-push-triggertemplate.yaml │ │ │ ├── myproject-tekton-triggers-admin-role.yaml │ │ │ ├── myproject-tekton-triggers-admin-rolebinding.yaml │ │ │ └── myproject-tekton-triggers-admin-serviceaccount.yaml │ │ ├── kustomize │ │ │ └── base │ │ │ ├── kustomization.yaml │ │ │ ├── myproject-clone-build-push-pipeline.yaml │ │ │ ├── myproject-clone-push-serviceaccount.yaml │ │ │ ├── myproject-git-event-triggerbinding.yaml │ │ │ ├── myproject-git-repo-eventlistener.yaml │ │ │ ├── myproject-image-registry-secret.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-run-clone-build-push-triggertemplate.yaml │ │ │ ├── myproject-tekton-triggers-admin-role.yaml │ │ │ ├── myproject-tekton-triggers-admin-rolebinding.yaml │ │ │ └── myproject-tekton-triggers-admin-serviceaccount.yaml │ │ └── openshift-template │ │ └── template.yaml │ ├── compose │ │ └── docker-compose.yaml │ ├── knative │ │ ├── api-service.yaml │ │ ├── redis-service.yaml │ │ └── web-service.yaml │ ├── knative-parameterized │ │ ├── helm-chart │ │ │ └── myproject │ │ │ ├── Chart.yaml │ │ │ └── templates │ │ │ ├── api-service.yaml │ │ │ ├── redis-service.yaml │ │ │ └── web-service.yaml │ │ ├── kustomize │ │ │ └── base │ │ │ ├── api-service.yaml │ │ │ ├── kustomization.yaml │ │ │ ├── redis-service.yaml │ │ │ └── web-service.yaml │ │ └── openshift-template │ │ └── template.yaml │ ├── yamls │ │ ├── api-deployment.yaml │ │ ├── api-service.yaml │ │ ├── myproject-ingress.yaml │ │ ├── redis-deployment.yaml │ │ ├── redis-service.yaml │ │ ├── web-deployment.yaml │ │ └── web-service.yaml │ └── yamls-parameterized │ ├── helm-chart │ │ └── myproject │ │ ├── Chart.yaml │ │ ├── templates │ │ │ ├── api-deployment.yaml │ │ │ ├── api-service.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── redis-deployment.yaml │ │ │ ├── redis-service.yaml │ │ │ ├── web-deployment.yaml │ │ │ └── web-service.yaml │ │ ├── values-dev.yaml │ │ ├── values-prod.yaml │ │ └── values-staging.yaml │ ├── kustomize │ │ ├── base │ │ │ ├── api-deployment.yaml │ │ │ ├── api-service.yaml │ │ │ ├── kustomization.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── redis-deployment.yaml │ │ │ ├── redis-service.yaml │ │ │ ├── web-deployment.yaml │ │ │ └── web-service.yaml │ │ └── overlays │ │ ├── dev │ │ │ ├── apps-v1-deployment-api.yaml │ │ │ ├── apps-v1-deployment-redis.yaml │ │ │ ├── apps-v1-deployment-web.yaml │ │ │ └── kustomization.yaml │ │ ├── prod │ │ │ ├── apps-v1-deployment-api.yaml │ │ │ ├── apps-v1-deployment-redis.yaml │ │ │ ├── apps-v1-deployment-web.yaml │ │ │ └── kustomization.yaml │ │ └── staging │ │ ├── apps-v1-deployment-api.yaml │ │ ├── apps-v1-deployment-redis.yaml │ │ ├── apps-v1-deployment-web.yaml │ │ └── kustomization.yaml │ └── openshift-template │ ├── parameters-dev.yaml │ ├── parameters-prod.yaml │ ├── parameters-staging.yaml │ └── template.yaml ├── scripts │ ├── builddockerimages.bat │ ├── builddockerimages.sh │ ├── pushimages.bat │ └── pushimages.sh └── source ├── api │ ├── Dockerfile │ ├── index.js │ ├── package-lock.json │ └── package.json ├── docker-compose.yaml └── web ├── Dockerfile ├── fib.php └── index.php 43 directories, 107 files Inside the scripts directory note some helpful scripts that Move2Kube has generated to help build and push the needed container images.\nBuild all the images using the builddockerimages.sh script. # click to see the output $ cd myproject/ $ ./builddockerimages.sh [+] Building 4.3s (10/10) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 133B 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for docker.io/library/node:14 2.5s =\u003e [auth] library/node:pull token for registry-1.docker.io 0.0s =\u003e [internal] load build context 0.0s =\u003e =\u003e transferring context: 3.69kB 0.0s =\u003e [1/4] FROM docker.io/library/node:14@sha256:e5c6aac226819f88d6431a56f502972d323d052b1b6108094ba7e6b07154a542 0.0s =\u003e CACHED [2/4] WORKDIR /app 0.0s =\u003e [3/4] COPY . . 0.0s =\u003e [4/4] RUN npm install 1.5s =\u003e exporting to image 0.1s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:d5a8e3d3f05592f6edefe5df286c31c2327dbde4ad3d5832fc059f1a9381157a 0.0s =\u003e =\u003e naming to docker.io/library/fibonacci-api:latest 0.0s Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them /Users/user/Desktop/tutorial/myproject [+] Building 2.5s (8/8) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 82B 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for docker.io/library/php:7-apache 2.2s =\u003e [auth] library/php:pull token for registry-1.docker.io 0.0s =\u003e [internal] load build context 0.0s =\u003e =\u003e transferring context: 1.51kB 0.0s =\u003e CACHED [1/2] FROM docker.io/library/php:7-apache@sha256:729ad01c7d8e10fd992a6d4f3eb05dce3fb69bdf5c4fb4a9de4be4f4f5ae4dcc 0.0s =\u003e [2/2] COPY . /var/www/html/ 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:f5d91c6d96de3f8bb4c2c5d8bf6cde84985b7ee29d00ad21fad07e05cbe5ddca 0.0s =\u003e =\u003e naming to docker.io/library/fibonacci-web:latest 0.0s Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them /Users/user/Desktop/tutorial/myproject done Push the images to the registry and namespace selected using the pushimages.sh script. # click to see the output $ ./pushimages.sh The push refers to repository [quay.io/move2kube/fibonacci-web] 29db8d44d6a6: Pushed 10dfb82106c4: Layer already exists 7446d340e7f8: Layer already exists 55d40777afe6: Layer already exists 56543a169be6: Layer already exists b299cffd87cb: Layer already exists 23946094ff3f: Layer already exists 6c39776a30a0: Layer already exists 564928686313: Layer already exists 6e4300c6b758: Layer already exists ee0ca96d307e: Layer already exists 0fdfbbf7aebd: Layer already exists 2a3138346faa: Layer already exists 2edcec3590a4: Layer already exists latest: digest: sha256:b34a669c75afda3dd4b8d5ef264a6f818cb394bb147d754d6e1a8699798a4c70 size: 3242 The push refers to repository [quay.io/move2kube/fibonacci-api] aef80d5c2943: Pushed 4471bdef8049: Pushed 5825d126ab35: Layer already exists d48d998e8307: Layer already exists 1f95b68fc83b: Layer already exists c1a45f6975fa: Layer already exists be099ea57c79: Layer already exists 2b2dfe091b20: Layer already exists df74cf750cc8: Layer already exists 75a95a2ddc29: Layer already exists e8fb9c1faa8f: Layer already exists 9d1a9278f26b: Layer already exists latest: digest: sha256:521be8d409c29414274c912600dc7606b7db591f69abb2fbfb5e402ccb547878 size: 2840 Note: If using Quay.io, change the pushed repositories visibility to Public or the Kubernetes pods may fail to pull the images from the registry and could fail to start due to ErrImagePullBack.\nIf there is a Kubernetes cluster already, log in to it. Or, start MiniKube to start a local Kubernetes cluster. $ minikube start 😄 minikube v1.24.0 on Darwin 12.0.1 ✨ Using the docker driver based on existing profile 👍 Starting control plane node minikube in cluster minikube 🚜 Pulling base image ... 🏃 Updating the running docker \"minikube\" container ... 🐳 Preparing Kubernetes v1.22.3 on Docker 20.10.8 ... 🔎 Verifying Kubernetes components... ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5 💡 After the addon is enabled, please run \"minikube tunnel\" and your ingress resources would be available at \"127.0.0.1\" ▪ Using image k8s.gcr.io/ingress-nginx/controller:v1.0.4 ▪ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 ▪ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 🔎 Verifying ingress addon... 🌟 Enabled addons: storage-provisioner, default-storageclass, ingress 🏄 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default Important: Enable the ingress addon.\n$ minikube addons enable ingress 💡 After the addon is enabled, please run \"minikube tunnel\" and your ingress resources would be available at \"127.0.0.1\" ▪ Using image k8s.gcr.io/ingress-nginx/controller:v1.0.4 ▪ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 ▪ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 🔎 Verifying ingress addon... 🌟 The 'ingress' addon is enabled Check if the kubectl related command will run. $ kubectl get pods Deploy the Kubernetes YAMLs to the Kubernetes/MiniKube cluster. $ kubectl apply -f deploy/yamls deployment.apps/api created service/api created ingress.networking.k8s.io/myproject created deployment.apps/redis created service/redis created deployment.apps/web created service/web created View all the Kubernetes resources that were created. $ kubectl get all NAME READY STATUS RESTARTS AGE pod/api-84fc6cf59f-6z4nl 1/1 Running 0 8h pod/api-84fc6cf59f-72lmx 1/1 Running 0 8h pod/redis-5c94584bb-c9zk5 1/1 Running 0 8h pod/redis-5c94584bb-sv2zx 1/1 Running 0 8h pod/web-999d4cc74-6ckbj 1/1 Running 0 8h pod/web-999d4cc74-97hnc 1/1 Running 0 8h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/api ClusterIP 10.103.24.55 \u003cnone\u003e 1234/TCP 8h service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 12h service/redis ClusterIP None \u003cnone\u003e \u003cnone\u003e 8h service/web ClusterIP 10.100.18.139 \u003cnone\u003e 8080/TCP 8h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/api 2/2 2 2 8h deployment.apps/redis 2/2 2 2 8h deployment.apps/web 2/2 2 2 8h NAME DESIRED CURRENT READY AGE replicaset.apps/api-84fc6cf59f 2 2 2 8h replicaset.apps/redis-5c94584bb 2 2 2 8h replicaset.apps/web-999d4cc74 2 2 2 8h Important: This step is required only if the app has been deployed on MiniKube cluster.\nAccess the running application using the Ingress created by starting a tunnel to the MiniKube cluster. $ minikube tunnel ❗ The service/ingress myproject requires privileged ports to be exposed: [80 443] 🔑 sudo permission will be asked for it. 🏃 Starting tunnel for service myproject. Password: Access the app on the ingress specified during the ingress host domain QA. (For MiniKube, it will be http://localhost). Conclusion In this tutorial showed hot to transform a Docker Compose application with multiple services. We used Move2Kube to come up with a plan for migration, transform the input using the plan, generate the appropriate build scripts, Kubernetes YAMLs, etc. and deployed them to MiniKube.\nSource\n","categories":"","description":"","excerpt":"Move2Kube automatically analyzes all the YAML files in the …","ref":"/docs/move2kube/tutorials/migratedockercomposekube/","tags":"","title":"Migrate from Docker Compose to Kubernetes"},{"body":"This tutorial shows how to install Move2Kube and use the Move2Kube process (collect, plan and transform) to create deployment artifacts for Cloud Foundry apps using the data from samples/cloud-foundry.\nUsing the transform command, Move2Kube will automatically analyze all the artifacts in the cloud-foundry directory and transform and create all the artifacts required for deploying the application in Kubernetes.\n$ move2kube transform -s cloud-foundry Prerequisites A source directory containing the source code files and/or the manifest.yml file of a Cloud Foundry application. A sample of this is present in the move2kube-demos repository. This tutorial will be using the cloud-foundry sample from this repository.\n$ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/cloud-foundry -r move2kube-demos View the structure inside the ./cloud-foundry directory which contains the source code files and the manifest.yml file.\n$ tree cloud-foundry cloud-foundry/ ├── cfnodejsapp │ ├── main.js │ ├── manifest.yml │ ├── package-lock.json │ └── package.json └── m2k_collect └── cf └── cfapps.yaml Install Move2Kube.\nInstall a container runtime: Docker or Podman.\nInstall Kubectl.\nTo verify that dependencies were correctly installed you can run the below commands.\n$ move2kube version $ docker version or console $ podman info\n```console $ kubectl version Install the Cloud Foundry CLI. To demonstrate how to use Move2Kube to migrate a Cloud Foundry (CF) application to Kubernetes, this tutorial will use the source code inside the cloud-foundry/cfnodejsapp directory. To try out Move2Kube on a CF application, in place of the sample cloud-foundry directory, provide the correct path of the source directory (containing the source code and/or manifest files) of your CF application to Move2Kube during the plan phase.\nOptional: Deploy a simple NodeJS application into CF. If a CF app is already running, use that instead. Provision a CF app with the name cfnodejsapp using the cloud provider (Ex: IBM Cloud).\nMake note of the API endpoint (API endpoints for the IBM Cloud Foundry service can be found here).\nLogin to CF.\n$ cf login -a \u003cYOUR CF API endpoint\u003e Deploy the sample application to CF. The source code of the sample application is present inside the ./cloud-foundry/cfnodejsapp folder. $ cf push -f ./cloud-foundry/cfnodejsapp -p ./cloud-foundry/cfnodejsapp Visit the URL of the application (get this by running cf apps) to see it running. Generating target artifacts Now that a CF app is running it can be transformed using Move2Kube’s three stage process: collect, plan, and transform. Run these steps from the directory where cloud-foundry directory is present:\nOptional: This step is required only to see metadata such as environment variables from a running instance. If there is no running app, the m2k_collect directoy that comes with the sample. Collect some data about the running CF application. Limit the collection to only Cloud Foundry information using the -a cf annotation flag.\n```console $ cf login -a \u003cYOUR CF API endpoint\u003e $ move2kube collect -a cf INFO[0000] Begin collection INFO[0000] [*collector.CfAppsCollector] Begin collection INFO[0013] [*collector.CfAppsCollector] Done INFO[0013] [*collector.CfServicesCollector] Begin collection INFO[0027] [*collector.CfServicesCollector] Done INFO[0027] Collection done INFO[0027] Collect Output in [/Users/username/m2k_collect]. Copy this directory into the source directory to be used for planning. ``` The data collected will be stored in a new directory called ./m2k_collect.\n```console $ ls m2k_collect cf ``` The ./m2k_collect/cf directory contains the YAML file which has the runtime information of the particular application being transformed including the buildpacks that are supported, the memory, the number of instances, and the ports that are supported. If there are environment variables, it collects that information too.\n* Move the `./m2k_collect/cf` directory into the source directory `./cloud-foundry`. ```console $ mv m2k_collect cloud-foundry/ ``` Create a plan on how to transform the app to run on Kubernetes. In the plan phase, it is going to combine the runtime metadata (if present) with source artifacts and come up with a plan. Provide the path to the source directory (containing the source code and/or manifest files of CF application) using the -s flag.\n```console $ move2kube plan -s cloud-foundry INFO[0000] Loading Configuration INFO[0000] [*configuration.ClusterMDLoader] Loading configuration INFO[0000] [*configuration.ClusterMDLoader] Done INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory INFO[0000] [DockerfileDetector] Planning transformation INFO[0000] [DockerfileDetector] Done INFO[0000] [ComposeAnalyser] Planning transformation INFO[0000] [ComposeAnalyser] Done INFO[0000] [ZuulAnalyser] Planning transformation INFO[0000] [ZuulAnalyser] Done INFO[0000] [CloudFoundry] Planning transformation INFO[0000] Identified 1 namedservices and 0 unnamed transformer plans INFO[0000] [CloudFoundry] Done INFO[0000] [Base Directory] Identified 1 namedservices and 0 unnamed transformer plans INFO[0000] Transformation planning - Base Directory done INFO[0000] Planning Transformation - Directory Walk INFO[0000] Identified 1 namedservices and 0 unnamed transformer plans in . INFO[0000] Transformation planning - Directory Walk done INFO[0000] [Directory Walk] Identified 1 namedservices and 0 unnamed transformer plans INFO[0000] [Named Services] Identified 1 namedservices INFO[0000] No of services identified : 1 INFO[0000] Plan can be found at [/Users/username/m2k.plan]. ``` Move2Kube has created a m2k.plan which is essentially a YAML file. View what is inside the plan file.\n```console $ cat m2k.plan ``` ```yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: Plan metadata: name: myproject spec: sourceDir: cloud-foundry services: cfnodejsapp: - transformerName: CloudFoundry paths: CfManifest: - cfnodejsapp/manifest.yml CfRunningManifest: - m2k_collect/cf/cfapps.yaml ServiceDirPath: - cfnodejsapp configs: CloudFoundryService: serviceName: cfnodejsapp ContainerizationOptions: - Nodejs-Dockerfile - transformerName: Nodejs-Dockerfile paths: ServiceDirPath: - cfnodejsapp transformers: Buildconfig: m2kassets/built-in/transformers/kubernetes/buildconfig/buildconfig.yaml CloudFoundry: m2kassets/built-in/transformers/cloudfoundry/cloudfoundry.yaml ClusterSelector: m2kassets/built-in/transformers/kubernetes/clusterselector/clusterselector.yaml ComposeAnalyser: m2kassets/built-in/transformers/compose/composeanalyser/composeanalyser.yaml ComposeGenerator: m2kassets/built-in/transformers/compose/composegenerator/composegenerator.yaml ContainerImagesPushScriptGenerator: m2kassets/built-in/transformers/containerimage/containerimagespushscript/containerimagespushscript.yaml DockerfileDetector: m2kassets/built-in/transformers/dockerfile/dockerfiledetector/dockerfiledetector.yaml DockerfileImageBuildScript: m2kassets/built-in/transformers/dockerfile/dockerimagebuildscript/dockerfilebuildscriptgenerator.yaml DockerfileParser: m2kassets/built-in/transformers/dockerfile/dockerfileparser/dockerfileparser.yaml DotNetCore-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/dotnetcore/dotnetcore.yaml EarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/earanalyser/ear.yaml EarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/earrouter/earrouter.yaml Golang-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/golang/golang.yaml Gradle: m2kassets/built-in/transformers/dockerfilegenerator/java/gradle/gradle.yaml Jar: m2kassets/built-in/transformers/dockerfilegenerator/java/jar/jar.yaml Jboss: m2kassets/built-in/transformers/dockerfilegenerator/java/jboss/jboss.yaml Knative: m2kassets/built-in/transformers/kubernetes/knative/knative.yaml Kubernetes: m2kassets/built-in/transformers/kubernetes/kubernetes/kubernetes.yaml KubernetesVersionChanger: m2kassets/built-in/transformers/kubernetes/kubernetesversionchanger/kubernetesversionchanger.yaml Liberty: m2kassets/built-in/transformers/dockerfilegenerator/java/liberty/liberty.yaml Maven: m2kassets/built-in/transformers/dockerfilegenerator/java/maven/maven.yaml Nodejs-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/nodejs/nodejs.yaml PHP-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/php/php.yaml Parameterizer: m2kassets/built-in/transformers/kubernetes/parameterizer/parameterizer.yaml Python-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/python/python.yaml ReadMeGenerator: m2kassets/built-in/transformers/readmegenerator/readmegenerator.yaml Ruby-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/ruby/ruby.yaml Rust-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/rust/rust.yaml Tekton: m2kassets/built-in/transformers/kubernetes/tekton/tekton.yaml Tomcat: m2kassets/built-in/transformers/dockerfilegenerator/java/tomcat/tomcat.yaml WarAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/waranalyser/war.yaml WarRouter: m2kassets/built-in/transformers/dockerfilegenerator/java/warrouter/warrouter.yaml WinConsoleApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winconsole/winconsole.yaml WinSLWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winsilverlightweb/winsilverlightweb.yaml WinWebApp-Dockerfile: m2kassets/built-in/transformers/dockerfilegenerator/windows/winweb/winweb.yaml ZuulAnalyser: m2kassets/built-in/transformers/dockerfilegenerator/java/zuul/zuulanalyser.yaml ``` A few things to note:\nMove2Kube has detected the cfnodejsapp service, which is the name of the sample CF application from its manifest.yml. The plan file indicates the application can be transformed using the CloudFoundry or Nodejs-Dockerfile transformers. Move2Kube can use the source artifacts manifest.yaml and also the runtime information from cfapps.yaml and combine all of them to do the transformation. Run move2kube transform on this plan. $ move2kube transform INFO[0000] Detected a plan file at path /Users/username/m2k.plan. Will transform using this plan. ? Select all transformer types that you are interested in: Hints: [Services that don't support any of the transformer types you are interested in will be ignored.] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] \u003e [✓] Jboss [✓] Kubernetes [✓] Liberty [✓] ZuulAnalyser [✓] CloudFoundry [✓] DockerfileParser [✓] Golang-Dockerfile [✓] Gradle [✓] Python-Dockerfile [✓] ReadMeGenerator [✓] ContainerImagesPushScriptGenerator [✓] DotNetCore-Dockerfile [✓] Parameterizer [✓] Tekton [✓] WinSLWebApp-Dockerfile [✓] Buildconfig [✓] ClusterSelector [✓] ComposeAnalyser [✓] PHP-Dockerfile [✓] Ruby-Dockerfile [✓] WinConsoleApp-Dockerfile [✓] DockerfileImageBuildScript [✓] EarRouter [✓] Rust-Dockerfile [✓] Tomcat [✓] WarAnalyser [✓] EarAnalyser [✓] Knative [✓] Maven [✓] WinWebApp-Dockerfile [✓] ComposeGenerator [✓] Jar [✓] DockerfileDetector [✓] KubernetesVersionChanger [✓] Nodejs-Dockerfile [✓] WarRouter Accept the default by pressing the Enter key. Jboss, Kubernetes, Liberty, ZuulAnalyser, CloudFoundry, DockerfileParser, Golang-Dockerfile, Gradle, Python-Dockerfile, ReadMeGenerator, ContainerImagesPushScriptGenerator, DotNetCore-Dockerfile, Parameterizer, Tekton, WinSLWebApp-Dockerfile, Buildconfig, ClusterSelector, ComposeAnalyser, PHP-Dockerfile, Ruby-Dockerfile, WinConsoleApp-Dockerfile, DockerfileImageBuildScript, EarRouter, Rust-Dockerfile, Tomcat, WarAnalyser, EarAnalyser, Knative, Maven, WinWebApp-Dockerfile, ComposeGenerator, Jar, DockerfileDetector, KubernetesVersionChanger, Nodejs-Dockerfile, WarRouter ? Select all services that are needed: Hints: [The services unselected here will be ignored.] [Use arrows to move, space to select, \u003cright\u003e to all, \u003cleft\u003e to none, type to filter] \u003e [✓] cfnodejsapp Select the cfnodejsapp service. cfnodejsapp INFO[0275] Starting Plan Transformation INFO[0275] Iteration 1 INFO[0275] Iteration 2 - 1 artifacts to process INFO[0275] Transformer CloudFoundry processing 1 artifacts INFO[0275] Transformer ZuulAnalyser processing 2 artifacts INFO[0275] Transformer ZuulAnalyser Done INFO[0275] Transformer CloudFoundry Done INFO[0275] Created 0 pathMappings and 3 artifacts. Total Path Mappings : 0. Total Artifacts : 1. INFO[0275] Iteration 3 - 3 artifacts to process INFO[0275] Transformer ClusterSelector processing 2 artifacts ? Choose the cluster type: Hints: [Choose the cluster type you would like to target] [Use arrows to move, type to filter] \u003e Kubernetes Openshift AWS-EKS Azure-AKS GCP-GKE IBM-IKS IBM-Openshift Now, it asks to select the cluster type to deploy to. Select the Kubernetes cluster type.\n```console Kubernetes INFO[0351] Transformer ClusterSelector Done INFO[0351] Transformer Kubernetes processing 2 artifacts ? What URL/path should we expose the service cfnodejsapp's 8080 port on? Hints: [Enter :- not expose the service, Leave out leading / to use first part as subdomain, Add :N as suffix for NodePort service type, Add :L for Load Balancer service type] (/cfnodejsapp) ``` Accpet the default by pressing the Enter key. /cfnodejsapp ? Provide the minimum number of replicas each service should have Hints: [If the value is 0 pods won't be started by default] (2) Select the default answer again, which means 2 replicas for each service.\n```console 2 ? Enter the URL of the image registry : Hints: [You can always change it later by changing the yamls.] [Use arrows to move, type to filter] Other (specify custom option) index.docker.io \u003e quay.io us.icr.io ``` Select quay.io as the image registry host. Select Other if your registry name is not here. quay.io ? Enter the namespace where the new images should be pushed : Hints: [Ex : myproject] (myproject) m2k-tutorial Input the namespace to deploy- m2k-tutorial under. (For example, a namespace m2k-tutorial in quay.io) m2k-tutorial ? [quay.io] What type of container registry login do you want to use? Hints: [Docker login from config mode, will use the default config from your local machine.] [Use arrows to move, type to filter] Use existing pull secret \u003e No authentication UserName/Password Select the container registry login type. No authentication INFO[0841] Optimization done INFO[0841] Begin Optimization INFO[0841] Optimization done INFO[0841] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 2. Total Artifacts : 3. INFO[0841] Transformer Knative Done INFO[0841] Transformer Tekton processing 2 artifacts INFO[0841] Begin Optimization INFO[0841] Optimization done INFO[0841] Generating Tekton pipeline for CI/CD INFO[0841] No remote git repos detected. You might want to configure the git repository links manually. ? Provide the ingress host domain Hints: [Ingress host domain is part of service URL] (myproject.com) my-cluster-ingress-host-domain.com Enter the ingress hosting domain from the cluster being deployed to. The ingress hosting domain will differ based on the cluster being fetched from. ? Provide the TLS secret for ingress Hints: [Leave empty to use http] Accept the by-default TLS secret by pressing the Enter key. INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Generating Tekton pipeline for CI/CD INFO[1094] No remote git repos detected. You might want to configure the git repository links manually. INFO[1094] Created 20 pathMappings and 2 artifacts. Total Path Mappings : 22. Total Artifacts : 3. INFO[1094] Transformer Tekton Done INFO[1094] Transformer Buildconfig processing 2 artifacts INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Created 0 pathMappings and 0 artifacts. Total Path Mappings : 22. Total Artifacts : 3. INFO[1094] Transformer Buildconfig Done INFO[1094] Transformer ComposeGenerator processing 2 artifacts INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Created 2 pathMappings and 0 artifacts. Total Path Mappings : 24. Total Artifacts : 3. INFO[1094] Transformer ComposeGenerator Done INFO[1094] Transformer Kubernetes processing 2 artifacts INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Total transformed objects : 3 INFO[1094] Begin Optimization INFO[1094] Optimization done INFO[1094] Total transformed objects : 3 INFO[1094] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 26. Total Artifacts : 3. INFO[1094] Transformer Kubernetes Done INFO[1094] Transformer Nodejs-Dockerfile processing 1 artifacts ? Select port to be exposed for the service cfnodejsapp : Hints: [Select Other if you want to expose the service cfnodejsapp to some other port] [Use arrows to move, type to filter] \u003e 8080 Other (specify custom option) Select the port to expose the cfnodejsapp service. 8080 INFO[1184] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 28. Total Artifacts : 3. INFO[1184] Transformer Nodejs-Dockerfile Done INFO[1184] Iteration 4 INFO[1184] Transformer ReadMeGenerator processing 4 artifacts INFO[1184] Created 1 pathMappings and 0 artifacts. Total Path Mappings : 29. Total Artifacts : 11. INFO[1184] Transformer ReadMeGenerator Done INFO[1184] Transformer DockerfileImageBuildScript processing 2 artifacts ? Select the container runtime to use : Hints: [The container runtime selected will be used in the scripts] [Use arrows to move, type to filter] \u003e docker podman Select the container runtime to use. docker INFO[1184] Created 1 pathMappings and 2 artifacts. Total Path Mappings : 30. Total Artifacts : 11. INFO[1184] Transformer DockerfileImageBuildScript Done INFO[1184] Transformer DockerfileParser processing 1 artifacts INFO[1184] Created 0 pathMappings and 1 artifacts. Total Path Mappings : 30. Total Artifacts : 11. INFO[1184] Transformer DockerfileParser Done INFO[1184] Transformer Parameterizer processing 4 artifacts INFO[1184] Created 12 pathMappings and 0 artifacts. Total Path Mappings : 42. Total Artifacts : 11. INFO[1184] Transformer Parameterizer Done INFO[1184] Iteration 5 INFO[1184] Transformer Tekton processing 2 artifacts INFO[1184] Begin Optimization INFO[1184] Optimization done INFO[1184] Generating Tekton pipeline for CI/CD INFO[1184] No remote git repos detected. You might want to configure the git repository links manually. INFO[1184] Begin Optimization INFO[1184] Optimization done INFO[1184] Generating Tekton pipeline for CI/CD INFO[1184] No remote git repos detected. You might want to configure the git repository links manually. INFO[1184] Created 20 pathMappings and 2 artifacts. Total Path Mappings : 62. Total Artifacts : 14. INFO[1184] Transformer Tekton Done INFO[1184] Transformer Buildconfig processing 2 artifacts INFO[1184] Begin Optimization INFO[1185] Optimization done INFO[1185] Created 0 pathMappings and 0 artifacts. Total Path Mappings : 62. Total Artifacts : 14. INFO[1185] Transformer Buildconfig Done INFO[1185] Transformer ComposeGenerator processing 2 artifacts INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Created 2 pathMappings and 0 artifacts. Total Path Mappings : 64. Total Artifacts : 14. INFO[1185] Transformer ComposeGenerator Done INFO[1185] Transformer ContainerImagesPushScriptGenerator processing 2 artifacts INFO[1185] Created 1 pathMappings and 1 artifacts. Total Path Mappings : 65. Total Artifacts : 14. INFO[1185] Transformer ContainerImagesPushScriptGenerator Done INFO[1185] Transformer Kubernetes processing 2 artifacts INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Total transformed objects : 3 INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Total transformed objects : 3 INFO[1185] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 67. Total Artifacts : 14. INFO[1185] Transformer Kubernetes Done INFO[1185] Transformer ContainerImagesBuildScriptGenerator processing 2 artifacts INFO[1185] Created 2 pathMappings and 1 artifacts. Total Path Mappings : 69. Total Artifacts : 14. INFO[1185] Transformer ContainerImagesBuildScriptGenerator Done INFO[1185] Transformer Knative processing 2 artifacts INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Begin Optimization INFO[1185] Optimization done INFO[1185] Created 2 pathMappings and 2 artifacts. Total Path Mappings : 71. Total Artifacts : 14. INFO[1185] Transformer Knative Done INFO[1185] Iteration 6 INFO[1185] Transformer Parameterizer processing 4 artifacts INFO[1185] Created 12 pathMappings and 0 artifacts. Total Path Mappings : 83. Total Artifacts : 22. INFO[1185] Transformer Parameterizer Done INFO[1185] Transformer ReadMeGenerator processing 6 artifacts INFO[1185] Created 1 pathMappings and 0 artifacts. Total Path Mappings : 84. Total Artifacts : 22. INFO[1185] Transformer ReadMeGenerator Done INFO[1185] Plan Transformation done INFO[1185] Transformed target artifacts can be found at [/Users/username/github/move2kube-demos/samples/myproject]. The transformation is successful and the target artifacts can be found inside the ./myproject directory. The structure of the ./myproject directory can be seen by running:\n$ tree myproject myproject/ ├── Readme.md ├── deploy │ ├── cicd │ │ ├── tekton │ │ │ ├── cfnodejsapp-vcapasenv-secret.yaml │ │ │ ├── myproject-clone-build-push-pipeline.yaml │ │ │ ├── myproject-clone-push-serviceaccount.yaml │ │ │ ├── myproject-git-event-triggerbinding.yaml │ │ │ ├── myproject-git-repo-eventlistener.yaml │ │ │ ├── myproject-image-registry-secret.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-run-clone-build-push-triggertemplate.yaml │ │ │ ├── myproject-tekton-triggers-admin-role.yaml │ │ │ ├── myproject-tekton-triggers-admin-rolebinding.yaml │ │ │ └── myproject-tekton-triggers-admin-serviceaccount.yaml │ │ └── tekton-parameterized │ │ ├── helm-chart │ │ │ └── myproject │ │ │ ├── Chart.yaml │ │ │ └── templates │ │ │ ├── cfnodejsapp-vcapasenv-secret.yaml │ │ │ ├── myproject-clone-build-push-pipeline.yaml │ │ │ ├── myproject-clone-push-serviceaccount.yaml │ │ │ ├── myproject-git-event-triggerbinding.yaml │ │ │ ├── myproject-git-repo-eventlistener.yaml │ │ │ ├── myproject-image-registry-secret.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-run-clone-build-push-triggertemplate.yaml │ │ │ ├── myproject-tekton-triggers-admin-role.yaml │ │ │ ├── myproject-tekton-triggers-admin-rolebinding.yaml │ │ │ └── myproject-tekton-triggers-admin-serviceaccount.yaml │ │ ├── kustomize │ │ │ └── base │ │ │ ├── cfnodejsapp-vcapasenv-secret.yaml │ │ │ ├── kustomization.yaml │ │ │ ├── myproject-clone-build-push-pipeline.yaml │ │ │ ├── myproject-clone-push-serviceaccount.yaml │ │ │ ├── myproject-git-event-triggerbinding.yaml │ │ │ ├── myproject-git-repo-eventlistener.yaml │ │ │ ├── myproject-image-registry-secret.yaml │ │ │ ├── myproject-ingress.yaml │ │ │ ├── myproject-run-clone-build-push-triggertemplate.yaml │ │ │ ├── myproject-tekton-triggers-admin-role.yaml │ │ │ ├── myproject-tekton-triggers-admin-rolebinding.yaml │ │ │ └── myproject-tekton-triggers-admin-serviceaccount.yaml │ │ └── openshift-template │ │ └── template.yaml │ ├── compose │ │ └── docker-compose.yaml │ ├── knative │ │ └── cfnodejsapp-service.yaml │ ├── knative-parameterized │ │ ├── helm-chart │ │ │ └── myproject │ │ │ ├── Chart.yaml │ │ │ └── templates │ │ │ └── cfnodejsapp-service.yaml │ │ ├── kustomize │ │ │ └── base │ │ │ ├── cfnodejsapp-service.yaml │ │ │ └── kustomization.yaml │ │ └── openshift-template │ │ └── template.yaml │ ├── yamls │ │ ├── cfnodejsapp-deployment.yaml │ │ ├── cfnodejsapp-service.yaml │ │ ├── cfnodejsapp-vcapasenv-secret.yaml │ │ └── myproject-ingress.yaml │ └── yamls-parameterized │ ├── helm-chart │ │ └── myproject │ │ ├── Chart.yaml │ │ ├── templates │ │ │ ├── cfnodejsapp-deployment.yaml │ │ │ ├── cfnodejsapp-service.yaml │ │ │ ├── cfnodejsapp-vcapasenv-secret.yaml │ │ │ └── myproject-ingress.yaml │ │ ├── values-dev.yaml │ │ ├── values-prod.yaml │ │ └── values-staging.yaml │ ├── kustomize │ │ ├── base │ │ │ ├── cfnodejsapp-deployment.yaml │ │ │ ├── cfnodejsapp-service.yaml │ │ │ ├── cfnodejsapp-vcapasenv-secret.yaml │ │ │ ├── kustomization.yaml │ │ │ └── myproject-ingress.yaml │ │ └── overlays │ │ ├── dev │ │ │ ├── apps-v1-deployment-cfnodejsapp.yaml │ │ │ └── kustomization.yaml │ │ ├── prod │ │ │ ├── apps-v1-deployment-cfnodejsapp.yaml │ │ │ └── kustomization.yaml │ │ └── staging │ │ ├── apps-v1-deployment-cfnodejsapp.yaml │ │ └── kustomization.yaml │ └── openshift-template │ ├── parameters-dev.yaml │ ├── parameters-prod.yaml │ ├── parameters-staging.yaml │ └── template.yaml ├── scripts │ ├── builddockerimages.bat │ ├── builddockerimages.sh │ ├── pushimages.bat │ └── pushimages.sh └── source ├── cfnodejsapp │ ├── Dockerfile │ ├── main.js │ ├── manifest.yml │ ├── package-lock.json │ └── package.json └── m2k_collect └── cf └── cfapps.yaml Move2Kube has created all the deployment artifacts present inside the ./myproject directory.\nDeploying the application to Kubernetes with the generated target artifacts Open the ./myproject directory. $ cd myproject/ $ ls Readme.md deploy scripts source Run the builddockerimages.sh script inside the ./myproject/scripts directory. Note: This step may take some time to complete.\n$ cd scripts $ ./builddockerimages.sh [+] Building 7.1s (8/8) FINISHED =\u003e [internal] load build definition from Dockerfile 0.1s =\u003e =\u003e transferring dockerfile: 37B 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for registry.access.redha 2.7s =\u003e [internal] load build context 0.0s =\u003e =\u003e transferring context: 354B 0.0s =\u003e CACHED [1/3] FROM registry.access.redhat.com/ubi8/ 0.0s =\u003e [2/3] COPY . . 0.1s =\u003e [3/3] RUN npm install 4.0s =\u003e exporting to image 0.1s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:7bd59bff8763073644bd68cd3f 0.0s =\u003e =\u003e naming to docker.io/library/cfnodejsapp 0.0s /Users/username/github/move2kube-demos/samples/myproject done Run the pushimages.sh script to push the applications images to the registry specified during the transform phase. For this step, it is required to log in to your Docker registry. To log in to quay.io run docker login quay.io.\nTo log in to IBM Cloud us.icr.io registry refer here.\n$ ./pushimages.sh Using default tag: latest The push refers to repository [quay.io/m2k-tutorial/cfnodejsapp] d98785f949ff: Layer already exists 2234a0b938d7: Layer already exists 967d006c4be4: Layer already exists 90c628b74ee4: Layer already exists e7f8a4365a01: Layer already exists a7be0896acef: Layer already exists 73eb3b4bebc5: Layer already exists latest: digest: sha256:75283b09042b1454567b5e99d6d99374daad07fe46ee6843ace7dca29f085fd7 size: 1789 Note: If the image repository was pushed to quay.io, open the Repository’s Visibility in quay.io cfnodejsapp repository’s settings, select whether the repository cfnodejsapp is to be public or private so that it can be properly accessed by the Kubernetes cluster.\nDeploy the application with kubectl apply command using the YAML files Move2Kube created inside the ./myproject/deploy/yamls directory. $ cd .. $ kubectl apply -f deploy/yamls deployment.apps/cfnodejsapp created service/cfnodejsapp created ingress.networking.k8s.io/myproject created The application is now accessible on the cluster.\nCheck the pods status by running: $ kubectl get pods NAME READY STATUS RESTARTS AGE cfnodejsapp-58d777bd44-8ct2m 1/1 Running 0 7s cfnodejsapp-58d777bd44-hq6lf 1/1 Running 0 7s View the ingress to see the URL where the app has been deployed to. $ kubectl get ingress myproject Conclusion That is a simple way to combine multiple types of information like runtime, source, and cluster, and do a holistic transformation of a Cloud Foundry app to Kubernetes.\nSource\n","categories":"","description":"","excerpt":"This tutorial shows how to install Move2Kube and use the Move2Kube …","ref":"/docs/move2kube/tutorials/migratedeploycfapps/","tags":"","title":"Migrate and deploy Cloud Foundry applications to Kubernetes"},{"body":"Move2Kube interacts with users through a series of questions during the transformation phase. After looking at the output, re-running it and giving different answers to some of the questions may be necessary. In order to avoid answering all of the same questions over and over, Move2Kube provides a simple configuration file.\nIn the directory where the move2kube transform command was run there is a file called m2kconfig.yaml which contains the answers provided to all of the questions that were asked. There is also a m2kqacache.yaml file which contains both the questions and the answers in more detail and can be used when running the transform using the --config flag\nSummary command\n$ move2kube transform --config path/to/m2kconfig.yaml Running a tranform non-interactively Download the language platforms sample. Each directory contains a simple web application written in different languages. $ curl https://move2kube.konveyor.io/scripts/download.sh | bash -s -- -d samples/language-platforms -r move2kube-demos $ ls language-platforms django\tgolang\tjava-gradle\tjava-gradle-war\tjava-maven\tjava-maven-war\tnodejs\tphp\tpython\truby\trust Important: If there already is a m2kconfig.yaml from a previous run, skip the next step.\nRun the plan and transform on the language-platforms source, answer all the questions as appropriate. $ ls language-platforms $ move2kube plan -s language-platforms INFO[0000] Configuration loading done INFO[0000] Planning Transformation - Base Directory ... INFO[0000] Plan can be found at [/Users/user/Desktop/tutorial/m2k.plan]. $ move2kube transform INFO[0000] Detected a plan file at path /Users/user/Desktop/tutorial/m2k.plan. Will transform using this plan. ... INFO[0095] Plan Transformation done INFO[0095] Transformed target artifacts can be found at [/Users/user/Desktop/tutorial/myproject]. View the m2kconfig.yaml file in the output. $ ls m2k.plan\tm2kconfig.yaml\tm2kqacache.yaml\tmyproject\tlanguage-platforms $ cat m2kconfig.yaml The `m2kconfig.yaml` might look different depending on what questions were asked and the answers given. move2kube: containerruntime: docker minreplicas: \"2\" services: golang: \"8080\": urlpath: /golang enable: true ports: - \"8080\" java-gradle: \"9080\": urlpath: /java-gradle enable: true wartransformer: Liberty java-maven: \"9080\": urlpath: /java-maven enable: true wartransformer: Liberty myproject-django: \"8080\": urlpath: /myproject-django enable: true port: \"8080\" myproject-java-war: \"9080\": urlpath: /myproject-java-war enable: true wartransformer: Liberty myproject-php: \"8082\": urlpath: /myproject-php enable: true myproject-python: \"8080\": urlpath: /myproject-python enable: true port: \"8080\" nodejs: \"8080\": urlpath: /nodejs enable: true port: \"8080\" ruby: \"8080\": urlpath: /ruby enable: true port: \"8080\" rust: \"8085\": urlpath: /rust enable: true port: \"8085\" target: clustertype: Kubernetes imageregistry: logintype: No authentication namespace: myproject url: quay.io ingress: host: myproject.com tls: \"\" transformers: types: - EarRouter - WinConsoleApp-Dockerfile - Gradle - Jar - WinSLWebApp-Dockerfile - DockerfileImageBuildScript - DotNetCore-Dockerfile - EarAnalyser - Golang-Dockerfile - Ruby-Dockerfile - Tomcat - Buildconfig - ComposeGenerator - ContainerImagesPushScriptGenerator - DockerfileParser - WarAnalyser - WinWebApp-Dockerfile - WarRouter - ZuulAnalyser - KubernetesVersionChanger - Tekton - Maven - PHP-Dockerfile - Parameterizer - Python-Dockerfile - ClusterSelector - DockerfileDetector - Kubernetes - Liberty - Rust-Dockerfile - Nodejs-Dockerfile - ReadMeGenerator - CloudFoundry - ComposeAnalyser - Jboss - Knative transformerselector: \"\" The config file only contains the answers provided to the questions. To better understand the config file, look at its companion file m2kqacache.yaml\n$ cat m2kqacache.yaml ``` ```yaml apiVersion: move2kube.konveyor.io/v1alpha1 kind: QACache spec: solutions: - id: move2kube.transformers.types type: MultiSelect description: 'Select all transformer types that you are interested in:' hints: - Services that don't support any of the transformer types you are interested in will be ignored. options: - Golang-Dockerfile - Nodejs-Dockerfile - ContainerImagesPushScriptGenerator - ReadMeGenerator - ComposeAnalyser - EarAnalyser - Ruby-Dockerfile - WarAnalyser - DotNetCore-Dockerfile - Python-Dockerfile - Liberty - Knative - CloudFoundry - Tomcat - ZuulAnalyser - Tekton - Rust-Dockerfile - KubernetesVersionChanger - DockerfileParser - Parameterizer - PHP-Dockerfile - Kubernetes - WinWebApp-Dockerfile - Maven - Jboss - ComposeGenerator - Gradle - WinConsoleApp-Dockerfile - Buildconfig - DockerfileDetector - WarRouter - EarRouter - ClusterSelector - Jar - DockerfileImageBuildScript - WinSLWebApp-Dockerfile default: - Golang-Dockerfile - Nodejs-Dockerfile - ContainerImagesPushScriptGenerator - ReadMeGenerator - ComposeAnalyser - EarAnalyser - Ruby-Dockerfile - WarAnalyser - DotNetCore-Dockerfile - Python-Dockerfile - Liberty - Knative - CloudFoundry - Tomcat - ZuulAnalyser - Tekton - Rust-Dockerfile - KubernetesVersionChanger - DockerfileParser - Parameterizer - PHP-Dockerfile - Kubernetes - WinWebApp-Dockerfile - Maven - Jboss - ComposeGenerator - Gradle - WinConsoleApp-Dockerfile - Buildconfig - DockerfileDetector - WarRouter - EarRouter - ClusterSelector - Jar - DockerfileImageBuildScript - WinSLWebApp-Dockerfile answer: - EarRouter - WinConsoleApp-Dockerfile - Gradle - Jar - WinSLWebApp-Dockerfile - DockerfileImageBuildScript - DotNetCore-Dockerfile - EarAnalyser - Golang-Dockerfile - Ruby-Dockerfile - Tomcat - Buildconfig - ComposeGenerator - ContainerImagesPushScriptGenerator - DockerfileParser - WarAnalyser - WinWebApp-Dockerfile - WarRouter - ZuulAnalyser - KubernetesVersionChanger - Tekton - Maven - PHP-Dockerfile - Parameterizer - Python-Dockerfile - ClusterSelector - DockerfileDetector - Kubernetes - Liberty - Rust-Dockerfile - Nodejs-Dockerfile - ReadMeGenerator - CloudFoundry - ComposeAnalyser - Jboss - Knative - id: move2kube.transformerselector type: Input hints: - Set the transformer selector config. default: \"\" answer: \"\" - id: move2kube.services.[].enable type: MultiSelect description: 'Select all services that are needed:' hints: - The services unselected here will be ignored. options: - java-maven - myproject-django - myproject-python - rust - golang - java-gradle - myproject-java-war - myproject-php - nodejs - ruby default: - java-maven - myproject-django - myproject-python - rust - golang - java-gradle - myproject-java-war - myproject-php - nodejs - ruby answer: - java-maven - myproject-django - myproject-python - rust - golang - java-gradle - myproject-java-war - myproject-php - nodejs - ruby - id: move2kube.services.java-gradle.wartransformer type: Select description: Select the transformer to use for service java-gradle options: - Liberty - Tomcat - Jboss default: Liberty answer: Liberty - id: move2kube.services.ruby.port type: Select description: 'Select port to be exposed for the service ruby :' hints: - Select Other if you want to expose the service ruby to some other port options: - \"8080\" - Other (specify custom option) default: \"8080\" answer: \"8080\" - id: move2kube.services.nodejs.port type: Input description: 'Enter the port to be exposed for the service nodejs: ' hints: - The service nodejs will be exposed to the specified port default: \"8080\" answer: \"8080\" - id: move2kube.services.myproject-python.port type: Select description: 'Select port to be exposed for the service myproject-python :' hints: - Select Other if you want to expose the service myproject-python to some other port options: - \"8080\" - Other (specify custom option) default: \"8080\" answer: \"8080\" - id: move2kube.services.myproject-django.port type: Select description: 'Select port to be exposed for the service myproject-django :' hints: - Select Other if you want to expose the service myproject-django to some other port options: - \"8080\" - Other (specify custom option) default: \"8080\" answer: \"8080\" - id: move2kube.services.java-maven.wartransformer type: Select description: Select the transformer to use for service java-maven options: - Liberty - Tomcat - Jboss default: Liberty answer: Liberty - id: move2kube.services.rust.port type: Select description: 'Select port to be exposed for the service rust :' hints: - Select Other if you want to expose the service rust to some other port options: - \"8085\" - Other (specify custom option) default: \"8085\" answer: \"8085\" - id: move2kube.services.myproject-java-war.wartransformer type: Select description: Select the transformer to use for service myproject-java-war options: - Liberty - Tomcat - Jboss default: Liberty answer: Liberty - id: move2kube.services.golang.ports type: MultiSelect description: 'Select ports to be exposed for the service golang :' hints: - Select Other if you want to add more ports options: - \"8080\" - Other (specify custom option) default: - \"8080\" answer: - \"8080\" - id: move2kube.containerruntime type: Select description: 'Select the container runtime to use :' hints: - The container runtime selected will be used in the scripts options: - docker - podman default: docker answer: docker - id: move2kube.services.\"rust\".\"8085\".urlpath type: Input description: What URL/path should we expose the service rust's 8085 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /rust answer: /rust - id: move2kube.services.\"myproject-django\".\"8080\".urlpath type: Input description: What URL/path should we expose the service myproject-django's 8080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /myproject-django answer: /myproject-django - id: move2kube.services.\"golang\".\"8080\".urlpath type: Input description: What URL/path should we expose the service golang's 8080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /golang answer: /golang - id: move2kube.services.\"nodejs\".\"8080\".urlpath type: Input description: What URL/path should we expose the service nodejs's 8080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /nodejs answer: /nodejs - id: move2kube.services.\"ruby\".\"8080\".urlpath type: Input description: What URL/path should we expose the service ruby's 8080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /ruby answer: /ruby - id: move2kube.services.\"myproject-python\".\"8080\".urlpath type: Input description: What URL/path should we expose the service myproject-python's 8080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /myproject-python answer: /myproject-python - id: move2kube.services.\"myproject-php\".\"8082\".urlpath type: Input description: What URL/path should we expose the service myproject-php's 8082 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /myproject-php answer: /myproject-php - id: move2kube.minreplicas type: Input description: Provide the minimum number of replicas each service should have hints: - If the value is 0 pods won't be started by default default: \"2\" answer: \"2\" - id: move2kube.target.imageregistry.url type: Select description: 'Enter the URL of the image registry : ' hints: - You can always change it later by changing the yamls. options: - Other (specify custom option) - us.icr.io - quay.io - registry.ng.bluemix.net default: quay.io answer: quay.io - id: move2kube.target.imageregistry.namespace type: Input description: 'Enter the namespace where the new images should be pushed : ' hints: - 'Ex : myproject' default: myproject answer: myproject - id: move2kube.target.imageregistry.logintype type: Select description: '[quay.io] What type of container registry login do you want to use?' hints: - Docker login from config mode, will use the default config from your local machine. options: - Use existing pull secret - No authentication - UserName/Password default: No authentication answer: No authentication - id: move2kube.target.clustertype type: Select description: 'Choose the cluster type:' hints: - Choose the cluster type you would like to target options: - AWS-EKS - Azure-AKS - GCP-GKE - IBM-IKS - IBM-Openshift - Kubernetes - Openshift default: Kubernetes answer: Kubernetes - id: move2kube.target.ingress.host type: Input description: Provide the ingress host domain hints: - Ingress host domain is part of service URL default: myproject.com answer: myproject.com - id: move2kube.target.ingress.tls type: Input description: Provide the TLS secret for ingress hints: - Leave empty to use http default: \"\" answer: \"\" - id: move2kube.services.\"myproject-java-war\".\"9080\".urlpath type: Input description: What URL/path should we expose the service myproject-java-war's 9080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /myproject-java-war answer: /myproject-java-war - id: move2kube.services.\"java-maven\".\"9080\".urlpath type: Input description: What URL/path should we expose the service java-maven's 9080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /java-maven answer: /java-maven - id: move2kube.services.\"java-gradle\".\"9080\".urlpath type: Input description: What URL/path should we expose the service java-gradle's 9080 port on? hints: - Enter :- not expose the service - Leave out leading / to use first part as subdomain - Add :N as suffix for NodePort service type - Add :L for Load Balancer service type default: /java-gradle answer: /java-gradle The cache file contains both the questions and the answers along with additional information about each question, such as the default answer, the type of the question, the question IDs, any hints that were provided, etc.\nThe config file stores the answer to a question under the key specified by the question’s ID.\nFor example, the question What URL/path should we expose the service java-maven's 9080 port on? has the id move2kube.services.\"java-maven\".\"9080\".urlpath. So we find the answer in the config file /java-maven stored as:\nmove2kube: services: java-maven: \"9080\": urlpath: /java-maven Every time Move2Kube goes to ask a question, it first checks the config file to see if it has already been answered using the question ID. If the ID is not present in the config file, Move2Kube will usually ask the user for the answer meaning the answer to any question can be provided by storing it in the config file.\nRun the transform again with the generated config file. $ mv myproject old # rename the output directory from the previous run to avoid conflicts $ ls m2k.plan\tm2kconfig.yaml\tm2kqacache.yaml\told\tlanguage-platforms $ move2kube transform --config m2kconfig.yaml INFO[0000] Detected a plan file at path /Users/user/Desktop/tutorial/m2k.plan. Will transform using this plan. INFO[0000] Starting Plan Transformation ... INFO[0007] Plan Transformation done INFO[0007] Transformed target artifacts can be found at [/Users/user/Desktop/tutorial/myproject]. $ ls m2k.plan\tm2kconfig.yaml\tm2kqacache.yaml\tmyproject\told\tlanguage-platforms This time Move2Kube did not ask any questions because all of the answers were provided by editing the config file directly to change the answer to a question. Some answers can be removed from the config file to prompt Move2Kube to ask those questions again. This provides a convenient way to iterate quickly, as well as a way to run Move2Kube non-interatively.\nSource\n","categories":"","description":"","excerpt":"Move2Kube interacts with users through a series of questions during …","ref":"/docs/move2kube/tutorials/runnoninteractively/","tags":"","title":"Run transforms non-interactively"},{"body":"These are some of the source platforms that Move2Kube supports.\nSource\n","categories":"","description":"","excerpt":"These are some of the source platforms that Move2Kube supports.\nSource …","ref":"/docs/move2kube/tutorials/","tags":"","title":"Tutorials"},{"body":"Minikube can be installed using Docker or Podman web interfaces.\nInstalling Move2Kube using Docker Follow the steps below to install Move2Kube with options of persistence by mounting to the current directory, and advanced features by mounting to the Docker socket allowing Move2Kube to run container based transformers.\nMove2Kube can also be installed as a Helm Chart from ArtifactHub. For more information on Helm Chart and Operator see Move2Kube Operator\nProcedure .\nNote For bleeding edge features, development, and testing, follow the steps below, but replace v0.3.0 with latest.\nInstall Move2Kube. $ docker run --rm -it -p 8080:8080 quay.io/konveyor/move2kube-ui:v0.3.0 Set persistence (optional). $ docker run --rm -it -p 8080:8080 \\ -v \"${PWD}/move2kube-api-data:/move2kube-api/data\" \\ quay.io/konveyor/move2kube-ui:v0.3.0 Set advanced features (optional). $ docker run --rm -it -p 8080:8080 \\ -v \"${PWD}/move2kube-api-data:/move2kube-api/data\" \\ -v //var/run/docker.sock:/var/run/docker.sock \\ quay.io/konveyor/move2kube-ui:v0.3.0 Installing Move2Kube using Podman Run the command below to install Move2Kube. $ podman run --rm -it -p 8080:8080 quay.io/konveyor/move2kube-ui:v0.3.0 Accessing the Move2Kube UI Open a Web browser and navigate to http://localhost:8080/ to access the UI.\n**Note: There is a known issue when mounting directories in WSL. Some empty directories may be created in the root directory. If using Windows, use Powershell instead of WSL until this is fixed.\nSource\n","categories":"","description":"","excerpt":"Minikube can be installed using Docker or Podman web interfaces. …","ref":"/docs/move2kube/installation/installweb/","tags":"","title":"Web Interface"},{"body":"Installing on Linux / MacOS / Windows WSL (Recommended): The easiest way to install Move2Kube is to download the pre-built binaries for Linux, MacOS, or Windows from the list of releases on Github. Follow the steps below to install the latest stable version.\nProcedure\nInstall Move2Kube with one of the following options. Latest stable version: bash \u003c(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) A specific version (for example version v0.3.0-beta.0): MOVE2KUBE_TAG='v0.3.0-beta.0' bash \u003c(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Bleeding edge version: BLEEDING_EDGE='true' bash \u003c(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Without sudo: USE_SUDO=false bash \u003c(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Consider setting the following options. The script installs to /usr/local/bin by default. To install to a different directory: MOVE2KUBE_INSTALL_DIR=/my/new/install/dir bash \u003c(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Make the installation directory in the PATH to run Move2Kube as move2kube instead of /my/new/install/dir/move2kube. Combine the above two environment variables to install without sudo and install to a different directory. USE_SUDO=false MOVE2KUBE_INSTALL_DIR=/my/new/install/dir bash \u003c(curl https://raw.githubusercontent.com/konveyor/move2kube/main/scripts/install.sh) Alternate installations Move2Kube can be installed using Homebrew and Go.\nHomebrew\nbrew tap konveyor/move2kube brew install move2kube To install a specific version (for example version v0.3.0-beta.0):\nbrew install move2kube@0.3.0-beta.0 Go\nInstall using go get pulls from the main branch of Move2Kube with the latest development changes.\ngo get -u github.com/konveyor/move2kube Source\n","categories":"","description":"","excerpt":"Installing on Linux / MacOS / Windows WSL (Recommended): The easiest …","ref":"/docs/move2kube/installation/installcli/","tags":"","title":"Command line tool"},{"body":"As a software developer, technical debt is all those little things that you feel are hampering the efficiency of your coding. These are the issues that you (or your manager) choose to handle next time, because of the urgency of the current release. But sometimes, by the time you get to them, they have become so big you do not know where to start solving them. The difficult part is that decisions we make regarding technical debt have to balance between short term and long term implications of accumulating such debt, emphasizing the need to properly assess and address it when planning development cycles.\nIn their seminal article from 20121, Nord et al suggest a metric to measure technical debt, based on dependencies between architectural elements. They use this method to show how an organization should plan development cycles while taking into account the effect the technical debt will have on the overall resources required for each subsequent version.\nIn this post, we demonstrate that not only is technical debt key to making decisions regarding any specific application, it is also important when attempting to prioritize work between multiple applications. Specifically, modernization work. Moreover, we show a method that can be used to not only compare the performance of different design paths for a single application, but also compare the technical debt levels of multiple applications at an arbitrary point in their development life cycle.\nThe main idea of the aforementioned article is to provide a way to measure technical debt. It is done using a formula that mainly relies on the dependencies between architectural elements in the given application. It is worth noting that the article does not define what constitutes an architectural element or how to identify these elements when approaching an application.\nWe took a similar approach and devised a method to measure technical debt of an application based on the dependency graph between its classes. The dependency graph is a directional graph G =(V, E) in which the V = {c1, c2, …} is the set of all classes in the application and an edge e = ⟨c1, c2⟩ ∈ E exists between two vertices if class c1 depends on class c2 in the original code. We perform multi-faceted analysis on the graph to eventually come up with a score that describes the technical debt of the application. Here are some of the metrics we extract from the raw graph:\nAverage/median outdegree of the vertices on the graph Top N outdegree of any node in the graph. Longest paths between classes Using standard clustering algorithms on the graph, we identify communities of classes within the graph and measure additional metrics on them, such as:\nAverage outdegree of the identified communities Longest paths between communities Our hypothesis is that using these generic metrics on the dependency graphs, we can identify architectural issues which represent real technical debt in the original code base. Moreover, by analyzing dependencies on these two levels, class and community, we give a broad interpretation of what an architectural element is in practice, without attempting to formally define it.\nTo test this method, we created a data set of over fifty applications from a variety of domains (online retail, financial, automotive, etc…) and extracted the aforementioned metrics from them. We used this data set in two ways. First, we correlated specific instances of high ranking occurrences of outdegrees and long paths with local issues in the code. For example, identifying god classes by their high outdegree2. This proved efficient and increased our confidence level that this approach is valid in identifying local technical debt issues. The second approach was to try and provide a high level score that can be used not only to identify technical debt in a single application but to also be able to compare technical debt between applications, and to use it to help prioritize which should be addressed and how. To do that we introduced three indexes:\nComplexity — represents the effort required to add new features into the software. Risk — represents the potential risk that adding new features has on the stability of existing ones. Overall Debt — represents the overall amount of extra work required when attempting to add new features. We manually analyzed the applications in our data set, also employing the expert knowledge of the architects in charge of actively developing them, and scored each application’s complexity, risk, and overall debt on a scale of 1-5. We used these benchmarks to train a machine learning model which correlates the values of the extracted metrics with the indexes and normalizes them to a score of 0-100. At this point, we can use this trained model to issue a score per index for any new application we encounter which allows us to analyze entire portfolios of applications, and compare them both to one another and to our pre-calculated benchmark. The following graph depicts a sample of 21 applications demonstrating the relationship between the aforementioned metrics.\nThe overall debt levels were then converted into currency units, depicting the level of investment required to add new functionality into the system, e.g. only $0.1 of every $1 of any investment will go into innovation due to architectural technical debt. This is intended to help organizations build a business case for handling and removing architectural technical debt from their applications.\nWe have shown a method to measure technical debt of an application based on the dependencies between its classes. We have successfully used this method to both identify local issues which cause technical debt as well as providing a global score, comparable between applications. By employing this method, organizations can successfully assess and manage the technical debt in their software which will lead to a better overall decision making process.\nBio: Ori Saporta co-founded vFunction and serves as its Systems Architect. Prior to founding vFunction, Ori was the lead Systems Architect of WatchDox until its acquisition by Blackberry, where he continued to serve in the role of Distinguished Systems Architect. Prior to that, Ori was a Systems Architect at Israeli’s Intelligence Core Technology Unit (8200). Ori has a BSc in Computer Engineering from Tel-Aviv University and an MSc in Computer Science from the same institute for which his thesis subject was “Testing and Optimizing Data-Structure Implementations Under the RC11 Memory Model.\"\nNord, Robert L., et al. “In search of a metric for managing architectural technical debt.” 2012 Joint Working IEEE/IFIP Conference on Software Architecture and European Conference on Software Architecture. IEEE, 2012. ↩︎\nWikipedia: God object. ↩︎\n","categories":"","description":"we demonstrate that not only is technical debt key to making decisions regarding any specific application, it is also important when attempting to prioritize work between multiple applications.","excerpt":"we demonstrate that not only is technical debt key to making decisions …","ref":"/blog/2022/measure-manage-technical-debt/","tags":["Metrics"],"title":"A Method to Measure and Manage Technical Debt"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/metrics/","tags":"","title":"Metrics"},{"body":"The administrator view is intended to be used by administrators to set up the Tackle instance environment. Credentials This management module enables administrators to create and manage credentials for access to private repositories. It also allows for the architects to assign the credentials to applications without knowing their contents. Key credential attributes to have before starting the procedure include:\nName Description (optional) Type (Souce control, Maven settings file, Proxy) Created by Important There are three types of credentials. Follow the procedure below for the type being configured.\nConfiguring source control credentials Follow the steps below to create new credentials for a source control repository.\nProcedure\nClick Credentials in the left menu of the Administrator view. Click the Create new button. Enter the following information. Name Description (Optional) Select the credential type in the Type drop-down list. Souce Control Maven Settings File Proxy Select the credential type in the User Credentials drop-down list and enter the requested information. Username/Password Username Password (Hidden) Create credentials SCM Private Key/Passphrase SCM Private Key Private Key Passphrase (Hidden) Maven Settings File Note: Type specific credential information such as keys and passphrases will be hidden or shown as [Encrypted].\nClick Save. Tackle validates the input and creates a new credential. SCM keys must be parsed and checked for validity. If the validation fails, an error message displaying “not a valid key/XML file” is displayed.\nConfiguring Maven credentials Follow the steps below to create new credentials for a Maven repository.\nProcedure\nClick Credentials in the left menu of the Administrator view. Click the Create new button. Enter the following information. Name Description (Optional) Select Maven Settings File in the Type drop-down list. Upload the settings file. Click Save. Tackle validates the input and creates a new credential. Maven settings.xml files must be parsed and checked for validity. If the validation fails, an error message displaying “not a valid key/XML file” is displayed.\nConfiguring proxy credentials Follow the steps below to create new credentials for a proxy repository.\nProcedure\nClick Credentials in the left menu of the Administrator view. Click the Create new button. Enter the following information. Name Description (Optional) Select Proxy in the Type drop-down list. Enter the following information. Username Password Note: Type specific credential information such as keys and passphrases will be hidden or shown as [Encrypted].\nClick Save. Repositories This management module configures the repositories used by Tackle with the following options.\nConfiguring Git repositories Follow the steps below to configure a Git repository.\nClick Repositories and then Git in the left menu of the Administrator view. Click the Consume insecure Git repositories toggle switch to enable. Configuring Subversion repositories Follow the steps below to configure a Subversion repository.\nClick Repositories and then Subversion in the left menu of the Administrator view. Click the Consume insecure Subversion repositories toggle switch to enable. Subversion Configuring Maven repositories Follow the steps below to configure a Maven repository or clear the local artifact repository.\nConfiguring the repository Click Repositories and then Maven in the left menu of the Administrator view. Click the Force update of dependencies toggle switch to enable. Click the Consume insecure artifact repositories toggle switch to enable. Managing repository size Click Repositories and then Maven in the left menu of the Administrator view. Click the Clear repository link. Note Due to the size of the repository, the size change may not be evident despite the function working properly.\nProxy This management module configures HTTP \u0026 HTTPS proxy settings. To configure the proxies click the radio button and enter the following information.\nClick Proxy in the left menu of the Administrator view. Click the HTTP proxy or HTTPS proxy toggle switch to enable the proxy connection. Enter the following information Proxy host Proxy port Click the Authentincation toggle switch to enable authentication (optional). Select the credential from the drop-down list. Source\n","categories":"","description":"","excerpt":"The administrator view is intended to be used by administrators to set …","ref":"/docs/konveyor/admintasks/","tags":"","title":"Administrator view tasks"},{"body":"By Ramón Román Nissen\nWe’re thrilled to announce that Tackle 2.0 is now available in operatorhub.io. This new release marks a major milestone for the Tackle project, with a new architecture focused on enhancing performance, scalability and more importantly, extensibility of the tooling available in the Tackle toolkit.\nThis version includes the seamless integration with the analysis capabilities of the Windup project, enabling Tackle to effectively manage, assess and now also analyze applications to have a holistic view at your portfolio when dealing with large scale modernization and Kubernetes adoption initiatives. This is essential to provide key insights that allow Architects leading these massive projects to make informed decisions, thus reducing risks and making the required effort measurable and predictable.\nThe following is a list of the exciting new features included in Tackle 2.0:\nAdministrator perspective: Dedicated perspective to manage tool-wide configuration, with a similar approach and design to the OpenShift Administrator Perspective. Enhanced RBAC: Three new differentiated personas with different permissions — Administrator, Architect and Migrator Integration with repositories: Full integration with source code (Git, Subversion) and binaries (Maven) repositories to automate the retrieval of applications for analysis. Credentials management: Secure store for multiple credential types (source control, Maven settings files, proxy). Credentials are managed by Administrators and assigned by Architects to applications. Proxy integration: HTTP and HTTPS proxy configuration can be managed in the Tackle UI. Analysis module: Full integration with the Windup project to allow the execution of application analysis from the application inventory. Enhanced analysis modes: Aside from source and binary analysis modes, now Tackle includes the Source + Dependencies mode that parses the POM file available in the source repository to gather dependencies from corporate or public artifact repositories, adding them to the scope of the analysis. Analysis scope selection: Simplified user experience to configure the analysis scope, with the possibility to force the analysis of known Open Source libraries. Authless deployment: Tackle can now be optionally deployed without Keycloak, allowing full unauthenticated admin access to the tool. This is especially useful when deploying the tool in resource constrained environments like local instances of Minikube, where only a single user would have access to it. Seamless upgrades: Tackle lifecycle is now managed by a new operator with Capability Level II, allowing seamless upgrades between GA versions. I would like to take the chance to congratulate the engineering teams involved in the development of this new release of Tackle for making this major leap forward possible. Of course, this has to be extended as well to the UI/UX, QE and Documentation teams that made sure our tool is usable and reliable.\nMake sure you visit the new official documentation website for Tackle 2 to get started with the tool. We will keep updating the documentation to make sure we cover all the use cases and scenarios in which Tackle can be used. In case you find anything missing, please let us know by creating an issue in the Konveyor documentation repository.\nHappy modernizing!\nBio: Ramón Román is the Product Manager for the Migration Toolkit for Applications within the Modernization and Migration Solutions team in Red Hat. Prior to that, he was part of Red Hat’s Consulting organization, where he worked as an Architect in the field, helping customers succeed in large-scale application migrations and Red Hat OpenShift adoption projects.\n","categories":"","description":"New management, assessment, and analysis capabilities will help you streamline the modernization of your application portfolio to Kubernetes. See what is new with the open source tool Tackle 2.0.","excerpt":"New management, assessment, and analysis capabilities will help you …","ref":"/blog/2022/tackle-2-new-capabilities/","tags":["Tackle"],"title":"Tackle 2: New capabilities for modernizing applications to leverage Kubernetes"},{"body":"The latest version of Tackle has the Developers view and the new Architects view to support the three main roles of users:\nAdministrators: Has access to some application-wide configuration parameters that other users can consume but not change or browse.\nExample actions: Define Git credentials, Maven settings, .xml files.\nArchitects: Often the technical leads for the migration project that can create and modify applications and information related to it. The Architects do not need to have access to sensitive information, but can consume it.\nExample actions: Associate an existing credential to access the repository of a given application.\nMigrators: Developers that can run assessments and analysis, but cannot create or modify applications in the portfolio. Maybe an example action?\nDeveloper view The developer view is intended to be used by migrators and stakeholders and has three pages with different functionalities. Application inventory The Application inventory page manages the applications being migrated and is where the assessment and analysis processes are performed. The application list provides a holistic view of the application portfolio using an extensible tagging model to classify application types. The applications can be expanded to show more detailed information which can be edited and managed.\nThis page has two tabs with different information and functionality: Assessment and Analysis.\nApplication Assessments Use the Assessment tab to facilitate a conversation before migrating an application with stakeholders such as technical subject matter experts and application owners or users. Tackle does not prescribe solutions but instead provides a script of discussion points to identify potential migration risks.\nAssessment categories include:\nApplication details Application dependencies Application architecture Application observability Application cross-cutting concerns Reviewing the results will help stakeholders develop suitable migration strategies for applications or application types. Only one assessment can be done at once, but assessment results can be applied to other applications.\nAdditional functionality includes:\nCopy assessment Copy assessment and review Discard assessment Manage dependencies Manage credentials Delete Analysis Use the Analysis tab to perform an automated examination of the application that views binaries, source code, and dependencies to find possible issues that might prevent the application from running on the new platform. The process starts by retrieving and then analyzing one of the following repositories:\nBinary: Provides a full picture of the application and library dependencies that are embedded into the code. Source code: Only provides a view of the application. Source code + dependencies: Provides a full picture by downloading the source code from the repository, then parses and downloads the dependencies from Maven and adds them to the source code. There is an option to upload a locally stored binary. This only works if a single application is selected.\nAdditional functionality includes:\nManage dependencies Manage credentials Analysis details Cancel analysis Delete Reports The Reports page provides an overview of the assessments and reviews for the entire application inventory. The Reports page contains the following sections:\nCurrent landscape: Displays all applications according to their risk levels. Adoption candidate distribution: Lists the assessed applications with the following columns: Criticality is based on the Business criticality value of the review. Priority is based on the Work priority value of the review. Effort is based on the Effort estimate value of the review. Decision is based on the Proposed action value of the review. By default, all applications are selected. You can clear some of the application check boxes to filter the report.\nSuggested adoption plan: Displays a suggested adoption plan based on effort, priority, and dependencies. Identified risks: Lists the severe risks identified in the assessments for all applications. Controls The Controls page is where the application parameters are managed by the architect or developers as the instance is configured and edited as the project progresses.\nThe parameters include:\nStakeholders Stakeholder groups Job functions Business services Tags (Tag Types) Administrator view The administrator view is intended to be used by administrators to set up the Tackle instance environment. Credentials This management module enables administrators to create and manage credentials for access to private repositories. It also allows for the architects to assign the credentials to applications without knowing their contents. The credentials page displays the available credentials with an Edit and Delete buttons and the following fields:\nName Description Type Source Control Username/Password Source Private Key/Passphrase Maven Settings File Proxy Type specific information Created by Note: Type specific credential information such as keys and passphrases will be hidden or shown as [Encrypted].\nRepositories This management module configures the repositories used by Tackle with the following options. Git Consume insecure Git repositories Subversion Consume insecure Subversion repositories Maven Manage repository size Force update of dependencies Consume insecure artifact repositories Proxy This management module configures HTTP \u0026 HTTPS proxy settings and credentials. To configure the proxies click the radio button and enter the following information. Proxy host Proxy port Authentication (optional) Proxy credentials Source\n","categories":"","description":"","excerpt":"The latest version of Tackle has the Developers view and the new …","ref":"/docs/konveyor/views/","tags":"","title":"Views"},{"body":"Tackle instances have key parameters that are configured in the Controls window prior to migration by the project architect and can be added and edited as needed.\nThese parameters define applications and individuals, teams, verticals or areas within an organization affected or participating in the migration.\nStakeholders Stakeholder groups Job functions Business services Tag types Tags Seeding Tackle instance The steps to creating and configuring a Tackle instance can be performed in any order, but the suggested order below is the most efficient for creating stakeholders and tags.\nStakeholders\nCreate stakeholder groups Create job functions Create stakeholders Tags\nCreate tag types Create tags Tackle stakeholders and defined by:\nEmail Name Job function Stakeholder groups Creating a new stakeholder group Follow the steps below to create a new stakeholder group. There are no default stakeholder groups defined.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Stakeholder groups tab. Click the Create new button. Enter the following information: Name Description Member(s) Click the Create button. Creating a new job function Tackle uses the job function attribute to classify stakeholders and provides a list of default values that can be expanded. Follow the steps below to create a new job function not included in the default list.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Job functions tab. Click the Create new button. Enter a job function title into the Name text box. Click the Create button. Creating a new stakeholder Follow the steps below to create a new migration project stakeholder.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Stakeholders tab. Click the Create new button. Enter the following information: Email Name Job function: Addition beyond the standard can be created Stakeholder group Click the Create button. Business services Tackle uses the business services attribute to define the departments within the organization that use the application and will be affected by the migration.\nImportant: Business services must be created prior to creating or importing applications.\nCreating a new business service Follow the steps below to create a new business service. There are no default business services defined and must be created prior to creating applications.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Business services tab. Click the Create new button. Enter the following information: Name Description Owner Click the Create button. Creating new tag types Tackle uses tags to classify applications in multiple categories. Follow the steps below to create a new tag type not included in the default list.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Tags tab. Click the Create tag type button. Enter the following information: Name Rank - order the tags appear on applications Color Click the Create button. Creating new tags Follow the steps below to create a new tag not included in the default list.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Tags tab. Click the Create tag button. Enter the following information: Name Tag Type Click the Create button. Source\n","categories":"","description":"","excerpt":"Tackle instances have key parameters that are configured in the …","ref":"/docs/konveyor/instances/","tags":"","title":"Seeding Instances"},{"body":"Follow the steps below to provision the minikube cluster and install Konveyor.\nProvisioning Minikube Follow the steps below to provision minikube for single users deploying Konveyor on a workstation. These steps will configure minikube and enable:\nIngress so the Konveyor tool can publish outside of the Kubernetes cluster. Operator lifecycle manager (OLM) addon. Procedure\nProvision the minikube cluster with these recommended parameters. Replace \u003cprofile name\u003e with your choice of minikube profile name. [user@user ~]$ minikube start --memory=10g -p \u003cprofile name\u003e Enable the ingress addon. [user@user ~]$ minikube addons enable ingress -p \u003cprofile name\u003e Install Operator Lifecycle Manager (OLM), a tool to help manage the Operators running on your cluster. curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.25.0/install.sh | bash -s v0.25.0 Installing Konveyor Operator Operators are a structural layer that manages resources deployed on Kubernetes (database, front end, back end) to automatically create a Konveyor instance instead of doing it manually.\nFollow the steps below to install the Konveyor Operator in the my-konveyor-operator namespace (default) on any Kubernetes distribution, including minikube.\nProcedure Tab 0 Tab 1 Tab 2 \u003col\u003e \u003cli\u003eInstall the latest released Konveyor Operator.\u003c/li\u003e \u003c/ol\u003e \u003cpre tabindex=\"0\"\u003e\u003ccode\u003e[user@user ~]$ kubectl create -f https://operatorhub.io/install/konveyor-0.2/konveyor-operator.yaml \u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis step will create the \u003ccode\u003emy-konveyor-operator\u003c/code\u003e namespace, catalogsource and other OLM related objects.\u003c/p\u003e \u003ch3 id=\"installing-the-beta-version\"\u003eInstalling the beta version\u003c/h3\u003e \u003cp\u003eIf you need to deploy a latest beta release build please use the below url\u003c/p\u003e \u003cpre tabindex=\"0\"\u003e\u003ccode\u003e[user@user ~]$ kubectl create -f https://operatorhub.io/install/beta/konveyor-operator.yaml \u003c/code\u003e\u003c/pre\u003e \u003ch3 id=\"installing-the-latest-version\"\u003eInstalling the latest version\u003c/h3\u003e \u003cp\u003eIf you need to deploy a latest available build please follow the steps below,\u003c/p\u003e \u003cul\u003e \u003cli\u003eCreate a namespace\u003c/li\u003e \u003c/ul\u003e \u003cpre tabindex=\"0\"\u003e\u003ccode\u003eapiVersion: v1 kind: Namespace metadata: name: my-konveyor-operator \u003c/code\u003e\u003c/pre\u003e\u003cul\u003e \u003cli\u003eCreate a custom CatalogSource\u003c/li\u003e \u003c/ul\u003e \u003cpre tabindex=\"0\"\u003e\u003ccode\u003eapiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: konveyor namespace: my-konveyor-operator spec: displayName: Konveyor Operator publisher: Konveyor sourceType: grpc image: quay.io/konveyor/tackle2-operator-index:latest \u003c/code\u003e\u003c/pre\u003e\u003cul\u003e \u003cli\u003eCreate an operator group and Subscription using the custom Catalog Source\u003c/li\u003e \u003c/ul\u003e \u003cpre tabindex=\"0\"\u003e\u003ccode\u003e--- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: konveyor namespace: my-konveyor-operator spec: targetNamespaces: - my-konveyor-operator --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: konveyor-operator namespace: my-konveyor-operator spec: channel: development installPlanApproval: Automatic name: konveyor-operator source: konveyor sourceNamespace: my-konveyor-operator \u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cem\u003eNote: Latest builds are built nightly. It strictly for developmental purpose and not to be used in production.\u003c/em\u003e\u003c/p\u003e 2. Verify Konveyor was installed.\n[user@user ~]$ kubectl get pods -n my-konveyor-operator Repeat this step until konveyor-tackle-XXX and tackle-operator-XXX show 1/1 Running. Create the Tackle instance Follow the steps below to initiate the Tackle instance and set a custom resource (CR) with the tackle_hub.yaml file. CRs can be customized to meet the project needs.\nProcedure\nCreate the instance pointing to the CR file. [user@user ~]$ cat \u003c\u003c EOF | kubectl apply -f - kind: Tackle apiVersion: tackle.konveyor.io/v1alpha1 metadata: name: tackle namespace: my-konveyor-operator spec: rwx_supported: \"false\" feature_auth_required: \"false\" EOF Verify the instance [user@user ~]$ kubectl get pods -n my-konveyor-operator Repeat this step until all components are Completed or Running. Note: This can take one to five minutes depending on the cluster resources.\nLogging in to Konveyor UI Follow the steps below to log in to the Konveyor web console.\nProcedure\nAccess the minikube dashboard. This will enable the dashboard add-on, and open the proxy in the default web browser. [user@user ~]$ minikube dashboard -p \u003cprofile name\u003e Ensure the top dropdown namespace selector is set to the my-konveyor-operator Click Service then Ingresses Click the endpoint IP for the tackle ingress ingress to launch the Konveyor web console in a new browser window. Note: This may default to http://$IP_ADDR and fail to load, if so switch to https://$IP_ADDR\nIn some cases, it could still fail to load, open the terminal and type in kubectl port-forward service/tackle-ui 8080:8080 -n my-konveyor-operator and then access the webpage by visiting localhost:8080\nThe default auth enabled credentials are: admin/Passw0rd! Source\n","categories":"","description":"","excerpt":"Follow the steps below to provision the minikube cluster and install …","ref":"/docs/konveyor/installation/","tags":"","title":"Installing Konveyor"},{"body":"Tackle core functions are assessing and analyzing the applications for migration and are performed on the Application inventory page. Assessing applications Follow the steps below to facilitate discussion of application migration.\nProcedure\nClick Application inventory in the left menu in the Development view. Click the checkbox to the left of the application being assessed. Note: Only one application can be assessed at a time.\nClick the Assess button. Select the Stakeholders and Stakeholder groups from the drop-down lists to track who contributed to the assessment for future reference. Note: Additional list options can be added in the Stakeholder Groups or Stakeholders tab on the Controls page in the Developer view.\nClick the Next button. Click the radio button next to the option that best answers the questions in each category and click Next to go to the next section when complete. Click Save and Review to view the risks that should be taken into account. Applying assessments to other applications Follow the steps below to apply an application assessment to similar applications being migrated.\nProcedure\nClick Application inventory in the left menu in the Development view. Click the checkbox of the application with the completed assessment to copy. Click the menu kebab at the right of the selected application. Select Copy assessment or Copy assessment and review. Click the checkbox of the application(s) to copy the assessment or assessment and review to. Click the Copy button. Running application analysis Follow the steps below to analyze an application for migration.\nProcedure\nClick Application inventory in the left menu in the Development view. Click the Analysis tab. Click the checkbox to the left of the application being analyzed. Note: More than one application can be analyzed at a time.\nCheck the credentials assigned to the application. Click the Analyze button. Select source for analysis from the drop-down list. Click the Next button. Set the target to one or more of the transformation targets. Application server migration to… Containerization Quarkus OpenJDK OpenJDK 11 Linux Camel Click the Next button. Click a radio button to select one of the following scope options to narrow down and focus the analysis. Application and internal dependencies only Application and all dependencies, including known Open Source libraries Select the list of packages to be analyzed manually (Type the file name and click Add.) Exclude packages (Type the package name and click Add.) Click the Next button. Set custom rules by typing a name or searching and clicking the Add Rules button. Consider the following rules: Target Source Exclude tags: Rules with these tags are not processed. Note: Analysis engines use standard rules for a comprehensive set of migration targets, but if the target is not included or customized frameworks custom rules can be added. Custom rules files will be validated.\nClick the Next button. Add or remove targets and sources to narrow the analysis. Exclude rules tags as necessary. Click the Next button. Verify the analysis parameters. Click the Run button. Analysis status will show Scheduled as it downloads the image for the container to execute. When that is complete, it will show In-progress until complete.\nNote: The analysis will take minutes to hours to run depending on the size of the application and the cluster capacity and resources.\nTackle relies on Kubernetes scheduling capabilities to determine how many analyzer instances are created based on cluster capacity. If several applications are selected for analysis, by default, only one analyzer can be provisioned at a time. With more cluster capacity, more analysis processes can be executed in parallel. 19. Expand the application and click the Report link to the right of Analysis when completed.\nReviewing the Analysis Report Follow the steps below to review the analysis report. For more information see Chapter 3. Reviewing the reports of the CLI Guide Migration Toolkit For Applications 5.3.\nProcedure\nClick Application inventory in the left menu in the Development view. Expand the application with a completed analysis. Click the Report link. Click the dependencies or source links. Click the tabs to review the report. Source\n","categories":"","description":"","excerpt":"Tackle core functions are assessing and analyzing the applications for …","ref":"/docs/konveyor/assessanalyze/","tags":"","title":"Assessing and analyzing applications"},{"body":"Applications can be added to Tackle by creating new applications from scratch manually or by importing them. Tackle applications are defined by manually entered and pre-defined attributes:\nName (manual) Description (manual) Business service (pre-defined) Tags (pre-defined) Source code Binary Creating a new application Follow the steps below to add a new application to the inventory for assessment and analysis.\nNote: Before starting this procedure, it is helpful to set up business services, check tags and tag types, and create additions as needed.\nProcedure\nClick Application inventory in the left menu of the Developer view. Click the Create new button. Enter the following information: Name Description Business service Tags Comments Click the arrow to the right of Source Code to expand the section. Enter the following information: Repository type Source Repository Branch Root path Click the down arrow to the right of Binary to expand the section. Enter the following information: Group Artifact Version Packaging Click the Create button. Assigning application credentials Follow the steps below to assign credentials to one or more applications.\nProcedure\nClick Application inventory in the left menu of the Developer view. Click the Analysis tab. Click the menu kebab to the right of the Review button and select Manage credentials. Select the credential from the Source credentials or Maven setting drop-down list. Click the Save button. Importing application lists Tackle allows users to import a spreadsheet of existing applications instead of entering them manually. A recommended sample CSV template has been made available to users to fill in with the required information.\nDownloading the application list CSV template. Follow the steps below to download the sample CSV template.\nProcedure\nClick Application inventory in the left menu of the Developer view. Click the menu kebab button to the right of the Review button. Click Manage imports to open the Application imports page. Click the kebab menu to the right of the Import button and click Download CSV template. Importing an application list Follow the steps below to import a .csv file provided by the customer containing a list of applications and their attributes. Importing can be performed by using the Import or Manage import functions, but using the steps below is recommended for verifying the import was successful.\nNote: Importing will only add applications, it will not overwrite any existing ones.\nProcedure\nReview the import file for all the required information in the required format. Click Application inventory in the left menu of the Developer view. Click the menu kebab button to the right of the Review button. Click the Import button. Browse to the file and click the Import button. Verify the import has completed and check the number of rows accepted or rejected. Review the imported applications by clicking the arrow to the left of the check box to expand. Important: Rows accepted may not match the number of applications in the Application inventory list because some rows are dependencies. To verify, check the record type column of the CSV file for applications defined as 1 and dependencies as 2.\nSource\n","categories":"","description":"","excerpt":"Applications can be added to Tackle by creating new applications from …","ref":"/docs/konveyor/addapps/","tags":"","title":"Adding applications"},{"body":"Konveyor is a community of people passionate about helping others modernize and migrate their applications to the hybrid cloud by building tools, identifying patterns, and providing advice on how to break down monoliths, adopt containers, and embrace Kubernetes. To do so the Konveyor community is currently working on five projects to help users rehost, replatform, and refactor their applications to Kubernetes.\nWhile conversations with users and data from various sources gave us some visibility into how migrations were taking place, we wanted to inform our priorities with recent detailed data. To do so, Red Hat, a Konveyor community contributor, sponsored a survey of 600 IT decision makers, backend developers, and software architects in the US, UK, and English-speaking Asia-Pacific (APAC) to better understand the application modernization landscape.\nHere are some of the findings that relate to Konveyor and application modernization:\nApplication modernization is happening now It’s clear that there is urgency to roll out migration tooling. Among those we surveyed, those who were migrating or planning to, we found that they plan to modernize over half of their custom applications during the next year and over a quarter during the next six months. Only 20% expected the modernization to take more than two years. It’s clear this is happening right now.\nThat said, it’s not all smooth sailing and this doesn’t happen overnight. While some regions haven’t progressed far enough in their application modernization journey to experience serious barriers, others (notably the US) report that choosing a straightforward approach can be challenging. Organizations have many options to choose from and each option presents varying levels of complexity.\nMotivators for modernizing existing applications to Kubernetes The survey tells us modernization is mostly being driven by the need for reliability, scalability and security. Decreasing costs were also on the list but, as we frequently see in other surveys about technology adoption or change, it’s not the driving factor.\nKonveyor’s tool suite can be used to help realize organization’s migration plans.\nThe Konveyor tool suite maps closely to the migration strategies companies intend to use based on this survey: rehosting, replatforming, and refactoring.\nRehost applications to cloud with Forklift and Crane Rehosting (20% of total) involves copying applications to a cloud (whether hosted or on-prem) with minimal changes, i.e. “lift and shift.” This will be familiar to anyone with experience in virtualization early on when bare metal servers were often moved to a virtual machine (VM) for greater efficiency but with little or no changes to the software and with relatively modest operational changes.\nWhile rehosting doesn’t provide the same depth of benefits as replatforming or refactoring, it’s the first step in the right direction. Rehosting was the single most common approach Forklift and Crane are the relevant Konveyor projects here. Forklift is focused on migrating VMs to Kubernetes and provides the ability to migrate multiple VMs to KubeVirt with minimal downtime.\nCrane is a rehosting tool that meets a different use case. Crane helps organizations to migrate applications between Kubernetes clusters. There are many times when developers and operations teams want to migrate between older and newer versions of Kubernetes, evacuate a cluster, or migrate to different underlying infrastructure.\nReplatforming applications with Move2Kube Replatforming (18% of total) is using a cloud migration to upgrade operating systems, databases, and other components as needed in order to cloud-enable applications without changing core application code or architecture. Think of it as “lift, tinker, and shift.”\nThis is effectively an intermediate step between just rehosting and refactoring. In fact, it’s often a stepping stone; the survey found that 90% of those planning to refactor their apps plan to replatform them first.\nMove2Kube is a Konveyor project that allows customers to replatform their applications to Kubernetes orchestrated platforms. One area where replatforming is taking place is in the consolidation of container orchestration platforms to Kubernetes.\nDue to this consolidation, the Move2Kube project was started to focus on accelerating the process of replatforming to Kubernetes from platforms such as Swarm and Cloud Foundry. This project translates existing artifacts to Kubernetes artifacts to speed up the process of being able to run applications on Kubernetes.\nRefactoring applications with Tackle Refactoring (17%) is often viewed as the most challenging application modernization strategy but it’s the one with the most long-term impact. Refactoring involves re-architecting as cloud-native, for example, by containerizing workloads, transforming a monolith to microservices, or moving functions to a serverless architecture.\nWhile Tackle is not a full refactoring tool, it provides a series of interrelated tools that allow users to assess, analyze, and ultimately move their applications onto a Kubernetes orchestrated platform.\nAs you can see, organizations are taking a thoughtful approach to app migration and there’s no single right path. This data supports the Konveyor community’s approach of providing a toolbox, rather than just a singular tool, that can support transitioning applications however it is happening.\n","categories":"","description":"In MTC 1.4.0, a new feature called Direct Migration is available that will yield significant time savings for most customers migrating persistent volumes and/or internal images.","excerpt":"In MTC 1.4.0, a new feature called Direct Migration is available that …","ref":"/blog/2022/application-modernization-report/","tags":["Kubernetes"],"title":"Application Modernization Report Shows Need For Kubernetes-specific Migration Tooling"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/hackfest/","tags":"","title":"HackFest"},{"body":"January 31–February 4, 2022. 9 a.m.–5 p.m. EST each day\nGet free hands-on experience migrating different workload types to Kubernetes with open source tools and community engineer support. Register to get the calendar invite.\nThe Konveyor community has developed multiple use cases for you to learn time- and cost-efficient ways to migrate to Kubernetes. During this HackFest, if something is missing, not working as expected, or you simply get stuck — you can open a GitHub issue or get support from the community engineers.\nBy joining, you’ll help improve the effectiveness and usability of the open source tools in the Konveyor Community. All tools you can continue to leverage for free after the event is over to migrate to Kubernetes.\nYou’ll walk away knowing how to:\nRehost apps between Kubernetes clusters and automating with GitOps using the tool Crane\nReplatform apps to Kubernetes from other container orchestration platforms with tool Move2Kube\nRehost virtual machines to KubeVirt with tool Forklift\nAssess, analyze, and refactor applications to Kubernetes with tool Tackle.\nEach day will be dedicated to 1 or 2 use cases addressed by our tools. You can participate each day or the day we focus on the use case most relevant to you.\nRegister on this form to get the calendar invite and full agenda once it is finalized.\nDetails on scenario and instructions ","categories":"","description":"Get free hands-on experience migrating different workload types to Kubernetes with open source tools and community engineer support.","excerpt":"Get free hands-on experience migrating different workload types to …","ref":"/blog/2021/konveyor-community-hackfest/","tags":["HackFest"],"title":"Konveyor Community HackFest"},{"body":"My team has been working with organizations adopting containers, Kubernetes, and Red Hat OpenShift for more than three years now. When we started providing professional services around enterprise Kubernetes, it became clear we needed a program-level framework for adopting containers that spelled out the activities of multiple project teams. Some participants would be focused on container platform management and operations, some on continuous delivery, some on the application lifecycle, and others on cross-cutting process transformation.\nWe’ve had success using this framework to help customers rethink container adoption as less a matter of new technology implementation and more as an “organizational journey,” where people and process concerns are at least as important as the successful deployment of OpenShift clusters.\nOver time, we’ve realized the program framework is missing a guiding force that gets executive stakeholders engaged and can keep all participants focused on a consistent, meaningful set of objectives. Too often, we’ve seen IT and development managers concentrated on narrow, tactical objectives that don’t drive the big-picture, transformational needs of those enterprises. What we felt was lacking was a set of trackable and meaningful measures that could demonstrate progress to stakeholders in a more visible way.\nWe were very excited by the release of Accelerate by Dr. Nicole Forsgren, Jez Humble, and Gene Kim last year as the culmination of “a four-year research journey to investigate what capabilities and practices are important to accelerate the development and delivery of software, and, in turn, value to companies.” The authors, known for their work on Puppet Labs’ State of DevOps reports and books like Continuous Delivery and The Phoenix Project, were able to use survey data and statistical analysis to show relationships between specific capabilities/practices and organizational performance.\nOne of these capabilities, software delivery performance, should be of particular interest to organizations undergoing cloud adoption and/or the digital transformation. Forsgren and her co-authors showed a statistical link between software delivery performance and organization performance, including financial indicators like profitability, productivity, market share, and number of customers. Interestingly, the authors showed a link between software delivery performance and non-commercial performance as well: things like product quality, customer satisfaction, and achieving mission goals.\nEqually important, the authors defined software delivery performance in a concrete, measurable way that can be used as indicators for a range of transformative practices and capabilities. They defined software delivery performance using four metrics: Lead Time, Deployment Frequency, Mean Time to Recovery, and Change Failure Rate, described below.\nFinally, the authors enumerated the various practices and capabilities that drive software delivery performance as measured this way: test automation, deployment automation, loosely coupled architecture, and monitoring, among others.\nWhat this means is that we have specific measures that adopters of container platforms (among other emerging technologies) can use to guide how the technology is adopted in ways that can lead to better organizational performance. And we have a set of practices that can be applied against this technology backdrop, using containers and container platform as accelerators of those practices when possible.\nThe focus for the authors is on global performance, not local optimization, and on “outcomes not output,” so the organization rewards activities that matter, rather than sheer volume of activity. This last point is crucial. In an earlier post, I wrote about app onboarding to OpenShift. Taken to the extreme, a myopic focus on the percentage of the portfolio or number of apps migrated to X (containers, Kubernetes, OpenShift, AWS, “The Cloud”) is a focus on outputs not outcomes. It’s a measure that seems to indicate progress but does not directly determine the success of the cloud adoption program as a whole, success that must involve some wider notion of commercial or noncommercial performance.\nPut another way, cloud platforms do not automatically confer continuous delivery capabilities upon their adopters. They enable them. They accelerate them. But without changing the way we deliver software as an organization — the way we work — cloud technology (or any other newly introduced technology) will probably fail to match its promise.\nI will be writing more about how we put a metrics-based approach into practice with our customers in upcoming posts, starting with an update on how we’ve begun to capture these metrics in dashboards to keep stakeholders and project participants aligned to meaningful goals.\n","categories":"","description":"When we started providing professional services around enterprise Kubernetes, it became clear we needed a program-level framework for adopting containers that spelled out the activities of multiple project teams.","excerpt":"When we started providing professional services around enterprise …","ref":"/blog/2021/metrics-driven-approach-to-transformation/","tags":["Metrics","OpenShift"],"title":"Exploring a Metrics-Driven Approach to Transformation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/openshift/","tags":"","title":"OpenShift"},{"body":"In MTC 1.4.0, a new feature called Direct Migration is available that will yield significant time savings for most customers migrating persistent volumes and/or internal images. Direct Migration enables the migration of persistent volumes and internal images directly from the source cluster to the destination cluster without an intermediary replication repository. This introduces a significant performance enhancement while also providing better error and progress reporting information back to the end-user.\nPrior MTC releases only allowed a 2 step backup and restore process when handling images and persistent volumes, which didn’t take advantage of potential performance savings in the case of a migration with both the source and destination clusters having network connectivity. The prior process leveraged a filesystem copy tool called Restic integrated with Velero. MTC would need to copy PV and Image data twice: once from the source cluster to the replication repository, and a second time from the replication repository to the destination cluster. This process yielded flexibility for various environments, yet failed to take advantage of the more common case in a migration of direct network connectivity between clusters.\nWith Direct Migration in MTC 1.4, we are able to take advantage of any environments that have network connectivity between the clusters involved in the migration by handling the migration of PV data and Image data in a new way. Assuming remote clusters have exposed routes to enable external access to the internal registry, MTC will push internal images directly into the internal registry on the destination cluster from the source cluster. Assuming the clusters can communicate over port 443 via OpenShift routes, MTC will directly migrate Persistent Volume data from a source cluster to a destination cluster using a set of Rsync + Stunnel pods which we will describe more below.\nPrerequisites As mentioned above for direct image migrations, it is important for remote source and destination clusters internal registries be exposed for external network traffic.\nexpose OCP 4 cluster registry expose OCP 3 cluser registry For direct volume migrations, the source and destination clusters must be able to communicate over port 443 via OpenShift routes. MTC will create a route in each namespace to be migrated on the destination cluster and will be migrating the data by establishing an Rsync connection to this route. This means that the OpenShift routes must be accessible by the source cluster.\nThe above diagram shows a configuration of 3 clusters running MTC. The Host cluster does not need to be its own cluster and could be the source or the destination cluster.\nDirect Image Migration Details In direct image migration (DIM), the Migration Controller pod creates a DIM resource that contains a list of namespaces to migrate. The DIM controller then finds all of the imagestreams with internal image references in those namespaces and copies them directly into the destination registry. In order for this to succeed, DIM also ensures the namespace exists on the destination cluster prior to the copy.\nThe above diagram shows how DIM orchestrates the migration of an imagestream backed by the internal registry directly from the source cluster to the destination cluster.\nDirect Volume Migration Details For Direct Volume Migration, or DVM, the DVM controller takes in a list of PVCs to migrate and performs a few steps prior to copying the data over. For DVM to work, it is necessary for the source and destination clusters to be able to communicate over port 443 via OCP routes which are created by DVM.\nFirst, DVM sets up the needed configuration configmaps and secrets across all the namespaces to migrate and then creates transfer pods on the destination cluster in each namespace to be migrated. The transfer pods on the destination include both rsyncd and stunnel containers. Stunnel is used to open up a communication tunnel between the source and destination. The rsyncd container has volume mounts to all PVCs in the namespace which are to be migrated. An rsync client pod is created on the source (1 per PVC) to then rsync the data via the stunnel route.\nThe above diagram shows how MTC orchestrates a direct volume migration by standing up a set of rsync and stunnel pods to directly migrate the PV data from the source to the destination cluster.\nConclusion In MTC 1.4.0, users can expect significant performance enhancements when direct migration is taken advantage of. The direct migration feature also grants the user more granular progress and error reporting back in the user interface to make it easier to debug what went wrong with the migration.\n","categories":"","description":"In MTC 1.4.0, a new feature called Direct Migration is available that will yield significant time savings for most customers migrating persistent volumes and/or internal images.","excerpt":"In MTC 1.4.0, a new feature called Direct Migration is available that …","ref":"/blog/2021/direct-migration/","tags":["MTC"],"title":"Migrating Faster with Direct Migration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mtc/","tags":"","title":"MTC"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/community/","tags":"","title":"Community"},{"body":"","categories":"","description":"","excerpt":"","ref":"/components/","tags":"","title":"Components"},{"body":"","categories":"","description":"","excerpt":"","ref":"/components/konveyor-cli/","tags":"","title":"Kantra, Konveyor Command-Line-Interface (CLI)"},{"body":"","categories":"","description":"","excerpt":"","ref":"/components/kai/","tags":"","title":"Konveyor AI (Kai)"},{"body":"","categories":"","description":"","excerpt":"","ref":"/components/konveyor-hub/","tags":"","title":"Konveyor Hub"},{"body":"Scope of This Notice This Privacy Statement is intended to describe the Konveyor Project’s privacy practices and provide information about the choices you have regarding the ways in which information collected by the Konveyor Project is used and disclosed. For convenience, the Konveyor Project is referred to in this document as “Konveyor”.\nOur Commitment to Privacy At Konveyor, your privacy is important to us. To better protect your privacy, we have provided this Statement explaining our information practices and the choices you can make about the way your personal information is collected, used and disclosed. To make this Statement easy to find, we have made it available on our homepage and at every location where personally-identifiable information may be requested.\nThe Information We Collect This Privacy Statement applies to all information collected by or submitted to Konveyor, including personal data. “Personal data” is data that can be used to identify an individual.\nKonveyor collects personal data when:\nyou create a Gerrit and/or Bugzilla user account you participate in surveys and evaluations you submit questions or comments to us Konveyor may also collect personal data from individuals (with their consent) at conventions, trade shows and expositions.\nThe types of personal data collected may include (but are not limited to):\nyour first and last name your title and your company’s name your e-mail address any other identifier that permits Konveyor to make physical or online contact with you any information that Konveyor collects online from you and maintains in association with your account, such as: your GPG key ID your SSH public key your IRC nickname your language preference your timezone Publicly Available Personal Data In keeping with the open nature and spirit of Konveyor, some personal data attached to Konveyor accounts is made public by default. Specifically:\nyour first and last name your email address your language preference your SSH public key your timezone your GPG key ID (if defined) your IRC nickname (if defined) If you wish for this information to be kept private, you can opt-out of displaying this information publicly in your gerrit account preferences. If you choose to opt-out, Konveyor will still have access to this information, but it will not be displayed based on Gerrit capabilities to others, and will be considered Private. The only exception to this is for your email address, which may still be visible in some Konveyor services such as Bugzilla.\nUsing (Processing) Your Personal Data Konveyor uses the personal data you provide to:\ncreate and maintain your accounts attribute data and content you produce directly and indirectly in our public-facing services answer your questions send you information for research activities, including the production of statistical reports (such aggregated information is not used to contact the subjects of the report) send you surveys It is in Konveyor’s legitimate business interests to provide you with the information, communications, and services you request; to create a public record of the data and content produced by Konveyor’s services; and to maintain the integrity of that data and content for historical, scientific, and research purposes.\nSharing Your Personal Data Unless you consent, Konveyor will never process or share the personal data you provide to us except as described below. Konveyor may disclose your personal data to third parties under any of the following circumstances:\nYour publicly available personal data in the Gerrit account system, as described above, is accessible by anyone unless you, as the account holder, opt out as already described in this Privacy Statement. As required to provide service, and for e-mail housing (as a consequence of uses already described in this Privacy Statement). It is in Konveyor’s legitimate business interest to provide all users an accurate record of data and content provided by Konveyor’s services, and to maintain the integrity of that data and content for historical, scientific, and research purposes. This data and content may include but is not limited to email, code changes, comments, and artifacts. As required by law (such as responding to a valid subpoena, warrant, audit, or agency action, or to prevent fraud). For research activities, including the production of statistical reports (such aggregated information is used to describe our services and is not used to contact the subjects of the report). Receiving E-Mail For your protection, Konveyor may contact you in the event that we find an issue that requires your immediate attention. Konveyor processes your personal data in these cases to fulfill and comply with its contractual obligations to you, to provide the services you have requested.\nCookies and Other Browser Information Konveyor’s online services automatically capture IP addresses. We use IP addresses to help diagnose problems with our servers, to administer our website, and to help ensure the security of your interaction with our services. Your IP address is used to help identify you and your location, in order to provide you data and content from our services as quickly as possible. It is in Konveyor’s legitimate business interest to maximize the efficiency and effectiveness of its services for all users. As part of offering and providing customizable and personalized services, Konveyor uses cookies to store and sometimes track information about you. A cookie is a small amount of data that is sent to your browser from a Web server and stored on your computer’s hard drive. All sections of www.konveyor.io where you are prompted to log in or that are customizable require your browser to accept cookies.\nGenerally, we use cookies to:\nremind us of who you are and to access your gerrit account information (stored on our computers) in order to provide a better and more personalized service. This cookie is set when you register or “sign in” and is modified when you “sign out” of our services estimate audience size. Each browser accessing Konveyor gerrit is given a unique cookie that is used to determine the extent of repeat usage and usage by a registered user versus by an unregistered user measure certain traffic patterns, which areas of Konveyor’s network of websites you have visited, and your visiting patterns in the aggregate. We use this research to understand how our user’s habits are similar or different from one another so that we can make each new experience on www.konveyor.io a better one. We may use this information to better personalize the content, banners, and promotions you and other users will see on our sites. If you do not want your personal information to be stored by cookies, you can configure your browser so that it always rejects these cookies or asks you each time if you accept them or not. However, you must understand that the use of cookies may be necessary to provide certain services, and choosing to reject cookies will reduce the performance and functionality of the site. Your browser documentation includes instructions explaining how to enable, disable or delete cookies at the browser level (usually located in the “Help”, “Tools” or “Edit” facility).\nOur Commitment to Data Security Konveyor trains its administrators on our privacy policy guidelines and makes our privacy policy available to our partners. Our website uses Secure Socket Layer (SSL) technology, which encrypts your personal data when you send your personal information on the Gerrit website. In addition, Konveyor and its partners enter into confidentiality agreements which require that care and precautions be taken to prevent loss, misuse, or disclosure of your personal data.\nPublic Forums Reminder Konveyor often makes chat rooms, forums, mailing lists, message boards, and/or news groups available to its users. Please remember that any information that is disclosed in these areas becomes public information. Exercise caution when deciding to disclose your personal data. Although we value individual ideas and encourage free expression, Konveyor reserves the right to take necessary action to preserve the integrity of these areas, such as removing any posting that is vulgar or inappropriate. It is in Konveyor’s legitimate business interests to provide all users an accurate record of data and content provided in the public forums it maintains and uses; to maintain the integrity of that data and content for historical, scientific, and research purposes; and to provide an environment for the free exchange of ideas relevant and constructive to the development and propagation of open source software.\nOur Commitment to Children’s Online Privacy Out of special concern for children’s privacy, Konveyor does not knowingly accept online personal information from children under the age of 13. Konveyor does not knowingly allow children under the age of 13 to become registered members of our sites. Konveyor does not knowingly collect or solicit personal information about children under 13. In the event that Konveyor ever decides to expand its intended site audience to include children under the age of 13, those specific web pages will, in accordance with the requirements of the Children’s Online Privacy Protection Act (COPPA), be clearly identified and provide an explicit privacy notice addressed to children under 13. In addition, Konveyor will provide an appropriate mechanism to obtain parental approval, allow parents to subsequently make changes to or request removal of their children’s personal information, and provide access to any other information as required by law.\nAbout Links to Other Sites This site contains links to other sites. Konveyor does not control the information collection of sites that can be reached through links from www.konveyor.io. If you have questions about the data collection procedures of linked sites, please contact these sites directly.\nYour Rights and Choices in the EEA Where the EU General Data Protection Regulation 2016/679 (“GDPR”) applies to the processing of your personal data, especially when you access the website from a country in the European Economic Area (“EEA”), you have the following rights, subject to some limitations, against Konveyor:\nThe right to access your personal data; The right to rectify the personal data we hold about you; The right to erase your personal data; The right to restrict our use of your personal data; The right to object to our use of your personal data; The right to receive your personal data in a usable electronic format and transmit it to a third party (also known as the right of data portability); and The right to lodge a complaint with your local data protection authority. If you would like to exercise any of these rights, you may do so via our Personal Data Request Form. Please understand, however, the rights enumerated above are not absolute in all cases. Where the GDPR applies, you also have the right to withdraw any consent you have given to uses of your personal data. If you wish to withdraw consent that you have previously provided to Konveyor, you may do so via our privacy mail: legal@redhat.com. However, the withdrawal of consent will not affect the lawfulness of processing based on consent before its withdrawal.\nHow to Contact Us If you have any questions about any of these practices or Konveyor’s use of your personal information, please feel free to contact us by email at legal@redhat.com. Konveyor will work with you to resolve any concerns you may have about this Statement.\nChanges to this Privacy Statement Konveyor reserves the right to change this Privacy Statement from time to time. If we do make changes, the revised Privacy Statement will be posted on the main project’s website. A notice will be posted on our homepage for 30 days whenever this privacy statement is changed in a material way.\nThis Privacy Statement was last amended on October 28, 2020\n","categories":"","description":"","excerpt":"Scope of This Notice This Privacy Statement is intended to describe …","ref":"/privacy/","tags":"","title":"Privacy Statement for the Konveyor Community"},{"body":"","categories":"","description":"","excerpt":"","ref":"/ecosystem/","tags":"","title":"The Konveyor Ecosystem"}]